[{"authors":["fcorowe"],"categories":null,"content":"Francisco Rowe is the lead of the Geographic Data Science Lab and Senior Lecturer at University of Liverpool, United Kingdom.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en-uk","lastmod":-62135596800,"objectID":"7088df2715aefb2227007a6b6976363b","permalink":"/authors/fcorowe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fcorowe/","section":"authors","summary":"Francisco Rowe is the lead of the Geographic Data Science Lab and Senior Lecturer at University of Liverpool, United Kingdom.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":" The materials for each session are posted here. Work your way through the links on the left and complete the associated homeworks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en-uk","lastmod":1656086209,"objectID":"108da05078d325a5a1f01a1ff2583053","permalink":"/class/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/class/","section":"class","summary":"The materials for each session are posted here. Work your way through the links on the left and complete the associated homeworks.","tags":null,"title":"Class sessions","type":"docs"},{"authors":null,"categories":null,"content":"  Contents  Session outcomes Introduction Concepts  Data structure Types of variable Types of observation Tidy data  Techniques  Import Describe Transform Tidy  Conclusions References   Session outcomes By the end of this session you should gain the following knowledge:\n  Learn the vocabulary and concepts used to describe data. Appreciate the characteristics and importance of tidy data (Wickham 2014) for data processing and analysis.    By the end of this session you should gain the following practical skills:\n  Load flat file datasets in RStudio via querying an API. Calculate descriptive summaries over datasets. Apply high-level functions in dplyr and tidyr for working with data. Create statistical graphics that expose high-level structure in data for cleaning purposes.     Introduction This session covers some of the basics around how to describe and organise data. Whilst this might sound prosaic, there are several reasons why being able to consistently describe a dataset is important. First: it is the initial step in any analysis and helps delimit the research themes and technical procedures that can be deployed. This is especially relevant to modern Data Science-type workflows (like those supported by tidyverse), where it is common to apply the same analysis templates for working over data. Describing your dataset with a consistent vocabulary enables you to identify which analysis templates to reuse. Second relates to the point in session 1 that Geographic Data Science projects usually involve repurposing datasets for social science research for the first time. It is often not obvious whether the data contain sufficient detail and structure to characterise the target behaviours to be researched and the target populations they are assumed to represent. This leads to additional levels of uncertainty and places greater importance on the initial step of data processing, description and exploration.\nThrough the session we will learn both language and concepts for describing and thinking about data, but also how to deploy some of the most important data processing and organisation techniques in R to wrangle over a real dataset. We will be working throughout with data from New York’s Citibike scheme, accessed through the bikedata package, an API to Citibike’s publicly available origin-destination trip data.\n The idea of applying a consistent vocabulary to describing your data applies especially to working with modern visualization toolkits (ggplot2, Tableau, vega-lite), and will be covered in some detail during the next session as we introduce Visualization Fundamentals and the Grammar of Graphics.    Concepts Data structure In the module we will work with data frames in R. These are spreadsheet like representations where rows are observations (case/record) and columns are variables. Each variable (column) in a data frame is a vector that must be of equal length. Where observations have missing values for certain variables – that is, where they may violate this equal-length requirement – the missing values must be substituted with something, usually with NA or similar. This constraint occasionally causes difficulties, for example when working with variables that contain values of different length for an observation. In these cases we create a special class of column, a list-column, something we’ll return to later in the module.\nOrganising data according this simple structure – rows as observations, columns as variables – makes working with data more straightforward. A specific set of tools, made available via the tidyverse, can be deployed for doing most data tidying tasks (Wickham 2014).\n Types of variable   Table 1: A breakdown of Stevens (1946) variable types, operators and measures of central tendency that can be applied to each.    Measurement  Description  Example  Operators  Midpoint  Dispersion     Categories    Nominal  Non-orderable categories  Political parties; street names  = ≠  mode  entropy    Ordinal  Orderable categories  Terrorism threat levels  … | \u0026lt;\u0026gt;  … | median  … | percentile   Measures    Interval  Numeric measurements  Temperatures; years  … | + -  … | mean  … | variance    Ratio  … | Counts  Distances; prices  … | × ÷  … | mean  … | variance     A classification you may have encountered for describing variables is that developed by Stevens (1946), which considers the level of measurement of a variable. Stevens (1946) classed variables into two groups: variables that describe categories of things and variables that describe measurements of things. Categories include attributes like gender, titles, Subscribers or Casual users of a bikeshare scheme and ranked orders (1st, 2nd, 3rd largest etc.). Measurements include quantities like distance, age, travel time, number of journeys made on a bikeshare scheme.\nCategories can be further subdivided into those that are unordered (nominal) from those that are ordered (ordinal). Measurements can also be subdivided. Interval measurements are quantities that can be ordered and where the difference between two values is meaningful. Ratio measurements have both these properties, but also have a meaningful 0 – where 0 means the absence of something – and where the ratio of two values can be computed. The most common cited example of an interval measurement is temperature (in degrees C). Temperatures can be ordered and compared additively, but 0 degrees C does not mean the absence of temperature and 20 degrees C is not twice as “hot” as 10 degrees C.\nWhy is this important? The measurement level of a variable determines the types of data analysis procedures that can be performed and therefore allows us to efficiently make decisions when working with a dataset for the first time (Table 1).\n Types of observation Observations either together form an entire population or a subset, or sample that we expect represents a target population.\nYou no doubt will be familiar with these concepts, but we have to think a little more about this in Geographic Data Science applications as we may often be working with datasets that are so-called population-level. The Citibike dataset is a complete, population-level dataset in that every journey made through the scheme is recorded. Whether or not this is truly a population-level dataset, however, depends on the analysis purpose. When analysing the bikeshare dataset are we interested only in describing use within the Citibike scheme? Or are we taking the patterns observed through our analysis to make claims and inferences about cycling more generally?\nIf the latter, then there are problems as the level of detail we have on our sample is pretty trivial compared to traditional datasets, where we deliberately design data collection activities with a specified target population in mind. It may therefore be difficult to gauge how representative Citibike users and Citibike cycling is of New York’s general cycling population. The flipside is that passively collected data do not suffer from the same problems such as non-response bias and social-desirability bias as traditionally collected datasets.\n Tidy data I mentioned that we would be working with data frames organised such that columns always and only refer to variables and rows always and only refer to observations. This arrangement, called tidy (Wickham 2014), has two key advantages. First, if data are arranged in a consistent way, then it is easier to apply and re-use tools for wrangling them due to data having the same underlying structure. Second, placing variables into columns, with each column containing a vector of values, means that we can take advantage of R’s vectorised functions for transforming data – we will demonstrate this in the technical element of this session.\nThe three rules for tidy data:\nEach variable forms a column. Each observation forms a row. Each type of observational unit forms a table.  Drug treatment dataset To elaborate further, we can use the example given in Wickham (2014), a drug treatment dataset in which two different treatments were administered to participants.\nThe data could be represented as:\n  Table 2: Table 1 of Wickham (2014).    person  treatment_a  treatment_b      John Smith  –  2    Jane Doe  16  11    Mary Johnson  3  1     An alternative organisation could be:\n  Table 3: Alternative organisation of Table 1 of Wickham (2014).    treatment  John Smith  Jane Doe  Mary Johnson      treatment_a  –  16  3    treatment_b  2  11  1     Both present the same information unambiguously – Table 3 is simply Table 2 transposed. However, neither is tidy as the observations are spread across both the rows and columns. This means that we need to apply different procedures to extract, perform computations on, and visually represent, these data.\nMuch better would be to organise the table into a tidy form. To do this we need to identify the variables:\nperson: a categorical nominal variable which takes three values: John Smith, Jane Doe, Mary Johnson. treatment: a categorical nominal variable which takes values: a and b. result: a measurement ratio (I think) variable which six recorded values (including the missing value): -, 16, 3, 2, 11,  Each observation is then a test result returned for each combination of person and treatment.\nSo, a tidy organisation for this dataset would be:\n  Table 4: Tidy version of Table 1 of Wickham (2014).    person  treatment  result      John Smith  a  –    John Smith  b  2    Jane Doe  a  16    Jane Doe  b  11    Mary Johnson  a  3    Mary Johnson  b  1      Gapminder population dataset In chapter 12 of Wickham and Grolemund (2017), the benefits of this layout, particularly for working with R, are demonstrated with the gapminder dataset. I recommend reading this short chapter in full. We will be applying similar approaches in the technique part of this class (which follows shortly) and also the homework. To consolidate our conceptual understanding of tidy data let’s quickly look at the gapminder data, as it is a dataset we’re probably more likely to encounter.\nFirst, a tidy version of the data:\n  Table 5: Tidy excerpt from gapminder dataset.    country  year  cases  population      Afghanistan  1999  745  19987071    Afghanistan  2000  2666  20595360    Brazil  1999  37737  172006362    Brazil  2000  80488  174504898    China  1999  212258  1272915272    China  2000  213766  1280428583     So the variables:\ncountry: a categorical nominal variable. year: a date (cyclic ratio) variable. cases: a ratio (count) variable. population: a ratio (count) variable.  Each observation is therefore a recorded count of cases and population for a country in a year.\nAn alternative organisation of this dataset that appears in Wickham and Grolemund (2017) is below. This is untidy as the observations are spread across two rows. This makes operations that we might want to perform on the cases and population variables – for example computing exposure rates – somewhat tedious.\n  Table 6: Untidy excerpt of gapminder dataset: observations spread across rows    country  year  type  count      Afghanistan  1999  cases  745    Afghanistan  1999  population  19987071    Afghanistan  2000  cases  2666    Afghanistan  2000  population  20595360    Brazil  1999  cases  37737    Brazil  1999  population  174504898    …  …  …  …     This actually doesn’t appear in Wickham and Grolemund (2017), but imagine that the gapminder dataset instead reported values of cases separately by gender. A type of representation I’ve often seen in social science domains, probably as it is helpful for data entry, is where observations are spread across the columns. This too creates problems for performing aggregate functions, but also for specifying visualization designs (in ggplot2) as we will discover in the next session.\n  Table 7: Untidy possible excerpt of gapminder dataset: observations spread across columns    country  year  f_cases  m_cases  f_population  m_population      Afghanistan  1999  447  298  9993400  9993671    Afghanistan  2000  1599  1067  10296280  10299080    Brazil  1999  16982  20755  86001181  86005181    Brazil  2000  39440  41048  87251329  87253569    China  1999  104007  108252  636451250  636464022    China  2000  104746  109759  640212600  640215983        Techniques The technical element to this session involves importing, describing, transforming and tidying data from a large bikeshare scheme – New York’s Citibike scheme.\n Download the  02-template.Rmd file for this session and save it to the reports folder of your vis-for-gds project that you created in session 1. Open your vis-for-gds project in RStudio and load the template file by clicking File \u0026gt; Open File ... \u0026gt; reports/02-template.Rmd.  Import In the template file there is a discussion of how to setup your R session with key packages – tidyverse , fst, lubridate, sf – and also the bikedata package for accessing bikeshare data.\nAvailable via the bikedata package are trip and occupancy data for a number of bikeshare schemes (as below). We will work with data from New York’s Citibike scheme for June 2020. A list of all cities covered by the bikedata package is below:\nbike_cities() ## city city_name bike_system ## 1 bo Boston Hubway ## 2 ch Chicago Divvy ## 3 dc Washington DC CapitalBikeShare ## 4 gu Guadalajara mibici ## 5 la Los Angeles Metro ## 6 lo London Santander ## 7 mo Montreal Bixi ## 8 mn Minneapolis NiceRide ## 9 ny New York Citibike ## 10 ph Philadelphia Indego ## 11 sf Bay Area FordGoBike In the template there are code chunks demonstrating how to download and process these data using bikedata’s API. This is mainly for illustrative purposes and the code chunks take some time to execute. We ultimately use the fst package for serializing and reading in the these data. So I suggest you ignore the import code and calls to the bikedata API and instead follow the instructions for downloading and reading in the .fst file with the trips data and also the .csv  file containing stations data, with:\n# Create subdirectory in data folder for storing bike data. if(!dir.exists(here(\u0026quot;data\u0026quot;, \u0026quot;bikedata\u0026quot;))) dir.create(here(\u0026quot;data\u0026quot;, \u0026quot;bikedata\u0026quot;)) # Read in .csv file of stations data from url. tmp_file \u0026lt;- tempfile() url \u0026lt;- \u0026quot;https://www.roger-beecham.com/datasets/ny_stations.csv\u0026quot; curl::curl_download(url, tmp_file, mode=\u0026quot;wb\u0026quot;) ny_stations \u0026lt;- read_csv(tmp_file) # Read in .fst file of trips data from url. tmp_file \u0026lt;- tempfile() cs_url \u0026lt;- \u0026quot;https://www.roger-beecham.com/datasets/ny_trips.fst\u0026quot; curl::curl_download(url, tmp_file, mode=\u0026quot;wb\u0026quot;) ny_trips \u0026lt;- read_fst(tmp_file) # Write out to subdirectory for future use. write_fst(trips, here(\u0026quot;data\u0026quot;, \u0026quot;ny_trips.fst\u0026quot;)) write_csv(stations, here(\u0026quot;data\u0026quot;, \u0026quot;ny_stations.csv\u0026quot;)) # Clean workspace. rm(url, tmp_file)  fst implements in the background various operations such as multi-threading to reduce load on disk space. It therefore makes it possible to work with large datasets in-memory in R rather than connecting to a database and serving up summaries/subsets to be loaded into R. We will be working with just 2 million records, but with fst it is possible to work in-memory with much larger datasets – in Lovelace et al. (2020) we ended up working with 80 million + trip records.   If you completed the reading and research from the Session 1 Homework, some of the above should be familiar to you. The key arguments to look at are read_csv() and read_fst(), into which we pass the path to the file. In this case we created a tmpfile() within the R session. We then write these data out and save locally to the project’s data folder. This is useful as we only want to download the data once. In the write_*\u0026lt;\u0026gt; functions we reference this location using the here package’s here() function. here is really useful for reliably creating paths relative to your project’s root. To read in these data for future sessions:\n# Read in these local copies of the trips and stations data. ny_trips \u0026lt;- read_fst(here(\u0026quot;data\u0026quot;, \u0026quot;ny_trips.fst\u0026quot;)) ny_stations \u0026lt;- read_csv(here(\u0026quot;data\u0026quot;, \u0026quot;ny_stations.csv\u0026quot;)) Notice that we use assignment here (\u0026lt;-) so that these data are loaded as objects and appear in the Environment pane of your RStudio window. An efficient description of data import with read_csv() is also in Chapter 11 of Wickham and Grolemund (2017).\nny_stations and ny_trips are data frames, spreadsheet type representations containing observations in rows and variables in columns. Inspecting the layout of the stations data with View(ny_stations) you will notice that the top line is the header and contains column (variable) names.  Figure 1: ny_trips and ny_stations as they appear when calling View().   Describe There are several functions for generating a quick overview of a data frame’s contents. glimpse\u0026lt;dataset-name\u0026gt; is particularly useful. It provides a summary of the data frame dimensions – we have c. 1.9 million trip observations in ny_trips and 11 variables1. The function also prints out the object type for each of these variables, with the variables either of type int or chr in this case.\nglimpse(ny_trips) ## Rows: 1,882,273 ## Columns: 11 ## $ id \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21… ## $ city \u0026lt;chr\u0026gt; \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;… ## $ trip_duration \u0026lt;dbl\u0026gt; 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 69… ## $ start_time \u0026lt;chr\u0026gt; \u0026quot;2020-06-01 00:00:03\u0026quot;, \u0026quot;2020-06-01 00:00:03\u0026quot;, \u0026quot;2020-06-01 00:00:09\u0026quot;, \u0026quot;202… ## $ stop_time \u0026lt;chr\u0026gt; \u0026quot;2020-06-01 00:17:46\u0026quot;, \u0026quot;2020-06-01 01:03:33\u0026quot;, \u0026quot;2020-06-01 00:17:06\u0026quot;, \u0026quot;202… ## $ start_station_id \u0026lt;chr\u0026gt; \u0026quot;ny3419\u0026quot;, \u0026quot;ny366\u0026quot;, \u0026quot;ny389\u0026quot;, \u0026quot;ny3255\u0026quot;, \u0026quot;ny367\u0026quot;, \u0026quot;ny248\u0026quot;, \u0026quot;ny3232\u0026quot;, \u0026quot;ny3263… ## $ end_station_id \u0026lt;chr\u0026gt; \u0026quot;ny3419\u0026quot;, \u0026quot;ny336\u0026quot;, \u0026quot;ny3562\u0026quot;, \u0026quot;ny505\u0026quot;, \u0026quot;ny497\u0026quot;, \u0026quot;ny247\u0026quot;, \u0026quot;ny390\u0026quot;, \u0026quot;ny496\u0026quot;,… ## $ bike_id \u0026lt;chr\u0026gt; \u0026quot;39852\u0026quot;, \u0026quot;37558\u0026quot;, \u0026quot;37512\u0026quot;, \u0026quot;39674\u0026quot;, \u0026quot;21093\u0026quot;, \u0026quot;39594\u0026quot;, \u0026quot;43315\u0026quot;, \u0026quot;16571\u0026quot;, \u0026quot;… ## $ user_type \u0026lt;chr\u0026gt; \u0026quot;Customer\u0026quot;, \u0026quot;Subscriber\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Subscriber… ## $ birth_year \u0026lt;chr\u0026gt; \u0026quot;1997\u0026quot;, \u0026quot;1969\u0026quot;, \u0026quot;1988\u0026quot;, \u0026quot;1969\u0026quot;, \u0026quot;1997\u0026quot;, \u0026quot;1990\u0026quot;, \u0026quot;1938\u0026quot;, \u0026quot;1995\u0026quot;, \u0026quot;1971\u0026quot;, \u0026quot;… ## $ gender \u0026lt;dbl\u0026gt; 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… glimpse(ny_stations) ## Rows: 1,010 ## Columns: 6 ## $ id \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2… ## $ city \u0026lt;chr\u0026gt; \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;ny\u0026quot;, \u0026quot;n… ## $ stn_id \u0026lt;chr\u0026gt; \u0026quot;ny116\u0026quot;, \u0026quot;ny119\u0026quot;, \u0026quot;ny120\u0026quot;, \u0026quot;ny127\u0026quot;, \u0026quot;ny128\u0026quot;, \u0026quot;ny143\u0026quot;, \u0026quot;ny144\u0026quot;, \u0026quot;ny146\u0026quot;, \u0026quot;ny150\u0026quot;,… ## $ name \u0026lt;chr\u0026gt; \u0026quot;W 17 St \u0026amp; 8 Ave\u0026quot;, \u0026quot;Park Ave \u0026amp; St Edwards St\u0026quot;, \u0026quot;Lexington Ave \u0026amp; Classon Ave\u0026quot;, \u0026quot;B… ## $ longitude \u0026lt;chr\u0026gt; \u0026quot;-74.00149746\u0026quot;, \u0026quot;-73.97803415\u0026quot;, \u0026quot;-73.95928168\u0026quot;, \u0026quot;-74.00674436\u0026quot;, \u0026quot;-74.00297088\u0026quot;, … ## $ latitude \u0026lt;chr\u0026gt; \u0026quot;40.74177603\u0026quot;, \u0026quot;40.69608941\u0026quot;, \u0026quot;40.68676793\u0026quot;, \u0026quot;40.73172428\u0026quot;, \u0026quot;40.72710258\u0026quot;, \u0026quot;40.6…   Table 8: A breakdown of data types in R.    Type  Description      lgl  Logical – vectors that can contain only TRUE or FALSE values    int  Integers – whole numbers    dbl  Double – real numbers with decimals    chr  Character – text strings    dttm  Date-times – a date + a time    fctr  Factors – represent categorical variables of fixed and potentially orderable values     The object type of a variable in a data frame relates to that variable’s measurement level. It is often useful to convert to types with greater specificity. For example, we may which to convert the start_time and stop_time variables to a date-time format so that various time-related functions could be used. For efficient storage, we may wish to convert the station identifier variables as int types by removing the redundant “ny” text which prefaces end_station_id, end_station_id, stn_id. The geographic coordinates are currently stored as type chr. These could be regarded as quantitative variables, floating points with decimals. So converting to type dbl or as a POINT geometry type (more on this later in the module) may be sensible.\nIn the  02-template.Rmd file there are code chunks for doing these conversions. There are some slightly more involved data transform procedures in this code. Don’t fixate too much on these, but the upshot can be seen when running glimpse() on the converted data frames:\nglimpse(ny_trips) ## Rows: 1,882,273 ## Columns: 10 ## $ id \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2… ## $ trip_duration \u0026lt;dbl\u0026gt; 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 695, 206,… ## $ start_time \u0026lt;dttm\u0026gt; 2020-06-01 00:00:03, 2020-06-01 00:00:03, 2020-06-01 00:00:09, 2020-06-01 00:00… ## $ stop_time \u0026lt;dttm\u0026gt; 2020-06-01 00:00:03, 2020-06-01 00:00:03, 2020-06-01 00:00:09, 2020-06-01 00:00… ## $ start_station_id \u0026lt;int\u0026gt; 3419, 366, 389, 3255, 367, 248, 3232, 3263, 390, 319, 237, 3630, 3610, 3708, 465… ## $ end_station_id \u0026lt;int\u0026gt; 3419, 336, 3562, 505, 497, 247, 390, 496, 3232, 455, 3263, 3630, 3523, 3740, 379… ## $ bike_id \u0026lt;int\u0026gt; 39852, 37558, 37512, 39674, 21093, 39594, 43315, 16571, 28205, 41760, 30745, 380… ## $ user_type \u0026lt;chr\u0026gt; \u0026quot;Customer\u0026quot;, \u0026quot;Subscriber\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Customer\u0026quot;, \u0026quot;Subscriber\u0026quot;, \u0026quot;Sub… ## $ birth_year \u0026lt;int\u0026gt; 1997, 1969, 1988, 1969, 1997, 1990, 1938, 1995, 1971, 1989, 1990, 1969, 1984, 19… ## $ gender \u0026lt;chr\u0026gt; \u0026quot;female\u0026quot;, \u0026quot;unknown\u0026quot;, \u0026quot;female\u0026quot;, \u0026quot;unknown\u0026quot;, \u0026quot;female\u0026quot;, \u0026quot;male\u0026quot;, \u0026quot;female\u0026quot;, \u0026quot;female\u0026quot;, … glimpse(ny_stations) ## Rows: 1,010 ## Columns: 5 ## $ id \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, … ## $ stn_id \u0026lt;int\u0026gt; 116, 119, 120, 127, 128, 143, 144, 146, 150, 151, 157, 161, 164, 167, 168, 173, 174, 19… ## $ name \u0026lt;chr\u0026gt; \u0026quot;W 17 St \u0026amp; 8 Ave\u0026quot;, \u0026quot;Park Ave \u0026amp; St Edwards St\u0026quot;, \u0026quot;Lexington Ave \u0026amp; Classon Ave\u0026quot;, \u0026quot;Barrow S… ## $ longitude \u0026lt;dbl\u0026gt; -74.00150, -73.97803, -73.95928, -74.00674, -74.00297, -73.99338, -73.98069, -74.00911,… ## $ latitude \u0026lt;dbl\u0026gt; 40.74178, 40.69609, 40.68677, 40.73172, 40.72710, 40.69240, 40.69840, 40.71625, 40.7208…  Transform Transform with dplyr   Table 9: dplyr funcitions (verbs) for manipulating data frames.    function()  Description      filter()  Picks rows (observations) if their values match a specified criteria    arrange()  Reorders rows (observations) based on their values    select()  Picks a subset of columns (variables) by name (or name characteristics)    rename()  Changes the name of columns in the data frame    mutate()  Adds new columns (or variables)    group_by()  Chunks the dataset into groups for grouped operations    summarise()  Calculate single-row (non-grouped) or multiple-row (if grouped) summary values    ..and more      dplyr is one of the most important packages for supporting modern data analysis workflows. The package provides a grammar of data manipulation, with access to functions that can be variously combined to support most data processing and transformation activity. Once you become familiar with dplyr functions (or verbs) you will find yourself generating analysis templates to re-use whenever you work on a new dataset.\nAll dplyr functions work in the same way:\nStart with a data frame. Pass some arguments to the function which control what you do to the data frame. Return the updated data frame.  So every dplyr function expects a data frame and will always return a data frame.\n Use pipes %\u0026gt;% with dplyr dplyr is most effective when its functions are chained together – you will see this shortly as we explore the New York bikeshare data. This chaining of functions can be achieved using the pipe operator (%\u0026gt;%). Pipes are used for passing information in a program. They take the output of a set of code (a dplyr specification) and make it the input of the next set (another dplyr specification).\nPipes can be easily applied to dplyr functions, and the functions of all packages that form the tidyverse. I mentioned in Session 1 that ggplot2 provides a framework for specifying a layered grammar of graphics (more on this in Session 3). Together with the pipe operator (%\u0026gt;%), dplyr supports a layered grammar of data manipulation.\n count() rows This might sound a little abstract so let’s use and combine some dplyr functions to generate some statistical summaries on the New York bikeshare data.\nFirst we’ll count the number of trips made in Jun 2020 by gender. dplyr has a convenience function for counting, so we could run the code below, also in the  02-template.Rmd for this session. I’ve commented the code block to convey what each line achieves.\nny_trips %\u0026gt;% # Take the ny_trips data frame. count(gender, sort=TRUE) # Run the count function over the data frame and set the sort parameter to TRUE. ## gender n ## 1 male 1044621 ## 2 female 586361 ## 3 unknown 251291 There are a few things happening in the count() function. It takes the gender variable from ny_trips, organises or groups the rows in the data frame according to its values (female | male | unknown), counts the rows and then orders the summarised output descending on the counts.\n summarise() over rows   Table 10: A breakdown of aggregate functions commonly used with summarise().    Function  Description      n()  Counts the number of observations    n_distinct(var)  Counts the number of unique observations    sum(var)  Sums the values of observations    max(var)|min(var)  Finds the min|max values of observations    mean(var)|median(var)|sd(var)| ...  Calculates central tendency of observations    ...  Many more     Often you will want to do more than simply counting and you may also want to be more explicit in the way the data frame is grouped for computation. We’ll demonstrate this here with a more involved analysis of the usage data and using some key aggregate functions (Table 10).\nA common workflow is to combine group_by() and summarise(), and in this case arrange() to replicate the count() example.\nny_trips %\u0026gt;% # Take the ny_trips data frame. group_by(gender) %\u0026gt;% # Group by gender. summarise(count=n()) %\u0026gt;% # Count the number of observations per group. arrange(desc(count)) # Arrange the grouped and summarised (collapsed) rows according to count. ## # A tibble: 3 x 2 ## gender count ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 male 1044621 ## 2 female 586361 ## 3 unknown 251291 In ny_trips there is a variable measuring trip duration in seconds (trip_duration) and distinguishing casual users from those formally registered to use the scheme (user_type - Customer vs. Subscriber). It may be instructive to calculate some summary statistics to see how trip duration varies between these groups.\nThe code below uses group_by(), summarise() and arrange() in exactly the same way, but with the addition of other aggregate functions profiles the trip_duration variable according to central tendency and by user_type.\nny_trips %\u0026gt;% # Take the ny_trips data frame. group_by(user_type) %\u0026gt;% # Group by user type. summarise( # Summarise over the grouped rows, generate a new variable for each type of summary. count=n(), avg_duration=mean(trip_duration/60), median_duration=median(trip_duration/60), sd_duration=sd(trip_duration/60), min_duration=min(trip_duration/60), max_duration=max(trip_duration/60) ) %\u0026gt;% arrange(desc(count)) # Arrange on the count variable. ## # A tibble: 2 x 6 ## user_type count avg_duration median_duration sd_duration min_duration max_duration ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Subscriber 1306688 20.2 14.4 110. 1.02 33090 ## 2 Customer 575585 43.6 23.2 393. 1.02 46982 Clearly there are some outlier trips that may need to be examined. Bikeshare schemes are built to incentivise short journeys of \u0026lt;30 minutes, but the maximum trip duration recorded above is clearly erroneous – 32 days. Ignoring these sorts of outliers by calculating the trip durations at the 95th percentiles is instructive. The max trip duration at the 95th percentile for Subscribers was almost 27 minutes and for Customers was 1 hours 26 mins. It makes sense that more casual users may have longer trip durations, as they are more likely to be tourists or occasional cyclists using the scheme for non-utility trips. However, they do skew the mean travel time.\nReturning to the breakdown of usage by gender, an interesting question is whether or not the male-female split in bikehare is similar to that of the cycling population of New York City as a whole. This might tell us something about whether the bikeshare scheme could be representative of wider cycling. This could be achieved with the code below. A couple of new additions: we use filter(), to remove observations where the gender of the cyclist is unknown. We also use mutate() for the first time, which allows us to modify or create new variables.\nny_trips %\u0026gt;% # Take the ny_trips data frame. filter(gender != \u0026quot;unknown\u0026quot;) %\u0026gt;% # Filter out rows with the value \u0026quot;unknown\u0026quot; on gender. group_by(gender) %\u0026gt;% # Group by gender. summarise(count=n()) %\u0026gt;% # Count the number of observations per group. mutate(prop=count/sum(count)) %\u0026gt;% # Add a new column called `prop`, divide the value in the row for the variable count by the sum of the count variable across all rows. arrange(desc(count)) # Arrange on the count variable. ## # A tibble: 2 x 3 ## gender count prop ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 male 1044621 0.640 ## 2 female 586361 0.360 As I’ve commented each line you hopefully get a sense of what is happening in the code above. I mentioned that dplyr functions read like verbs. This is a very deliberate design decision. With the code laid out as above – each dplyr verb occupying a single line, separated by a pipe (%\u0026gt;%) – you can generally understand the code with a cursory glance. There are obvious benefits to this. Once you become familiar with dplyr it becomes very easy to read, write and share code.\n Remembering that pipes (%\u0026gt;%) take the output of a set of code and make it the input of the next set, what do you think would happen if you were to comment out the call to arrange() in the code block above? Try it for yourself. You will notice that I use separate lines for each call to the pipe operator. This is good practice for supporting readibility of your code.    Manipulate dates with lubridate Let’s continue this investigation of usage by gender, and whether bikeshare might be representative of regular cycling, by profiling how usage varies over time. To do this we will need to work with date-time variables. The lubridate package provides various convenience functions for this.\nIn the code block below we extract the day of week and hour of day from the start_time variable using lubridate’s day accessor functions. Documentation on these can be accessed in the usual way (?\u0026lt;function-name\u0026gt;), but reading down the code it should be clear to you how this works. Next we count the number of trips made by hour of day, day of week and gender. The summarised data frame will be re-used several times in our analysis, so we store it as an object with a suitable name (ny_temporal) using the assignment operator.\n# Create a hod dow summary by gender and assign it the name \u0026quot;ny_temporal\u0026quot;. ny_temporal \u0026lt;- ny_trips %\u0026gt;% # Take the ny_trips data frame. mutate( day=wday(start_time, label=TRUE), # Create a new column identify dow. hour=hour(start_time)) %\u0026gt;% # Create a new column identify hod. group_by(gender, day, hour) %\u0026gt;% # Group by day, hour, gender. summarise(count=n()) %\u0026gt;% # Count the grouped rows. ungroup()  Whether or not to store derived data tables, like the newly assigned ny_temporal, in a session is not an easy decision. You want to try to avoid cluttering your Environment pane with many data objects. Often when generating charts it is necessary to create these sorts of derived tables as input data (to ggplot2) – and so when doing visual data analysis you may end up with an unhelpfully large number of these derived tables. The general rule I apply: if the derived table is to be used \u0026gt;3 times in a data analysis or is computationally intensive, assign it (\u0026lt;-) to an object.   In Figure 2 below these derived data are plotted. The template contains ggplot2 code for creating the graphic. Don’t obsess too much on it – more on this next session. The plot demonstrates a familiar weekday-weekend pattern of usage. Trip frequencies peak in the morning and evening rush hours during weekdays and mid/late-morning and afternoon during weekends. This is consistent with typical travel behaviour. Notice though that the weekday afternoon peak is much larger than the morning peak. There are several speculative explanations for this and re-running the plot on Subscriber users only may be instructive. A secondary observation is that whilst men and women share this overall pattern of usage, the relative number of trips taken by each day of week varies. Men make many more trips at peak times during the start of the week than they do later in the week. The same pattern does not appear for women. This is certainly something to follow up on, for example by collecting data over a longer period of time.\n Figure 2: Line charts generated with ggplot2. Plot data computed using dplyr and lubridate.   Our analysis is based on data from June 2020, a time when New York residents were emerging from lockdown. It would be instructive to compare with data from a non-Covid year. If there is a very clear contrast in usage between this data and a control (non-Covid) year, this suggests bikeshare data may be used for monitoring behavioural change. The fact that bikeshare is collected continuously makes this possible. Check out Jo Wood’s current work analysing Covid-related change in movement behaviours across a range of cities.    Relate tables with join() Trip distance is not recorded directly in the ny_trips table, but may be important for profiling usage behaviour. Calculating trip distance is eminently achievable as the ny_trips table contains the origin and destination station of every trip and the ny_stations table contains coordinates corresponding to those stations. To relate the two tables, we need to specify a join between them.\nA sensible approach is to:\nSelect all uniquely cycled trip pairs (origin-destination pairs) that appear in the ny_trips table. Bring in the corresponding coordinate pairs representing the origin and destination stations by joining on the ny_stations table. Calculate the distance between the coordinate pairs representing the origin and destination.  The code below is one way of achieving this.\nod_pairs \u0026lt;- ny_trips %\u0026gt;% # Take the ny_trips data frame. select(start_station_id, end_station_id) %\u0026gt;% unique() %\u0026gt;% # Select trip origin and destination (OD) station columns and extract unique OD pairs. left_join(ny_stations %\u0026gt;% select(stn_id, longitude, latitude), by=c(\u0026quot;start_station_id\u0026quot;=\u0026quot;stn_id\u0026quot;)) %\u0026gt;% # Select lat, lon columns from ny_stations and join on the origin column. rename(o_lon=longitude, o_lat=latitude) %\u0026gt;% # Rename new lat, lon columns -- associate with origin station. left_join(ny_stations %\u0026gt;% select(stn_id, longitude, latitude), by=c(\u0026quot;end_station_id\u0026quot;=\u0026quot;stn_id\u0026quot;)) %\u0026gt;% # Select lat, lon columns from ny_stations and join on the destination column. rename(d_lon=longitude, d_lat=latitude) %\u0026gt;% # Rename new lat, lon columns -- associate with destination station. rowwise() %\u0026gt;% # For computing distance calculation one row-at-a-time. mutate(dist=geosphere::distHaversine(c(o_lat, o_lon), c(d_lat, d_lon))/1000) %\u0026gt;% # Calculate distance and express in kms. ungroup() The code block above introduces some new functions: select() to pick or drop variables, rename() to rename variables and a convenience function for calculating straight line distance from polar coordinates (distHaversine()). The key function to emphasise is the left_join(). If you’ve worked with relational databases and SQL, dplyr’s join functions will be familiar to you. In a left_join, all the values from the main table are retained, the one on the left – ny_trips, and variables from the table on the right (ny_stations) are added. We specify explicitly the variable on which the tables should be joined with the by= parameter, station_id in this case. If there is a station_id in ny_trips that doesn’t exist in ny_stations then NA is returned.\nOther join functions provided by dplyr are in the table below. Rather than discussing each, I recommend consulting Chapter 13 of Wickham and Grolemund (2017).\n   *_join(x, y) ...       Table 11: A breakdown of dplyr join functions.    left_join()  all rows from x    right_join()  all rows from y    full_join()  all rows from both x and y    semi_join()  all rows from x where there are matching values in y, keeping just columns from x    inner_join()  all rows from x where there are matching values in y, return all combination of multiple matches in the case of multiple matches    anti_join  return all rows from x where there are not matching values in y, never duplicate rows of x      Figure 3: Histograms generated with ggplot2. Plot data computed using dplyr and lubridate  From the newly created distance variable, we can calculate the average (mean) trip distance for the 1.9m trips – 1.6km. This might seem very short, but remember that the distance calculation is problematic in that these are straight-line distances between pairs of docking stations. Really we should be calculating network distances derived from the cycle network in New York. A separate reason – discovered when generating a histogram on the dist variable – is that there are a large number of trips (124,403) that start and end at the same docking station. Initially these might seem to be unsuccessful hires – people failing to undock a bike for example. We could investigate this further by paying attention to the docking stations at which same origin-destination trips occur, as in the code block below.\nny_trips %\u0026gt;% filter(start_station_id==end_station_id) %\u0026gt;% group_by(start_station_id) %\u0026gt;% summarise(count=n()) %\u0026gt;% left_join(ny_stations %\u0026gt;% select(stn_id, name), by=c(\u0026quot;start_station_id\u0026quot;=\u0026quot;stn_id\u0026quot;)) %\u0026gt;% arrange(desc(count)) ## # A tibble: 958 x 3 ## start_station_id count name ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 ny3423 2017 West Drive \u0026amp; Prospect Park West ## 2 ny3881 1263 12 Ave \u0026amp; W 125 St ## 3 ny514 1024 12 Ave \u0026amp; W 40 St ## 4 ny3349 978 Grand Army Plaza \u0026amp; Plaza St West ## 5 ny3992 964 W 169 St \u0026amp; Fort Washington Ave ## 6 ny3374 860 Central Park North \u0026amp; Adam Clayton Powell Blvd ## 7 ny3782 837 Brooklyn Bridge Park - Pier 2 ## 8 ny3599 829 Franklin Ave \u0026amp; Empire Blvd ## 9 ny3521 793 Lenox Ave \u0026amp; W 111 St ## 10 ny2006 782 Central Park S \u0026amp; 6 Ave ## # … with 948 more rows All of the top 10 docking stations are either in parks, near parks or located along river promenades. This coupled with the fact that these trips occur in much greater relative number for casual than regular users (Customer vs Subscriber) is further evidence that these are valid trips.\n Write functions of your own Through most of the module we will be making use of functions written by others – mainly those developed for packages that form the tidyverse and therefore that follow a consistent syntax. However, there may be times where you need to abstract over some of your code to make functions of your own. Chapter 19 of Wickham and Grolemund (2017) presents some helpful guidelines around the circumstances under which the data scientist typically tends to write functions. Most often this is when you find yourself copy and pasting the same chunks of code with minimal adaptation.\nFunctions have three key characteristics:\nThey are (usually) named – the name should be expressive and communicate what the function does (we talk about dplyr verbs). They have brackets \u0026lt;function()\u0026gt; usually containing arguments – inputs which determine what the function does and returns. Immediately followed by \u0026lt;function()\u0026gt; are {} used to contain the body – in this is code that performs a distinct task, described by the function’s name.  Effective functions are short, perform single discrete operations and are intuitive.\nYou will recall that in the ny_trips table there is a variable called birth_year. From this we can derive cyclists’ approximate age. Below I have written a function get_age() for doing this. The function expects two arguments: yob – a year of birth as type chr; yref – a reference year. In the body, lubridate’s as.period function is used to calculate the time in years that elapsed, the value that the function returns. Once defined, and loaded into the session by being executed, it can be used (as below).\n# Function for calculating time elapsed between two dates in years (age). get_age \u0026lt;- function(yob, yref) { period \u0026lt;- lubridate::as.period(lubridate::interval(yob, yref),unit = \u0026quot;year\u0026quot;) return(period$year) } ny_trips \u0026lt;- ny_trips %\u0026gt;% # Take the ny_trips data frame. mutate( age=get_age(as.POSIXct(birth_year, format=\u0026quot;%Y\u0026quot;), as.POSIXct(\u0026quot;2020\u0026quot;, format=\u0026quot;%Y\u0026quot;)) # Calculate age from birth_date. ) We can use the two new derived variables – distance travelled and age – in our analysis. In Figure 4, we explore how approximate travel speeds vary by age, gender and trip distance. The code used to generate the summary data and plot is in the template file. Again the average “speed” calculation should be treated very cautiously as it is based on straight line distances and it is very difficult to select out “utility” from “leisure” trips. I have tried to do the latter by selecting trips that occur only on weekdays and that are made by Subscriber cyclists. Additionally, due to the heavy subsetting data become a little volatile for certain age groups and so I’ve aggregated the age variable into 5-year bands. Collecting more data is probably a good idea.\nThere are nevertheless some interesting patterns. Men tend to cycle at faster speeds than do women, although this gap narrows with the older age groups. The effect of age on speed cycled is more apparent for the longer trips. This trend is reasonably strong, although the volatility in the older age groups for trips \u0026gt;4.5km suggests we probably need more data and a more involved analysis to establish this. For example, it may be that the comparatively rare occurrence of trips in the 65-70 age group is made by only a small subset of cyclists. A larger dataset would result in a regression to the mean effect and negate any noise caused by outlier individuals. Certainly Figure 1 is an interesting data graphic – and the type of exploratory analysis demonstrated here, using dplyr functions, is most definitely consistent with that identified in the previous session when introducing Geographic Data Science.\n Figure 4: Line charts generated with ggplot2. Plot data computed using dplyr and lubridate    Tidy The ny_trips and ny_stations data already comply with the rules for tidy data (Wickham 2014). Each row in ny_trips is a distinct trip and each row in ny_stations a distinct station. However throughout the module we will undoubtedly encounter datasets that need to be reshaped. There are two key functions to learn here, made available via the tidyr package: pivot_longer() and pivot_wider(). pivot_longer() is used to tidy data in which observations are spread across columns, as in Table 6 (the gapminder dataset). pivot_wider() is used to tidy data in which observations are spread across rows, as in Table 7 (the gapminder dataset). You will find yourself using these functions, particularly pivot_longer(), not only for fixing messy data, but for flexibly reshaping data for use in ggplot2 specifications (more on this in sessions 3 and 4) or joining tables.\nA quick breakdown of pivot_longer:\npivot_longer( data, cols, # Columns to pivot longer (across rows). names_to=\u0026quot;name\u0026quot;, # Name of the column to create from values held in spread *column names*. values_to=\u0026quot;name\u0026quot; # Name of column to create form values stored in spread *cells* ) A quick breakdown of pivot_wider:\npivot_wider( data, names_from, # Column in the long format which contains what will be column names in the wide format. values_from # Column in the long format which contains what will be values in the new wide format. ) In the homework you will be tidying some messy derived tables based on the bikeshare data using both of these functions, but we can demonstrate their purpose in tidying the messy gapminder data in Table 7. Remember that these data were messy as the observations by gender were spread across the columns:\nuntidy_wide ## country year f_cases m_cases f_population m_population ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Afghanistan 1999 447 298 9993400 9993671 ## 2 Afghanistan 2000 1599 1067 10296280 10299080 ## 3 Brazil 1999 16982 20755 86001181 86005181 ## 4 Brazil 2000 39440 41048 87251329 87253569 ## 5 China 1999 104007 108252 636451250 636464022 ## 6 China 2000 104746 109759 640212600 640215983 First we need to gather the problematic columns with pivot_longer().\nuntidy_wide %\u0026gt;% pivot_longer(cols=c(f_cases: m_population), names_to=c(\u0026quot;gender_count_type\u0026quot;), values_to=c(\u0026quot;counts\u0026quot;)) ## country year gender_count_type counts ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Afghanistan 1999 f_cases 447 ## 2 Afghanistan 1999 m_cases 298 ## 3 Afghanistan 1999 f_population 9993400 ## 4 Afghanistan 1999 m_population 9993671 ## 5 Afghanistan 2000 f_cases 1599 ## 6 Afghanistan 2000 m_cases 1067 ## 7 Afghanistan 2000 f_population 10296280 ## 8 Afghanistan 2000 m_population 10299080 ## 9 Brazil 1999 f_cases 16982 ## 10 Brazil 1999 m_cases 20755 ## # … with 14 more rows So this has usefully collapsed the dataset by gender, we now have a problem similar to that in Table 6 where observations are spread across the rows – in this instance cases and population are better treated as separate variables. This can be fixed by separating the gender_count_type variables and then spreading the values of the new count_type (cases, population) across the columns. Hopefully you can see how this gets us to the tidy gapminder data structure in Table 5\nuntidy_wide %\u0026gt;% pivot_longer(cols=c(f_cases: m_population), names_to=c(\u0026quot;gender_count_type\u0026quot;), values_to=c(\u0026quot;counts\u0026quot;)) %\u0026gt;% separate(col=gender_count_type, into=c(\u0026quot;gender\u0026quot;, \u0026quot;count_type\u0026quot;), sep=\u0026quot;_\u0026quot;) ## country year gender count_type counts ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Afghanistan 1999 f cases 447 ## 2 Afghanistan 1999 m cases 298 ## 3 Afghanistan 1999 f population 9993400 ## 4 Afghanistan 1999 m population 9993671 ## 5 Afghanistan 2000 f cases 1599 ## 6 Afghanistan 2000 m cases 1067 ## 7 Afghanistan 2000 f population 10296280 ## 8 Afghanistan 2000 m population 10299080 ## 9 Brazil 1999 f cases 16982 ## 10 Brazil 1999 m cases 20755 ## # … with 14 more rows untidy_wide %\u0026gt;% pivot_longer(cols=c(f_cases: m_population), names_to=c(\u0026quot;gender_count_type\u0026quot;), values_to=c(\u0026quot;counts\u0026quot;)) %\u0026gt;% separate(col=gender_count_type, into=c(\u0026quot;gender\u0026quot;, \u0026quot;count_type\u0026quot;), sep=\u0026quot;_\u0026quot;) %\u0026gt;% pivot_wider(names_from=count_type, values_from=counts) ## country year gender cases population ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Afghanistan 1999 f 447 9993400 ## 2 Afghanistan 1999 m 298 9993671 ## 3 Afghanistan 2000 f 1599 10296280 ## 4 Afghanistan 2000 m 1067 10299080 ## 5 Brazil 1999 f 16982 86001181 ## 6 Brazil 1999 m 20755 86005181 ## 7 Brazil 2000 f 39440 87251329 ## 8 Brazil 2000 m 41048 87253569 ## 9 China 1999 f 104007 636451250 ## 10 China 1999 m 108252 636464022 ## 11 China 2000 f 104746 640212600 ## 12 China 2000 m 109759 640215983   Conclusions Developing the vocabulary and technical skills to systematically describe and organise data is crucial to modern data analysis. This session has covered the fundamentals here: that data consist of observations and variables of different types (Stevens 1946) and that in order to work effectively with datasets, especially in a functional way in R, these data must be organised according to the rules of tidy data (Wickham 2014). Most of the session content was dedicated to the techniques that enable these concepts to be operationalised. We covered how to download, transform and reshape a reasonably large set of data from New York’s Citibike scheme. In doing so, we generated insights that might inform further data collection and analysis activity. In the next session we will apply and extend this conceptual and technical knowledge as we introduce the fundamentals of visual data analysis and ggplot2’s grammar of graphics.\n References Lovelace, R., R. Beecham, E. Heinen, E. Vidal Tortosa, Y. Yuanxuan, C. Slade, and A. Roberts. 2020. “Is the London Cycle Hire Scheme Becoming More Inclusive? An Evaluation of the Shifting Spatial Distribution of Uptake Based on 70 Million Trips.” Transportation Research Part A: Policy and Practice 140 (October): 1–15.  Stevens, S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80.  Wickham, H. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23.  Wickham, H., and G. Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. Sebastopol, California: O’Reilly Media.     Note that the version of the trips data downloaded from my external repo contains a sample of just 500k records – this is not ideal, but was due to data storage limits on my external repo.↩︎\n   ","date":1652659200,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1656086209,"objectID":"9ae245330f59c57e6f62568d635c26e4","permalink":"/class/02-class/","publishdate":"2022-05-16T00:00:00Z","relpermalink":"/class/02-class/","section":"class","summary":"Contents  Session outcomes Introduction Concepts  Data structure Types of variable Types of observation Tidy data  Techniques  Import Describe Transform Tidy  Conclusions References   Session outcomes By the end of this session you should gain the following knowledge:\n  Learn the vocabulary and concepts used to describe data. Appreciate the characteristics and importance of tidy data (Wickham 2014) for data processing and analysis.","tags":null,"title":"Data fundamentals: Describe, wrangle, tidy","type":"docs"},{"authors":null,"categories":null,"content":" Contents  Session outcomes Welcome to Visualization for Geographic Data Science Why vis-for-gds?  Geographic Data Science Geographic Data Science and Visualization  What vis-for-gds? How vis-for-gds?  R for modern data analysis Rmarkdown for reproducible research  Getting started with R and RStudio  Install R and RStudio Open the RStudio IDE Compute in the console Install some packages Experiment with R Markdown R Scripts Create an RStudio Project  Conclusions References   Session outcomes By the end of this session you should gain the following knowledge:\n  Appreciate the motivation for this module – why visualization, why R and why ggplot2    By the end of this session you should gain the following practical skills:\n  Navigate the materials on this course website, having familiarised yourself with its structure Open R using the RStudio Integrated Developer Environment (IDE) Install and enable R packages and query package documentation Perform basic calculations via the R Console Render R Markdown files Create R Projects Read-in datasets from external resources as objects (specifically tibbles)     Welcome to Visualization for Geographic Data Science Welcome to Visualization for Geographic Data Science (vis-for-gds). In this first session we’ll cover the background to the module – the why, what and how of vis-for-gds. If you’ve not already you should check out the course outline on the Syllabus page for an overall preview of the module.\nThe main home for this module is this website. However, via Minerva you can access the Module Handbook. You will use submission boxes in Minerva to upload coursework in the usual way.\n Why vis-for-gds? Geographic Data Science It is now taken-for-granted that over the last decade or so new data, new technology and new ways of doing science have transformed how we approach the world’s problems. Evidence for this can be seen in the response to the Covid-19 pandemic. Enter Covid19 github into a search and you’ll be confronted with hundreds of repositories demonstrating how an ever-expanding array of data related to the pandemic can be collected, processed and analysed. Data Science is a term used widely to capture this shift and Geographic Data Science (GDS), probably first discussed coherently by Arribas-Bel and Reades (2018) and Singleton and Arribas-Bel (2019), when observing that many of data science’s applications are – or at least should be – of inherent interest to geographers.\nSince gaining traction in the corporate world, the definition of Data Science has been somewhat stretched, but it has its origins in the work of John Tukey’s The Future of Data Analysis (1962). Drawing on this, and a survey of more recent work, Donoho (2017) neatly identifies six key facets that a data science discipline might encompass 1 :\ndata gathering, preparation, and exploration; data representation and transformation; computing with data; data visualization and presentation; data modelling; and a more introspective “science about data science”   Geographic Data Science and Visualization Visualization is fundamental to meeting the unprecedented challenges and exploiting the wonderful opportunities of the ever-expanding deluge of data confronting virtually every field.\" \\  -- Prof. Jim Hollan of UC San Diego --  Visual approaches to data analysis are particularly suited to Geographic Data Science applications because where datasets are being repurposed for social and natural sciences research for the first time, contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone and so where the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance.   -- Glancing at this list, visualization could be interpreted as a single facet of Data Science process 2 – something that happens after data gathering, preparation, exploration, but before modelling. In this module you’ll learn that visualization is intrinsic to, and should inform, each of these activities, especially so when working with data sets that are spatial – for Geographic Data Science.\nLet’s develop this idea by asking why data visualizations are used in the first place. In her book Visualization Analysis and Design, Tamara Munzner (2014) considers how humans and computers interface in the decision-making process. She makes the point that data visualization is ultimately about connecting people with data in order to make decisions – or to install humans in a ‘decision-making-loop’. There are occasionally decision-making loops that are entirely computational and where an automated solution exists and is trusted. However, most require some form of human intervention.\nThe canonical example demonstrating how relying on computation alone can be problematic, and so for the use of visualization, is Anscombe’s quartet. Here, Anscombe (1973) presents four datasets, each containing eleven observations and two variables for each observation. The data are synthetic, but let’s say that they are the weight and height of independent samples taken from a population of postgraduate students studying Data Science.\nPresented with a new dataset it makes sense to compute some summaries and doing so, we observe that the data appear identical – they contain the same means, variances and strong positive correlation coefficient. This seems appropriate since the data are measuring weight and height. Although there may be some variation, we’d expect taller students to be heavier. Given these statistical summaries we can be assured that we are drawing samples from the same population of (Data Science) students.\n Figure 1: Data from Anscombe’s quartet  Laying out the data in a meaningful way, horizontally according to weight (x) and vertically according to the height (y) to form a scatterplot, we quickly see that whilst these data contain the same statistical properties they are very different. Only dataset #1 now looks plausible if it were truly a measure of weights and heights drawn from a population of students.\nAnscombe’s is a deliberately contrived example3, but there are real cases of important structure being missed, leading to poorly specified models and potentially faulty claims.\n Figure 2: Plots of Anscombe’s quartet  This is not to undermine the importance of numerical analysis. Numeric summaries that simplify patterns are extremely useful and Statistics has at its disposal an array of tools for helping to guard against making false claims from datasets – a theme that we will return to in session 6, 7 and 8 when we think critically about the use of visual approaches for data anlysis. There remain, though, certain classes of relation and context that cannot be easily captured through statistics alone.\nGeographic context is undoubtedly challenging to capture numerically; many of the early examples of data visualization have been of spatial phenomena and generated by Geographers (see Friendly 2007). We can also probably make a special case for the use of visual approaches in Geographic Data Science (GDS) applications due to its exploratory nature. Often in GDS datasets are being repurposed for social and natural sciences research for the first time; contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone; and so the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance. In this module we will demonstrate this as we explore (Session 4 and 5), model under uncertainty (Session 6 and 7) and communicate (Session 8 and 9) with various social science datasets.\n Watch Jo Wood’s talk demonstrating how visual techniques can be used to analyse urban travel behaviours. In the video Jo argues that bikeshare schemes can help democratise cycling, but also for their potential contributions to research – he briefly contrasts new, passively collected data sets with more “traditional” actively collected data for analysing how people move around cities. A compelling case is then made for the use of visualization to support this activity. Related work and further discussion is published in Beecham and Wood (2014).     Effective data visualizations should expose structure in data that would be difficult to expose through non-visual means --   What vis-for-gds? This is a very practical module. With the exception of this Introduction, the weekly sessions will blend both theory and practical coding activity. We will cover fundamentals around visual data analysis from Information Visualization and Statistics. As you read the session materials you will be writing data processing and analysis code and so be generating analysis outputs of your own. We will also be working with real datasets – from the Political Science, Urban and Transport Planning and Health domains. So we will hopefully be generating real findings and knowledge.\nTo do this in a genuine way – to generate real knowledge from datasets – we will have to cover a reasonably broad set of data processing and analysis procedures. As well as developing expertise around designing data-rich, visually compelling graphics (of the sort demonstrated in Jo Wood’s TEDx talk), we will need to cover more tedious aspects of data processing and wrangling. Additionally, if we are to learn how to generate and communicate and make claims under uncertainty with our data graphics, then we will need to cover some aspects of estimation and modelling from Statistics. In short, we will cover most of Donoho (2017)’s six key facets of a data science discipline:\ndata gathering, preparation, and exploration (Sessions 2, 3, 5); data representation and transformation (Sessions 2, 3); computing with data (Session 2, All sessions); data visualization and presentation (All sessions); data modelling (Sessions 4, 6, 7, 8); and a more introspective “science about data science” (All sessions)  There is already a rich and impressive set of open Resources practically introducing how to do modern Data Science, Visualization and Geographic Analysis. We will certainly draw on these at different stages in the module. What makes this module different from these existing resources is that we will be doing applied data science throughout – we will be identifying and diagnosing problems when gathering data, discovering patterns (some maybe even spurious) as we do exploratory analysis, and attempt to make claims under uncertainty as we generate models based on observed patterns. We will work with both new, passively-collected datasets, as well as more traditional, actively collected datasets located within various social science domains.\n How vis-for-gds? R for modern data analysis Through the module we will apply modern approaches to data analysis. All data collection, analysis and reporting activity will be completed using R and the RStudio Integrated Development Environment (IDE). Released as open source software as part of a research project in 1995, for some time R was the preserve of academics. From 2010s onwards, the R community expanded rapidly and along with Python is regarded as the key technology for doing data analysis. R is used increasingly outside of academia, by organisations such as Google [example], Facebook [example], Twitter [example], New York Times [example], BBC [example] and many more.\nThere are many benefits that come from being fully open-source, with a critical mass of users. Firstly, there is an array of online forums, tutorials and code examples from which to learn. Second, with such a large community, there are numerous expert R users who themselves contribute by developing libraries or packages that extend its use. As a result R is employed for a very wide set of use cases – this website was even built in R using amongst other things the blogdown package.\n The key reason for our use of R is the ecosystem of users and packages that have emerged in recent years. An R package is a bundle of code, data and documentation, usually hosted on the CRAN (Comprehensive R Archive Network).   Of particular importance is the tidyverse package. This is a set of packages for doing Data Science authored by a software development team at RStudio led by Hadley Wickham. tidyverse packages share a principled underlying philosophy, syntax and documentation. Contained within the tidyverse is its data visualization package, ggplot2. This package pre-dates the tidyverse – it started as Hadley Wickham’s PhD thesis and is one of the most widely-used toolkits for generating data graphics. As with other heavily used visualization toolkits (Tableau, vega-lite) it is inspired by Leland Wilkinson’s The Grammar of Graphics, the gg in ggplot stands for Grammar of Graphics. Understanding the design principles behind the Grammar of Graphics (and tidyverse) is necessary for modern data analysis and so we will cover this in detail in Session 3.\n Rmarkdown for reproducible research  Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them.\nRoger Peng, Jeff Leek and Brian Caffo\n In recent years there has been much introspection into how science works – around how statistical claims are made from reasoning over evidence. This came on the back of, amongst other things, a high profile paper published in Science, which found that of 100 recent peer-reviewed psychology experiments, the findings of only 39 could be replicated. The upshot is that researchers must now make every possible effort to make their work transparent, such that “all aspects of the answer generated by any given analysis [can] be tested” (Brunsdon and Comber 2020).\nA reproducible research project should be accompanied with:\n code and data that allows tables and figures presented in research outputs to be regenerated code and data that does what it claims (the code works) code and data that can be justified and explained through proper documentation  If these goals are met, then it may be possible for others to use the code on new and different data to study whether the findings reported in one project might be replicated or to use the same data, but update the code to, for example, extend the original analysis (to perform a re-analysis). This model – generate findings, test for replicability in new contexts and re-analysis – is essentially how knowledge development has always worked. However, to achieve this the data and procedures on which findings were generated must be made open and transparent.\nIn this setting, traditional proprietary data analysis software such as SPSS and Esri’s ArcGIS that support point-and-click interaction is problematic. First, whilst these software may rely on the sorts of packages and libraries with bundled code that R and Python uses for implementing statistical procedures, those libraries are closed. It is not possible, and therefore less common, for the researcher to fully interrogate into the underlying processes that are being implemented and the results need to be taken more or less on faith. Second, but probably most significantly (for us), it would be tedious to make notes describing all interactions performed when working with a dataset in SPSS or ArcGIS.\nAs a declarative programming language, it is very easy to provide such a provenance trail for your workflows in R since this necessarily exists in the analysis scripts. But more importantly, the Integrated Development Environments (IDEs) through which R (and Python) are most often accessed provide notebook environments that allow users to curate reproducible computational documents that blend input code, explanatory prose and outputs. In this module we will prepare these sorts of notebooks using R Markdown.\n  Getting started with R and RStudio I mentioned that the weekly sessions will blend both theory and practical coding activity. This Introduction has been dedicated more towards conceptual and procedural matters. For the practical element this time, we want to get you configured and familiar with R and RStudio.\nInstall R and RStudio  Install the latest version of R. Note that there are installations for Windows, macOS and Linux. Run the installation from the file you downloaded (an .exe or .pkg extension). Install the latest version of RStudio Desktop. Note again that there are separate installations depedning on operating system – for Windows an .exe extension, macOS a .dmg extension.   Open the RStudio IDE  Figure 3: The RStudio IDE   Once installed, open the RStudio IDE. Open an R Script by clicking File \u0026gt; New File \u0026gt; R Script .  You should see a set of windows roughly similar to those in the Figure (although I’ve already started on some of the computing exercises in the next section). The top left pane is used either as a Code Editor (the tab named Untitled1) or data viewer. This is where you’ll write, organise and comment R code for execution or inspect datasets as a spreadsheet representation. Below this in the bottom left pane is the R Console, in which you write and execute commands directly. To the top right is a pane with the tabs Environment and History. This displays all objects – data and plot items, calculated functions – stored in-memory during an R session. In the bottom right is a pane for navigating through project directories, displaying plots, details of installed and loaded packages and documentation on their functions.\n Compute in the console You will write and execute almost all code from the code editor pane. To start though let’s use R as a calculator by typing some commands into the Console. You’ll create an object (x) and assign it a value using the assignment operator (\u0026lt;-), then perform some simple statistical calculations using functions that are held within the base package.\n The base package is core and native to R. Unlike all other packages, it does not need to be installed and called explicitly. One means of checking the package to which a function you are using belongs is to call the help command (?) on that function: e.g. ?mean().    Type the commands contained in the code block below into your R Console. Notice that since you are assigning values to each of these objects they are stored in memory and appear under the Global Environment pane.  # Create variable and assign a value. x \u0026lt;- 4 # Perform some calculations using R as a calculator. x_2 \u0026lt;- x^2 # Perform some calculations using functions that form baseR. x_root \u0026lt;- sqrt(x_2)  Install some packages There are two steps to getting packages down and available in your working environment:\ninstall.packages(\"\u0026lt;package-name\u0026gt;\") downloads the named package from a repository. library(\u0026lt;package-name\u0026gt;) makes the package available in your current session.   Install the tidyverse, the core collection of packages for doing Data Science in R, by running the code below:  install.packages(\u0026quot;tidyverse\u0026quot;) If you have little or no experience in R, it is easy to get confused around downloading and then using packages in a session. For example, let’s say we want to make use of the simple features package (sf), which we will draw on heavily in the module for performing spatial operations.\n Run the code below:  library(sf) Unless you’ve previously installed sf, you’ll probably get an error message that looks like this:\n\u0026gt; Error in library(sf): there is no package called ‘sf’ So let’s install it.\n Run the code below:  install.packages(\u0026quot;sf\u0026quot;) And now it’s installed, why not bring up some documentation on one of its functions (st_contains()).\n Run the code below:  ?st_contains() Since you’ve downloaded the package but not made it available to your session, you should get the message:\n\u0026gt; No documentation for ‘st_contains’ in specified packages and libraries So let’s try again, by first calling library(sf).\n Run the code below:  library(sf) ## Linking to GEOS 3.7.2, GDAL 2.4.1, PROJ 6.1.0 ?st_contains() Now let’s install some of the remaining core packages on which the module depends.\n Run the block below, which passes a vector of package names to the install.packages() function:  install.packages(c(\u0026quot;devtools\u0026quot;,\u0026quot;here\u0026quot;, \u0026quot;rmarkdown\u0026quot;, \u0026quot;knitr\u0026quot;,\u0026quot;fst\u0026quot;,\u0026quot;tidyverse\u0026quot;, \u0026quot;lubridate\u0026quot;, \u0026quot;tidymodels\u0026quot;))  If you wanted to make use of a package only very occasionally in a single session, you could access it without explicitly loading it via library(\u0026lt;package-name\u0026gt;), using this syntax: \u0026lt;package-name\u0026gt;::\u0026lt;function_name\u0026gt;, e.g. ?sf::st_contains().    Experiment with R Markdown R Markdown documents are suffixed with the extension .Rmd and based partly on Markdown, a lightweight markup language originally used as a means of minimising tedious mark-up tags (\u0026lt;header\u0026gt;\u0026lt;/header\u0026gt;) when preparing HTML documents. The idea is that you trade some flexibility in the formatting of your HTML for ease-of-writing. Working with R Markdown is very similar to Markdown. Sections are denoted hierarchically with hashes (#, ##, ###) and emphasis using * symbols (*emphasis* **added** reads emphasis added ). Different from standard Markdown, however, R Markdown documents can also contain code chunks to be run when the document is rendered or typeset – they are a mechanism for producing elegant reproducible notebooks.\nEach session of the module has an accompanying R Markdown file. In later sessions you will use these to author computational notebooks that blend code, analysis prose and outputs.\n Download the  01-template.Rmd file for this session and open it in RStudio by clicking File \u0026gt; Open File ... \u0026gt; \u0026lt;your-downloads\u0026gt;/01-template.Rmd.  A quick anatomy of an R Markdown files :\n YAML - positioned at the head of the document and contains metadata determining amongst other things the author details and the output format when typesetting. TEXT - incorporated throughout to document and comment on your analysis. CODE chunks - containing discrete that are to be run when the .Rmd file is typeset or knit.   Figure 4: The anatomy of R Markdown  The YAML section of an .Rmd file controls how your file is typeset and consists of key: value pairs enclosed by ---. Notice that you can change the output format – so should you wish you can generate for example .pdf, .docx files for your reports.\n--- author: \u0026quot;Roger Beecham\u0026quot; date: \u0026#39;2021-05-01\u0026#39; title: \u0026quot;Session 01\u0026quot; output:html_document --- R Markdown files are rendered or typeset with the Knit button, annotated in the Figure above. This starts the knitr package and executes all the code chunks and outputs a markdown (.md) file. The markdown file can then be converted to many different output formats via pandoc.\n Knit the  01-template.Rmd file for this session, either by clicking the Knit button or by typing ctrl + ⇧ + K on Windows, ⌘ + ⇧ + K on macOS.  You will notice that R Markdown code chunks can be customised in different ways. This is achieved by populating fields in the curly brackets at the start of the code chunk:\n```{r \u0026lt;chunk-name\u0026gt;, echo=TRUE, eval=FALSE, cache=FALSE} # Some code that is either run or rendered. ``` A quick overview of the parameters.\n \u0026lt;chunk-name\u0026gt; - Chunks can be given distinct names. This is useful for navigating R markdown file. It also supports chaching – chunks with distinct names are only run once, important if certain chunks take some time to execute. echo=\u0026lt;TRUE|FALSE\u0026gt; - Determines whether the code is visible or hidden from the typeset file. If you output file is a data analysis report you may not wish to expose lengthy code chunks as these may disrupt the discursive text that appears outside of the code chunks. eval=\u0026lt;TRUE|FALSE\u0026gt; - Determines whether the code is evaluated (executed). This is useful if you wish to present some code in your document for display purposes. cache=\u0026lt;TRUE|FALSE\u0026gt; - Determines where the results from the code chunk are cached.  As part of the homework from this session you will do some more research on R Markdown. It is worth in advance downloading RStudio’s cheatsheets, which provide comprehensive details on how to configure R Markdown documents:\n Open RStudio and select Help \u0026gt; Cheatsheets \u0026gt; R Markdown Cheat Sheet | R Markdown Reference Guide   R Scripts Whilst there are obvious benefits to working in R Markdown documents when doing data analysis, there may be occasions where working in an script is preferable. Scripts are plain text files with the extension .R. Comments – text that are not executed as code – are denoted with the # symbol.\nI tend to use R Scripts for writing discrete but substantial code blocks that are to be executed. For example, I might generate a set of functions that relate to a particular use case and bundle these together in an R script. These then might be referred to in a data analysis from an .Rmd, which makes various use of these functions in a similar way as one might import a package. Below is an example script that we will encounter later in the module when creating flow visualizations in R very similar to those that appear in Jo Wood’s TEDx talk. This script is saved with the fie name bezier_path.R. If it were stored in a sensible location, like a project’s code folder, it could be called from an R Markdown file with source(./code/bezier_path). R Scripts can be edited in the same way as R Markdown files in RStudio, via the Code Editor pane.\n# bezier_path.R # # Author: Roger Beecham ############################################################################## #\u0026#39; Functions for generating input data for asymmetric bezier curve for OD data, #\u0026#39; such that the origin is straight and destination curve. The retuned tibble #\u0026#39; is passed to geom_bezier().Parametrtisation follows that published in #\u0026#39; Wood et al. 2011. doi: 10.3138/carto.46.4.239. #\u0026#39; @param data A df with origin and destination pairs representing 2D locations #\u0026#39; (o_east, o_north, d_east, d_north) in cartesian (OSGB) space. #\u0026#39; @param degrees For converting to radians. #\u0026#39; @return A tibble of coordinate pairs representing asymmetric curve get_trajectory \u0026lt;- function(data) { o_east=data$o_east o_north=data$o_north d_east=data$d_east d_north=data$d_north od_pair=data$od_pair curve_angle=get_radians(-90) east=(o_east-d_east)/6 north=(o_north-d_north)/6 c_east=d_east + east*cos(curve_angle) - north*sin(curve_angle) c_north=d_north + north*cos(curve_angle) + east*sin(curve_angle) d \u0026lt;- tibble( x=c(o_east,c_east,d_east), y=c(o_north,c_north,d_north), od_pair=od_pair ) } # Convert degrees to radians. get_radians \u0026lt;- function(degrees) { (degrees * pi) / (180) } To an extent R Scripts are more straightforward than R Markdown files in that you don’t have to worry about configuring code chunks. They are really useful for quickly developing bits of code. This can be achieved by highlighting over the code that you wish to execute and clicking the Run icon at the top of the Code Editor pane or by typing ctrl + rtn on Windows, ⌘ + rtn on macOS\n Create an RStudio Project Throughout this module we will use project-oriented workflows. This is where all files pertaining to a data analysis – data, code and outputs – are organised from a single root folder and where file path discipline is used such that all paths are relative to the project’s root folder (see Bryan \u0026amp; Hester 2020). You can imagine that this sort of self-contained project set-up is necessary for achieving reproducibility of your research. It allows anyone to take a project and run it on their own machines without having to make any adjustments.\nYou might have noticed that when you open RStudio it automatically points to a working directory, likely the home folder for your local machine, denoted with ~/ in the Console. RStudio will by default save any outputs to this folder and will also expect any data you use to be saved there. Clearly if you want to incorporate neat, self-contained project workflows then you will want to organise your work from a dedicated project folder rather than the default home folder for your machine. This can be achieved with the setwd(\u0026lt;path-to-your-project\u0026gt;) function. The problem with doing this is that you insert a path which cannot be understood outside of your local machine at the time it was created. This is a real pain. It makes simple things like moving projects around on your machine an arduous task and most importantly it hinders reproducibility if others are to reuse your work.\nRStudio Projects are a really excellent feature of the RStudio IDE that resolve these problems. Whenever you load up an RStudio Project, R starts up and the working directory is automatically set to the project’s root folder. If you were to move the project elsewhere on your machine, or to another machine, a new root is automatically generated – so RStudio projects ensure that relative paths work.\n Figure 5: Creating an RStudio Project  Let’s create a new Project for this module:\n Select File \u0026gt; New Project \u0026gt; New Directory. Browse to a sensible location and give the project a suitable name. Then click Create Project.  You will notice that the top of the Console window now indicates the root for this new project, in my case ~projects/vis-for-gds.\n In the root of your project, create folders called reports, code, data, figures. Save this session’s  01-template.Rmd file to the reports folder.  Your project’s folder structure should now look like this:\nvis-for-gds\\ vis-for-gds.Rproj code\\ data\\ figures\\ reports\\ 01-template.Rmd   Conclusions Visual data analysis approaches are necessary for exploring complex patterns in data and to make and communicate claims under uncertainty. This is especially true of Geographic Data Science applications, where:\n datasets are being repurposed for social and natural sciences research for the first time; contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone; and, consequently, where the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance.  In this module we will demonstrate this as we explore (Session 4 and 5), model under uncertainty (Session 6 and 7) and communicate (Session 8 and 9) with various social science datasets. We will work with both new, large-scale behavioural datasets, as well as more traditional, administrative datasets located within various social science domains: Political Science, Crime Science, Urban and Transport Planning.\nWe will do so using the statistical programming environment R, which along with Python, is the programming environment for modern data analysis. We will make use of various tools and software libraries that form part of the R ecosystem – the tidyverse for doing modern data science and R Markdown for authoring reproducible research projects.\n References Anscombe, F. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. doi:10.1080/00031305.1973.10478966.  Arribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 12 (10): e12403. doi:10.1111/gec3.12403.  Beecham, R., and J. Wood. 2014. “Exploring Gendered Cycling Behaviours Within a Large-Scale Behavioural Data-Set.” Transportation Planning and Technology 37 (1). Taylor \u0026amp; Francis: 83–97.  Brunsdon, C., and A. Comber. 2020. “Opening Practice: Supporting Reproducibility and Critical Spatial Data Science.” Journal of Geographical Systems.  Donoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (6): 745–66. doi:10.1080/10618600.2017.1384734.  Friendly, M. 2007. “A Brief History of Data Visualization.” In Handbook of Computational Statistics: Data Visualization, edited by C. Chen, W. Härdle, and A Unwin, III:1–34. Heidelberg: Springer-Verlag. http://datavis.ca/papers/hbook.pdf.  Matejka, J., and G. Fitzmaurice. 2017. “Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics Through Simulated Annealing.” In, 1290–94. CHI ’17. New York, NY, USA: Association for Computing Machinery. doi:10.1145/3025453.3025912.  Munzner, T. 2014. Visualization Analysis and Design. AK Peters Visualization Series. Boca Raton, FL: CRC Press.  Singleton, A., and D. Arribas-Bel. 2019. “Geographic Data Science.” Geographical Analysis. doi:10.1111/gean.12194.     For an excellent precis and interpretation of this for geographers, see Arribas-Bel and Reades (2018).↩︎\n Although not the case when actually reading Donoho (2017).↩︎\n Checkout Matejka and Fitzmaurice (2017)’s Same Stats, Different Graphs paper for a fun take one this.↩︎\n   ","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en-uk","lastmod":1656086209,"objectID":"ac47977a15b3902ca3402f61e5bf9df2","permalink":"/class/01-class/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/class/01-class/","section":"class","summary":"Contents  Session outcomes Welcome to Visualization for Geographic Data Science Why vis-for-gds?  Geographic Data Science Geographic Data Science and Visualization  What vis-for-gds? How vis-for-gds?  R for modern data analysis Rmarkdown for reproducible research  Getting started with R and RStudio  Install R and RStudio Open the RStudio IDE Compute in the console Install some packages Experiment with R Markdown R Scripts Create an RStudio Project  Conclusions References   Session outcomes By the end of this session you should gain the following knowledge:","tags":null,"title":"Introduction: Vis for Geographic Data Science","type":"docs"}]