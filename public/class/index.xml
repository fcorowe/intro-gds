<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Class sessions | Introduction to Geographic Data Science</title>
    <link>/class/</link>
      <atom:link href="/class/index.xml" rel="self" type="application/rss+xml" />
    <description>Class sessions</description>
    <generator>Wowchemy (https://wowchemy.com)</generator>
    <image>
      <url>/media/vis-for-gds.png</url>
      <title>Class sessions</title>
      <link>/class/</link>
    </image>
    
    <item>
      <title>Visualization for communication and storytelling</title>
      <link>/class/09-class/</link>
      <pubDate>Mon, 04 Jul 2022 00:00:00 +0000</pubDate>
      <guid>/class/09-class/</guid>
      <description>
&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;script src=&#34;../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;h2&gt;Contents&lt;/h2&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concepts&#34;&gt;Concepts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-driven-storytelling&#34;&gt;Data-driven storytelling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#designed-and-partial&#34;&gt;Designed and partial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#intuitive-and-compelling&#34;&gt;Intuitive and compelling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#political&#34;&gt;Political&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#techniques&#34;&gt;Techniques&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#import&#34;&gt;Import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-trajectories&#34;&gt;Plot trajectories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#add-labels-and-annotations&#34;&gt;Add labels and annotations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#build-legend&#34;&gt;Build legend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compose-graphic&#34;&gt;Compose graphic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;knowledge&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Appreciate&lt;/strong&gt; the main characteristics of data-driven stories.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Identify&lt;/strong&gt; how visual and rhetorical devices are used to communicate with data.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;practical skills&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Create&lt;/strong&gt; &lt;code&gt;ggplot2&lt;/code&gt; graphics with non-standard annotations.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;It is now taken-for-granted that we live in an evidence-based society in which data are deeply embedded in most domains and in how we approach most of the world’s problems. This recognition has coincided with, amongst other things, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Open-source-software_movement&#34;&gt;open source movement&lt;/a&gt; which has freed up access to, and technologies for working with, data. The response to Covid-19 is an excellent example. Enter &lt;a href=&#34;https://bit.ly/2xO2Y5S&#34;&gt;Covid19 github&lt;/a&gt; into a search and you’ll be confronted with hundreds of repositories demonstrating how data related to the pandemic can be collected, processed and analysed.&lt;/p&gt;
&lt;p&gt;This is exciting and feels very democratic. But there is responsibility amongst those constructing and sharing evidence-based arguments to do so with integrity; navigating the difficult tension between communicating a clear message – necessarily reducing some of the complexity – at the same time as acknowledging uncertainty. Those of you with an interest in these matters might follow &lt;a href=&#34;http://www.statslab.cam.ac.uk/~david/&#34;&gt;David Spiegelhalter&lt;/a&gt;, who’s job was created to support public communication of claims made from data, also &lt;a href=&#34;https://timharford.com&#34;&gt;Tim Harford&lt;/a&gt;’s excellent &lt;a href=&#34;https://www.bbc.co.uk/programmes/b006qshd&#34;&gt;More or Less&lt;/a&gt; series. In academia there has been a realisation within Information Visualization and Data Journalism of the role of narratives and storytelling in communicating with data &lt;span class=&#34;citation&#34;&gt;(e.g. &lt;a href=&#34;#ref-henry_data_2018&#34; role=&#34;doc-biblioref&#34;&gt;Henry-Riche et al. 2018&lt;/a&gt;)&lt;/span&gt;. Different from the approach to visual data analysis taken in most of this module, this work recognises that there is not a single, optimum solution to any visualization problem that exposes the true picture/story – no such formulation exists. But that careful design decisions must be made in light of data, audience and intended purpose.&lt;/p&gt;
&lt;p&gt;In this session we will review some of this literature with a special focus on approaches to communicating Covid-19 case numbers.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Watch John Burn-Murdoch’s talk at IEEE VIS 2020 on lessons learnt from generating (visual) data stories around the Covid-19 pandemic. Skip to 15:15 – 45:00 to catch John’s talk.&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;https://www.youtube.com/embed/xlN_QUdT6os&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concepts&lt;/h2&gt;
&lt;div id=&#34;data-driven-storytelling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data-driven storytelling&lt;/h3&gt;
&lt;!-- *We think this movement towards data-driven stories, which is apparent in both the data visualization research community and the professional journalism community, has the potential to form a crucial part of keeping the public informed, a movement sometimes referred to as the democratization of data – the making of data understandable to the general public. This exciting new development in the use of data visualization in media has revealed an emerging professional community in the already complex group of disciplines involved in data visualization. Data visualization has roots in many research fields including perception, computer graphics, design, and human-computer interaction, though only recently has this expanded to include journalism.

Data journalism
here has also been a growing consciousness that some of today’s most relevant stories are buried in data. This data can be quite hard to understand in its raw formats but can become much more generally accessible when visualized. Journalists have not only begun to use standard data visualizations such as charts and maps in their stories, but are also creating new ones that are tailored to the particular data type and to the message of the story they are writing. Since journalists are now able to easily share interactive data visualizations on the Web, the democratization of data visualization is accelerating with new compelling data visualizations emerging in the media daily...

By carefully structuring the information and integrating explanation to guide the consumer, journalists help lead readers towards a valid interpretation of the underlying data. --&gt;
&lt;p&gt;In sessions &lt;a href=&#34;../class/02-class/&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;../class/03-class/&#34;&gt;3&lt;/a&gt; and &lt;a href=&#34;../class/04-class/&#34;&gt;4&lt;/a&gt;, I made the case that visual data analysis is concerned with encoding data to visuals in a way that follows established guidelines known to work well for particular tasks.
Watching John’s talk, you hopefully got a sense that designing graphics for &lt;em&gt;communication&lt;/em&gt; requires slightly deeper thinking, with no obviously “optimal” way to proceed.&lt;/p&gt;
&lt;p&gt;This is not to suggest that visual data stories are devoid of underlying theories or frameworks. There are of course entire academic disciplines devoted to communication and storytelling. &lt;span class=&#34;citation&#34;&gt;Roth (&lt;a href=&#34;#ref-roth_cartographic_2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;’s recent survey paper on visual storytelling in Cartography considers these to usefully identify some of the characteristics of effective visual data stories. &lt;span class=&#34;citation&#34;&gt;Roth (&lt;a href=&#34;#ref-roth_cartographic_2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; enumerates 10 characteristics, but particularly important are that visual design stories are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Designed&lt;/strong&gt;: The analyst makes very deliberate decisions in light of audience and purpose. The goal of visual storytelling is not just to &lt;em&gt;show&lt;/em&gt; but also to &lt;em&gt;explain&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Partial&lt;/strong&gt;: Essential information is prioritised and made salient, with abstraction and brevity preferred over complexity and completeness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intuitive&lt;/strong&gt;: Visual narratives take advantage of our natural tendency to communicate via metaphor and story, with a clear “entry point” and clear progression.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compelling&lt;/strong&gt;: Visual stories often capture attention through an array of graphical devices – sequence, animation and interaction. They generate an aesthetic response.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relatable and situated&lt;/strong&gt;: Visual stories promote empathy, using devices that place the audience in the story setting. They are usually constructed from somewhere – according to a particular position.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Political&lt;/strong&gt;: Visual data stories often promote with clarity particular voices, interpretations or positions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the sections that follow, we will review some key Covid-19 visualizations and reflect on how these storytelling devices have been variously deployed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;designed-and-partial&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Designed and partial&lt;/h3&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:ft-vis&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/09-class_files/jbm-logchart.jpeg&#34; alt=&#34;[John Burn-Murdoch](https://www.ft.com/john-burn-murdoch)&#39;s international comparison of deaths, as it appeared on [twitter](https://twitter.com/jburnmurdoch/status/1241820210455285760).&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: &lt;a href=&#34;https://www.ft.com/john-burn-murdoch&#34;&gt;John Burn-Murdoch&lt;/a&gt;’s international comparison of deaths, as it appeared on &lt;a href=&#34;https://twitter.com/jburnmurdoch/status/1241820210455285760&#34;&gt;twitter&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A version of John Burn Murdoch’s international comparison chart is in Figure &lt;a href=&#34;#fig:ft-vis&#34;&gt;1&lt;/a&gt;. We can evaluate the graphic using some of the principles introduced in &lt;a href=&#34;../class/03-class/&#34;&gt;session 3&lt;/a&gt; and &lt;a href=&#34;../class/03-class/&#34;&gt;session 4&lt;/a&gt;. In mapping the two quantitive variables to &lt;em&gt;position on an aligned scale&lt;/em&gt; and &lt;em&gt;colour hue&lt;/em&gt; to distinguish countries, the graphic uses an encoding that exploits our cognitive abilities. By superimposing lines representing countries on the same coordinate space, it also nicely uses layout to support comparison &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gleicher_visual_2011&#34; role=&#34;doc-biblioref&#34;&gt;Gleicher and Roberts 2011&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Other important design decisions were mentioned in John’s talk. You might have noticed that these were justified with respect to two goals or tasks that he wished the reader to perform. Thinking about &lt;span class=&#34;citation&#34;&gt;Roth (&lt;a href=&#34;#ref-roth_cartographic_2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;’s characteristics of visual storytelling, the graphic is &lt;strong&gt;designed&lt;/strong&gt; with a very deliberate purpose:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Between country comparison:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;“&lt;em&gt;Are countries on the same course?&lt;/em&gt;”&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Comparison against milestone:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;“&lt;em&gt;How many days does it take a certain county to reach a given number of cases/deaths?&lt;/em&gt;”&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is possible to see each of these goals informing the configuration of the graphic and in the careful design decisions made to prioritise essential information – the graphic is &lt;strong&gt;partial&lt;/strong&gt;. Comparison between countries is most obviously supported by the use of a log-scale. This removes the dominant salient pattern in standard scales due to the exponential doubling and instead allows slopes (&lt;em&gt;growth rates&lt;/em&gt;) to be compared directly. In his talk John makes the point that log scales are, though, not so familiar to the average reader. In other versions of the graphic, useful annotations are provided identifying slope gradients that correspond to different growth rates, again narrowing on the essential goal of the graphic – between country comparison of growth rates. In the talk John mentioned approaches to dealing with daily volatility in the data and that a decision was made to use a smoothing function based on &lt;a href=&#34;https://en.wikipedia.org/wiki/Smoothing_spline&#34;&gt;splines&lt;/a&gt; rather than the more widely understood 7-day rolling average. This was not a comfortable decision as it introduced complexity, and so was incorporated discreetly (&lt;strong&gt;partial&lt;/strong&gt;/&lt;strong&gt;designed&lt;/strong&gt;).&lt;/p&gt;
&lt;!-- For example, the fact that rather than showing absolute time, the &#34;temporal&#34; x-axis is anchored according to days that elapsed since *n* cases/deaths are reported for each country.  --&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You might be interested to know that most &lt;a href=&#34;https://www.ft.com/graphics&#34;&gt;Financial Times graphics&lt;/a&gt; are built using &lt;code&gt;ggplot2&lt;/code&gt;. A presentation in which John makes the case for &lt;code&gt;ggplot2&lt;/code&gt; for Data Journalism &lt;a href=&#34;https://johnburnmurdoch.github.io/slides/r-ggplot/#/&#34;&gt;from this link&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;!-- For example, the fact that rather than showing absolute time, the &#34;temporal&#34; x-axis is anchored according to days that elapsed since *n* cases/deaths are reported for each country.  --&gt;
&lt;p&gt;An interesting design alternative that supports between country comparison is &lt;a href=&#34;https://aatishb.com&#34;&gt;Aatish Bhatia&lt;/a&gt;’s &lt;a href=&#34;https://aatishb.com/covidtrends/&#34;&gt;Covid Trends&lt;/a&gt; chart (Figure &lt;a href=&#34;#fig:covid-trends&#34;&gt;2&lt;/a&gt;). A double log scale is used and growth rate in new cases is presented on the y-axis with total case numbers, rather than time elapsed, plotted along the x-axis. Whilst the introduction of a double log scale might be judged to increase visual complexity, actually this design narrows or simplifies the reader’s visual judgement to looking at the thing that we are most interested in: comparison of country growth rates against the two day doubling (annotated with the diagonal). The chart is also accompanied with an excellent &lt;a href=&#34;https://www.hippocraticpost.com/covid-19-update/how-to-tell-if-were-beating-covid-19/&#34;&gt;explanatory video&lt;/a&gt;, in which many of the characteristics of visual data stories enumerated by &lt;span class=&#34;citation&#34;&gt;Roth (&lt;a href=&#34;#ref-roth_cartographic_2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; can be identified.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:covid-trends&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/09-class_files/covid-trends.png&#34; alt=&#34;[Aatish Bhatia](https://aatishb.com)&#39;s [Covid Trends](https://aatishb.com/covidtrends/) chart. [Source](https://github.com/aatishb/covidtrends#credits).&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: &lt;a href=&#34;https://aatishb.com&#34;&gt;Aatish Bhatia&lt;/a&gt;’s &lt;a href=&#34;https://aatishb.com/covidtrends/&#34;&gt;Covid Trends&lt;/a&gt; chart. &lt;a href=&#34;https://github.com/aatishb/covidtrends#credits&#34;&gt;Source&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;intuitive-and-compelling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Intuitive and compelling&lt;/h3&gt;
&lt;p&gt;According to &lt;span class=&#34;citation&#34;&gt;Roth (&lt;a href=&#34;#ref-roth_cartographic_2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, visual data stories are often &lt;em&gt;explanatory&lt;/em&gt;; they make &lt;strong&gt;compelling&lt;/strong&gt; use of graphical and rhetorical devices to support understanding. This is especially important in data-driven storytelling as it often involves concepts that are initially quite challenging. In Figure &lt;a href=&#34;#fig:lambrechts-connected&#34;&gt;3&lt;/a&gt; is a static image from a &lt;a href=&#34;https://flourish.studio/2021/04/06/masters-connected-scatter-maarten-lambrechts/&#34;&gt;data story written by Flourish&lt;/a&gt; based on design work by &lt;a href=&#34;https://twitter.com/maartenzam/status/1319622943526293505&#34;&gt;Marteen Lambrechts&lt;/a&gt;. The data story is essentially a design exposition &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-beecham_design_2020&#34; role=&#34;doc-biblioref&#34;&gt;Beecham et al. 2020&lt;/a&gt;; &lt;a href=&#34;#ref-wood_design_2018&#34; role=&#34;doc-biblioref&#34;&gt;Wood, Kachkaev, and Dykes 2018&lt;/a&gt;)&lt;/span&gt;, guiding readers from the familiar to the unfamiliar. First a standard time series chart of hospitalisations and deaths is presented. Deficiencies in this layout are explained before progressively introducing the transformations involved to generate the preferred graphic, a &lt;a href=&#34;http://steveharoz.com/research/connected_scatterplot/&#34;&gt;connected scatterplot&lt;/a&gt;. Ordering the story in this way explains design decisions and the trade-offs associated with visual design from a familiar starting point and helps justify new, sometimes unfamiliar encodings. Thinking about &lt;span class=&#34;citation&#34;&gt;Roth (&lt;a href=&#34;#ref-roth_cartographic_2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;’s characteristics of visual storytelling, formulating a design story in this way helps build &lt;strong&gt;intuition&lt;/strong&gt; – there is a clear entry point and clear progression.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:lambrechts-connected&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/09-class_files/lambrechts-connected-flourish.png&#34; alt=&#34;Static image from a data story written by [Flourish](https://flourish.studio/2021/04/06/masters-connected-scatter-maarten-lambrechts/) demonstrating how connected scatterplots can be used to analyse changes in hospitalisations and deaths.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Static image from a data story written by &lt;a href=&#34;https://flourish.studio/2021/04/06/masters-connected-scatter-maarten-lambrechts/&#34;&gt;Flourish&lt;/a&gt; demonstrating how connected scatterplots can be used to analyse changes in hospitalisations and deaths.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;For a more involved design exposition of connected scatterplots, see &lt;a href=&#34;http://www.dannydorling.org&#34;&gt;Danny Dorling&lt;/a&gt;’s work on &lt;a href=&#34;https://twitter.com/dannydorling/status/1245010922592231424&#34;&gt;slowdown&lt;/a&gt;. Note that Danny also encodes time using line width. This is an important addition, along with colour as demonstrated with &lt;a href=&#34;https://aviz.fr/~bbach/timecurves/&#34;&gt;timecurves&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;I have also tried to use this idea of guided design exposition for explaining complex designs in an &lt;a href=&#34;https://twitter.com/rJBeecham/status/1317019462453895168&#34;&gt;analysis of Covid-19 cases data&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-beecham_on_2021&#34; role=&#34;doc-biblioref&#34;&gt;Beecham et al. 2021&lt;/a&gt;)&lt;/span&gt; – exploring ways of showing simultaneously absolute and relative change in cases, with geographic context.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:ft-anim&#34;&gt;4&lt;/a&gt;, again by &lt;a href=&#34;https://www.ft.com/john-burn-murdoch&#34;&gt;John Burn-Murdoch&lt;/a&gt;, is another stellar exposition of how animation can be used to build &lt;strong&gt;intuition&lt;/strong&gt;. The main objective is to demonstrate how different 2020 is in terms of admissions to intensive care compared to a normal year. This was in response to claims that Covid-19 behaves much like seasonal flu; to this extent the graphic is also quite &lt;strong&gt;political&lt;/strong&gt;. Each year from 2013-14 is added to the chart and the y-axis rescaled to reflect the new numbers. This helps build expectation around normal variablitity in a similar way to the &lt;a href=&#34;https://medium.com/hci-design-at-uw/hypothetical-outcomes-plots-experiencing-the-uncertain-b9ea60d7c740&#34;&gt;hypothetical outcome plots&lt;/a&gt; covered in the previous session. The expectation is then roundly debunked by the introduction of the 2020-21 line in red, with animated rescaling of the y-axis used for further emphasis.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:ft-anim&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/09-class_files/jbm-anim.gif&#34; alt=&#34;[John Burn-Murdoch](https://www.ft.com/john-burn-murdoch)&#39;s animated rescaling [via twitter](https://twitter.com/jburnmurdoch/status/1347200811303055364) demonstrating how different in terms of intensive care admissions 2020/21 is to a standard flu season.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: &lt;a href=&#34;https://www.ft.com/john-burn-murdoch&#34;&gt;John Burn-Murdoch&lt;/a&gt;’s animated rescaling &lt;a href=&#34;https://twitter.com/jburnmurdoch/status/1347200811303055364&#34;&gt;via twitter&lt;/a&gt; demonstrating how different in terms of intensive care admissions 2020/21 is to a standard flu season.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;political&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Political&lt;/h3&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:ft-vaccine&#34;&gt;5&lt;/a&gt; presents a final example of &lt;a href=&#34;https://twitter.com/jburnmurdoch/status/1382013080448724994&#34;&gt;visual narrative&lt;/a&gt; from &lt;a href=&#34;https://www.ft.com/john-burn-murdoch&#34;&gt;John Burn-Murdoch&lt;/a&gt; with an obviously political purpose. The graphic was created in response to claims from the UK’s Prime Minister that it is largely restrictions rather than vaccination that has reduced infection rates in the country. Interesting here is how annotation and &lt;strong&gt;visual saliency&lt;/strong&gt; is used to direct how the graphic is perceived. If the graphic was only annotated with points in time when lockdown and vaccination was initiated, it would &lt;em&gt;invite&lt;/em&gt; us to make judgements about the effects of these two events on infectious rates. That John makes highly salient annotations labelling the (unmeasurable) effect of &lt;strong&gt;lockdown&lt;/strong&gt; and &lt;strong&gt;vaccine&lt;/strong&gt; is an interesting addition. There is little room for ambiguity.&lt;/p&gt;
&lt;p&gt;Initially I was a little surprised by this presentation – labelling the chart with an unmeasurable effect – and wondered if it had graphical integrity (see &lt;span class=&#34;citation&#34;&gt;Tufte (&lt;a href=&#34;#ref-tufte_visual_1983&#34; role=&#34;doc-biblioref&#34;&gt;1983&lt;/a&gt;)&lt;/span&gt;’s &lt;a href=&#34;https://infovis-wiki.net/wiki/Lie_Factor&#34;&gt;lie factor&lt;/a&gt;). Given the fact that John’s work is so carefully considered, communicated transparently and with authority, I think it probably does have integrity. I do wonder whether John’s reflections on his experiences of generating data stories earlier in the pandemic informed his thinking here – that the messages readers interpreted from his cases charts varied depending on the prior expectations and political beliefs of those consuming them.&lt;/p&gt;
&lt;!-- * design cues facilitate prioritization of particular interpretations in visualizations --&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:ft-vaccine&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/09-class_files/jbm-vaccine.jpeg&#34; alt=&#34;[John Burn-Murdoch](https://www.ft.com/john-burn-murdoch)&#39;s analysis [via twitter](https://twitter.com/jburnmurdoch/status/1382013080448724994) evaluating the role of lockdown and the vaccine.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: &lt;a href=&#34;https://www.ft.com/john-burn-murdoch&#34;&gt;John Burn-Murdoch&lt;/a&gt;’s analysis &lt;a href=&#34;https://twitter.com/jburnmurdoch/status/1382013080448724994&#34;&gt;via twitter&lt;/a&gt; evaluating the role of lockdown and the vaccine.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    An additional aside: The annotations in Figure &lt;a href=&#34;#fig:ft-vaccine&#34;&gt;5&lt;/a&gt; have heavy saliency thanks to John’s parsimonious use of axis titles, marks and legends (and other &lt;a href=&#34;https://infovis-wiki.net/wiki/Data-Ink_Ratio&#34;&gt;non-data-ink&lt;/a&gt;).
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;!-- https://www.statschat.org.nz/wp-content/uploads/2020/04/fox-axis.jpeg --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;techniques&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Techniques&lt;/h2&gt;
&lt;p&gt;The technical element demonstrates how to design plots deliberatively with annotations in &lt;code&gt;ggplot2&lt;/code&gt;. We will recreate a &lt;em&gt;glyphmap&lt;/em&gt; type graphic that originally appeared in &lt;a href=&#34;https://www.washingtonpost.com/nation/2020/05/24/coronavirus-rural-america-outbreaks/?arc404=true&#34;&gt;The Washington Post&lt;/a&gt; to tell a story of growth in Covid-19 cases by US county.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:wp&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/09-class_files/wp.png&#34; alt=&#34;Glyphmap design displaying growth in COVID-19 cases by US county, based on the design by Thebault and Hauslohner, original in [The Washington Post](https://www.washingtonpost.com/nation/2020/05/24/coronavirus-rural-america-outbreaks/?arc404=true).&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Glyphmap design displaying growth in COVID-19 cases by US county, based on the design by Thebault and Hauslohner, original in &lt;a href=&#34;https://www.washingtonpost.com/nation/2020/05/24/coronavirus-rural-america-outbreaks/?arc404=true&#34;&gt;The Washington Post&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In the graphic (Figure &lt;a href=&#34;#fig:wp&#34;&gt;6&lt;/a&gt;), each US county is represented as a line which is encoded with daily growth rates in new cases between 3rd May and 26th May. Lines are positioned at the geographic centre of each county. The graphic is data dense and without careful decisions on which aspects to emphasise, it would be quite unreadable. Line thickness is varied according to relative infection rates (cumulative cases/population size) and growth rate is double encoded with colour value – darker and steeper lines have higher growth rates. Even with these additions, it is challenging to discern complete trajectories, but instead a typical model or expectation of these trajectories can be learnt from visually scanning the graphic. That there is spatial autocorrelation in trajectories means an overall pattern of exposure can be inferred, before eyes are then drawn to exceptions. Initially these are towards the extreme end; tall, steep, dark and thick lines, suggesting large absolute numbers, rapid growth rates and high exposure. Secondarily, interesting subtle patterns can be discerned, for example a thick and mid-dark line surrounded by lines that are generally lighter and thinner; a county that appears locally exceptional in having a comparatively high growth and exposure rate.&lt;/p&gt;
&lt;p&gt;The design is impressive, and there is an obvious benefit to showing growth rates in their spatial position. However, we are not looking at absolute numbers here. The counties that are most salient are not those with the largest case counts. Rather, they have experienced rapid growth since the number of cases reported on 3rd May. So the graphic is most certainly &lt;strong&gt;partial&lt;/strong&gt; and &lt;strong&gt;designed&lt;/strong&gt; to suit a particular purpose. A slight adjustment in my implementation in Figure &lt;a href=&#34;#fig:wp&#34;&gt;6&lt;/a&gt; was to only show growth rates for counties that had non-negligible case counts on 3rd May (&lt;span class=&#34;math inline&#34;&gt;\(\geq20\)&lt;/span&gt; cases).&lt;/p&gt;
&lt;p&gt;Without the careful integration of annotations and non-standard legends, the graphic would not be so successful. The aim of this technical session is to demonstrate an approach to generating heavily designed annotations – custom legends, which are often necessary when communicating with maps. For a more extensive demonstration of how charts can be annotated and refined, I highly recommend &lt;a href=&#34;https://socviz.co/refineplots.html#refineplots&#34;&gt;Chapter 8&lt;/a&gt; of &lt;span class=&#34;citation&#34;&gt;Healy (&lt;a href=&#34;#ref-healy_data_2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; and the &lt;a href=&#34;https://bbc.github.io/rcookbook/&#34;&gt;BBC Visual and Data Journalism Cookbook&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;import&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Import&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download the &lt;a href=&#34;../homework/09-homework_files/09-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 09-template.Rmd&lt;/a&gt; file for this session and save it to the &lt;code&gt;reports&lt;/code&gt; folder of your &lt;code&gt;vis-for-gds&lt;/code&gt; project.&lt;/li&gt;
&lt;li&gt;Open your &lt;code&gt;vis-for-gds&lt;/code&gt; project in RStudio and load the template file by clicking &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Open File ...&lt;/code&gt; &amp;gt; &lt;code&gt;reports/09-template.Rmd&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The template file lists the required packages – &lt;code&gt;tidyverse&lt;/code&gt; and &lt;code&gt;sf&lt;/code&gt;. The data were collected using &lt;a href=&#34;https://kieranhealy.org/&#34;&gt;Kieran Healy&lt;/a&gt;’s &lt;a href=&#34;https://kjhealy.github.io/covdata/&#34;&gt;&lt;code&gt;covdata&lt;/code&gt;&lt;/a&gt; package, with attribution to the &lt;a href=&#34;https://kjhealy.github.io/covdata/articles/new-york-times.html&#34;&gt;county-level cumulative cases dataset&lt;/a&gt; released and maintained by data journalists at the New York Times (&lt;a href=&#34;https://github.com/nytimes/covid-19-data&#34;&gt;this repo&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The template provides access to a version of this dataset that I have ‘staged’ for charting. For this I filtered cases data on the dates covered by the Washington Post graphic (3rd to 25th May); identified counties whose daily case counts were &lt;span class=&#34;math inline&#34;&gt;\(\geq20\)&lt;/span&gt; cases on 3rd May; calculated daily growth rates, anchored to the recorded case counts on 3rd May; calculated ‘end’ growth rates and daily counts for each county (those recorded on 25th May); and finally a binned growth rate variable identifying counties with daily case counts on 25rd May that were &lt;span class=&#34;math inline&#34;&gt;\(\leq2\times\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\geq2\times\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\geq4\times\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\geq7\times\)&lt;/span&gt; the daily case counts measured on 3rd May. Also there is a &lt;code&gt;state_boundaries&lt;/code&gt; dataset to download, which contains &lt;code&gt;geometry&lt;/code&gt; data for each US state, collected from &lt;a href=&#34;https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html&#34;&gt;US Census Bureau&lt;/a&gt; as well as coordinate variables describing the geographic centroid of each state. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Albers_projection&#34;&gt;Albers Equal Area&lt;/a&gt; projection is used.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-trajectories&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot trajectories&lt;/h3&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:wp-basic&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/09-class_files/wp-basic.png&#34; alt=&#34;Glyphmap design displaying growth in COVID-19 cases by US county, without legend and annotations.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: Glyphmap design displaying growth in COVID-19 cases by US county, without legend and annotations.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The main graphic is reasonably straightforward to construct.&lt;/p&gt;
&lt;p&gt;The code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_data %&amp;gt;%
  ggplot()+
  geom_sf(data=state_boundaries, fill=&amp;quot;#eeeeee&amp;quot;, colour=&amp;quot;#bcbcbc&amp;quot;, size=0.2)+
  coord_sf(crs=5070, datum=NA, clip=&amp;quot;off&amp;quot;)+
  geom_point(
    data=.%&amp;gt;% filter(date==&amp;quot;2020-05-03&amp;quot;),
    aes(x=x, y=y, colour=binned_growth_rate, alpha=binned_growth_rate, size=case_rate)
    )+
  # Plot case data.
  geom_path(
    aes(
      x=x+(day_num*6000)-6000, y=y+(growth_rate*50000)-50000, group=fips,
      colour=binned_growth_rate, size=case_rate, alpha=binned_growth_rate
      ),
    lineend=&amp;quot;round&amp;quot;
  ) +
  scale_colour_manual(values=c(&amp;quot;#fa9fb5&amp;quot;, &amp;quot;#dd3497&amp;quot;, &amp;quot;#7a0177&amp;quot;, &amp;quot;#49006a&amp;quot;))+
  scale_size(range=c(.1,2.5))+
  scale_alpha_ordinal(range=c(.2,1))+
  guides(colour=FALSE, size=FALSE, alpha=FALSE)+
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot specification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: The main dataset – the staged &lt;code&gt;county_data&lt;/code&gt; file. Separately there is a &lt;code&gt;state_boundaries&lt;/code&gt; file, used to draw state boundaries and later label states. For the points drawn at the centroid of each US county (&lt;code&gt;geom_point()&lt;/code&gt;), the data are filtered so that only a single day is represented (&lt;code&gt;filter(date==&#34;2020-05-03&#34;)&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: For &lt;code&gt;geom_point()&lt;/code&gt;, x-position and y-position is mapped to county centroid (&lt;code&gt;x&lt;/code&gt;,&lt;code&gt;y&lt;/code&gt; in &lt;code&gt;county_data&lt;/code&gt;), points are coloured according to &lt;code&gt;binned_growth_rate&lt;/code&gt; (both &lt;code&gt;colour&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt;) and sized according to that county’s &lt;code&gt;case_rate&lt;/code&gt;. The same colour and size encoding is used for the lines (&lt;code&gt;geom_path()&lt;/code&gt;). County lines are again anchored at county centroids but offset in &lt;code&gt;x&lt;/code&gt; according to time elapsed (&lt;code&gt;day_num&lt;/code&gt;) and in &lt;code&gt;y&lt;/code&gt; according to &lt;code&gt;growth_rate&lt;/code&gt;. The constants applied to &lt;code&gt;growth_rate&lt;/code&gt; (5000) and &lt;code&gt;day_num&lt;/code&gt; (6000), which control the space occupied by the lines, was arrived at manually through trial and error. In order to draw separate lines for each county, we set the &lt;code&gt;group=&lt;/code&gt; argument to the county identifier variable &lt;code&gt;fips&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_point()&lt;/code&gt; for the start points centred on county centroids and &lt;code&gt;geom_path()&lt;/code&gt; for the lines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt;: &lt;code&gt;scale_colour_manual()&lt;/code&gt; for the binned growth rate colours; &lt;code&gt;scale_alpha()&lt;/code&gt; for an ordinal transparency range – the floor for this is 0.2 and not 0, otherwise counties with the smallest binned growth rates would not be visible; &lt;code&gt;scale_size()&lt;/code&gt; for varying size continuously according to case rate, the range was arrived at through trial and error.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: We don’t want the default legend to appear and so &lt;code&gt;guides()&lt;/code&gt; turns these off; additionally &lt;code&gt;theme_void()&lt;/code&gt; for removing default axes, gridlines etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;add-labels-and-annotations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Add labels and annotations&lt;/h3&gt;
&lt;p&gt;The two-letter state boundaries can be added in a &lt;code&gt;geom_text()&lt;/code&gt; layer, positioned in &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; at state centroids. For obvious reasons this needs to appear &lt;em&gt;after&lt;/em&gt; the first call to &lt;code&gt;geom_sf()&lt;/code&gt;, which draws the filled state outlines:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_data %&amp;gt;%
  ggplot()+
  geom_sf(data=state_boundaries, fill=&amp;quot;#eeeeee&amp;quot;, colour=&amp;quot;#bcbcbc&amp;quot;, size=0.2)+
  geom_text(data=state_boundaries, aes(x=x,y=y,label=STUSPS), alpha=.8)+
  ...
  ...
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the selected counties, we create a filtered data frame of those counties with just one row for each county. This is a little more tedious as we have to manually identify these in a &lt;code&gt;filter()&lt;/code&gt;. Note that we filter on &lt;code&gt;date&lt;/code&gt; first, so that only one row is returned for each county. Within the &lt;code&gt;mutate()&lt;/code&gt; some manual abbreviations are made for state names and also &lt;code&gt;end_rate&lt;/code&gt; variable is rounded to whole numbers for better labelling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Counties to annotate.
annotate &amp;lt;- county_data %&amp;gt;% filter(
    date==&amp;quot;2020-05-03&amp;quot;,
    county==c(&amp;quot;Huntingdon&amp;quot;) &amp;amp; state==&amp;quot;Pennsylvania&amp;quot; |
    county==c(&amp;quot;Lenawee&amp;quot;) &amp;amp; state==&amp;quot;Michigan&amp;quot; |
    county==c(&amp;quot;Crawford&amp;quot;) &amp;amp; state==&amp;quot;Iowa&amp;quot; |
    county==c(&amp;quot;Wapello&amp;quot;) &amp;amp; state==&amp;quot;Iowa&amp;quot; |
    county==c(&amp;quot;Lake&amp;quot;) &amp;amp; state==&amp;quot;Tennessee&amp;quot; |
    county==&amp;quot;Texas&amp;quot; &amp;amp; state == c(&amp;quot;Oklahoma&amp;quot;) |
    county==c(&amp;quot;Duplin&amp;quot;) &amp;amp; state==&amp;quot;North Carolina&amp;quot; |
    county==c(&amp;quot;Santa Cruz&amp;quot;) &amp;amp; state==&amp;quot;Arizona&amp;quot;|
    county==c(&amp;quot;Titus&amp;quot;) &amp;amp; state==&amp;quot;Texas&amp;quot;|
    county==c(&amp;quot;Yakima&amp;quot;) &amp;amp; state==&amp;quot;Washington&amp;quot;
    ) %&amp;gt;%
  mutate(
    state_abbr=case_when(
      state==&amp;quot;Pennsylvania&amp;quot; ~ &amp;quot;Penn.&amp;quot;,
      state==&amp;quot;Iowa&amp;quot; ~ &amp;quot;Iowa&amp;quot;,
      state==&amp;quot;Tennessee&amp;quot; ~ &amp;quot;Tenn.&amp;quot;,
      state==&amp;quot;Oklahoma&amp;quot; ~ &amp;quot;Okla.&amp;quot;,
      state==&amp;quot;Texas&amp;quot; ~ &amp;quot;Texas&amp;quot;,
      state==&amp;quot;North Carolina&amp;quot; ~ &amp;quot;N.C.&amp;quot;,
      state==&amp;quot;Washington&amp;quot; ~ &amp;quot;Wash.&amp;quot;,
      state==&amp;quot;Michigan&amp;quot; ~ &amp;quot;Mich.&amp;quot;,
      state==&amp;quot;Arizona&amp;quot; ~ &amp;quot;Arizona&amp;quot;,
      TRUE ~ &amp;quot;&amp;quot;
    ),
    end_rate_round = round(end_rate,0)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting these is again quite straightforward with &lt;code&gt;geom_text()&lt;/code&gt;. The &lt;code&gt;paste0()&lt;/code&gt; function is used to build labels that display &lt;code&gt;county&lt;/code&gt; and then &lt;code&gt;state_abbr&lt;/code&gt;. These appear below each county and y-position is offset accordingly. Additionally the counties are given a bold font by passing an argument to &lt;code&gt;family=&lt;/code&gt; (not in the code below as I have used a font not native to all machines). The same approach is used for the rate labels, but with an incremented y-position offset so that they don’t overlap the county name labels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_data %&amp;gt;%
  ggplot()+
  geom_sf(data=state_boundaries, fill=&amp;quot;#eeeeee&amp;quot;, colour=&amp;quot;#bcbcbc&amp;quot;, size=0.2)+
  ...
  geom_text(data=annotate, aes(x=x,y=y-20000,label=paste0(county,&amp;quot;, &amp;quot;,state_abbr)), alpha=1, size=3)+
  geom_text(data=annotate, aes(x=x,y=y-65000,label=paste0(end_rate_round,&amp;quot;X more cases&amp;quot;)), alpha=1, size=2.5)+
  ...
  ...
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;build-legend&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Build legend&lt;/h3&gt;
&lt;p&gt;To generate the legend, we need to know the spatial extent of US mainland so that we can draw outside of this. This can be arrived at using &lt;code&gt;st_bbox()&lt;/code&gt; and also &lt;code&gt;width&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; dimensions for US expressed in spatial units.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bounding box for mainland US.
bbox &amp;lt;- st_bbox(state_boundaries)
width &amp;lt;- bbox$xmax-bbox$xmin
height &amp;lt;- bbox$ymax-bbox$ymin&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then create a dataset for the top right legend displaying the different categories of growth rate (Figure &lt;a href=&#34;#fig:wp-growth-legend&#34;&gt;8&lt;/a&gt;). Counties filtered by their different growth rates (in the &lt;code&gt;filter()&lt;/code&gt; below) were identified manually. In the &lt;code&gt;mutate()&lt;/code&gt; we set x-position to start at the right quarter of the graphic (&lt;code&gt;bbox$xmax-.25*width&lt;/code&gt;) and y-position to start slightly above the top of the graphic &lt;code&gt;bbox$ymax+.05*height&lt;/code&gt;. &lt;code&gt;case_rate&lt;/code&gt; is set to a constant as we don’t want line width to vary and also a manually created &lt;code&gt;label&lt;/code&gt; variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Legend : growth
legend_growth &amp;lt;- county_data %&amp;gt;%
  filter(
    county==&amp;quot;Dubois&amp;quot; &amp;amp; state==&amp;quot;Indiana&amp;quot; |
    county==&amp;quot;Androscoggin&amp;quot; &amp;amp; state==&amp;quot;Maine&amp;quot; |
    county==&amp;quot;Fairfax&amp;quot; &amp;amp; state==&amp;quot;Virginia&amp;quot; |
    county==&amp;quot;Bledsoe&amp;quot; &amp;amp; state==&amp;quot;Tennessee&amp;quot;
  ) %&amp;gt;%
  mutate(
    x=bbox$xmax-.25*width,y=bbox$ymax+.05*height,
    case_rate=.01,
    label=case_when(
      county == &amp;quot;Dubois&amp;quot; ~ &amp;quot;7x more cases than on May 3&amp;quot;,
      county == &amp;quot;Androscoggin&amp;quot; ~ &amp;quot;4x&amp;quot;,
      county == &amp;quot;Fairfax&amp;quot; ~ &amp;quot;2x&amp;quot;,
      county == &amp;quot;Bledsoe&amp;quot; ~ &amp;quot;About the same as on May 3&amp;quot;
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:wp-growth-legend&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/09-class_files/wp-growth.png&#34; alt=&#34;Legend demonstrating growth rates.&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: Legend demonstrating growth rates.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A separate dataset is also created for drawing the top left legend (Figure &lt;a href=&#34;#fig:wp-case-legend&#34;&gt;9&lt;/a&gt;), showing different case rates relative to population size. In the &lt;code&gt;mutate()&lt;/code&gt; we set x-position to start towards the left of the graphic (&lt;code&gt;bbox$xmax-.88*width&lt;/code&gt;) and y-position to start slightly above the top of the graphic &lt;code&gt;bbox$ymax+.05*height&lt;/code&gt;. We want to draw three lines corresponding to a low, medium and high growth rate and so &lt;code&gt;pivot_longer()&lt;/code&gt; to duplicate the daily case data over rows. Each line needs to be drawn next to one another and this is achieved with the &lt;code&gt;offset_day&lt;/code&gt; variable, a multiple applied to the geographic &lt;code&gt;width&lt;/code&gt; of US in the eventual &lt;code&gt;ggplot2&lt;/code&gt; specification.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Legend : case
legend_case &amp;lt;- county_data %&amp;gt;%
  filter(
    county == &amp;quot;Kings&amp;quot; &amp;amp; state==&amp;quot;California&amp;quot; ) %&amp;gt;%
  mutate(
    x=bbox$xmax-.88*width,y=bbox$ymax+.05*height,
    binned_growth_rate=factor(binned_growth_rate)
  ) %&amp;gt;%
  select(x, y, day_num, growth_rate, binned_growth_rate, fips) %&amp;gt;%
  mutate(
    low=.001, mid=.009, high=.015,
  ) %&amp;gt;%
  pivot_longer(cols=c(low, mid, high), names_to=&amp;quot;offset&amp;quot;, values_to=&amp;quot;offset_rate&amp;quot;) %&amp;gt;%
  mutate(
    offset_day= case_when(
      offset == &amp;quot;low&amp;quot; ~ 0,
      offset == &amp;quot;mid&amp;quot; ~ .04,
      offset == &amp;quot;high&amp;quot; ~ .08
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:wp-case-legend&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/09-class_files/wp-case.png&#34; alt=&#34;Legend demonstrating case rates.&#34; width=&#34;60%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: Legend demonstrating case rates.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;compose-graphic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compose graphic&lt;/h3&gt;
&lt;p&gt;The code block below demonstrates how derived data for the legends are used in the &lt;code&gt;ggplot2&lt;/code&gt; specification. Exactly the same mappings is used in the legend as the main graphic, and so the call to &lt;code&gt;geom_path()&lt;/code&gt; looks similar, except for the different use of x- and y- position. Labels for the legends are generated using &lt;code&gt;annotate()&lt;/code&gt; and again positioned using location information contained in &lt;code&gt;bbox&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_data %&amp;gt;%
  ggplot()+
  geom_sf(data=state_boundaries, fill=&amp;quot;#eeeeee&amp;quot;, colour=&amp;quot;#bcbcbc&amp;quot;, size=0.2)+
  ...
  ...
  ...
  # Plot growth legend lines.
  geom_path(
    data=legend_growth,
    aes(
      x=x+(day_num*6000)-6000, y=y+(growth_rate*50000)-50000,
      group=fips, colour=binned_growth_rate, size=case_rate,
      alpha=binned_growth_rate
      ),
    lineend=&amp;quot;round&amp;quot;
  ) +
  # Text label for growth legend lines.
  geom_text(
    # For appropriate positioning, we manually edit the growth_rate values of
    # Bledsoe, no growth county.
    data=legend_growth %&amp;gt;% filter(day_num == max(county_data$day_num)) %&amp;gt;%
    mutate(growth_rate=if_else(county==&amp;quot;Bledsoe&amp;quot;, -1,growth_rate)),
    aes(
      x=x+(day_num*6000)+10000,y=y+(growth_rate*50000)-50000,
      label=str_wrap(label, 15)
      ),
      alpha=1, size=2.5, hjust=0, vjust=0
      )+
  annotate(&amp;quot;text&amp;quot;,
    x=bbox$xmax-.25*width, y=bbox$ymax+.08*height,
    label=str_wrap(&amp;quot;Line height and colour show change in reported cases
    relative to May 3&amp;quot;,35), alpha=1, size=3.5, hjust=1)+
  # Plot case legend lines.
  geom_path(
    data=legend_case,
    aes(
      x=x+(day_num*6000)-6000+offset_day*width, y=y+(growth_rate*50000)-50000,
      group=paste0(fips,offset), colour=binned_growth_rate, size=offset_rate,
      alpha=binned_growth_rate
    ),
    lineend=&amp;quot;round&amp;quot;
    ) +
  # Text label for case legend lines.
  annotate(&amp;quot;text&amp;quot;, x=bbox$xmax-.88*width, y=bbox$ymax+.04*height, label=&amp;quot;Less&amp;quot;,
    alpha=1, size=2.5, hjust=0.5)+
  annotate(&amp;quot;text&amp;quot;, x=bbox$xmax-.8*width, y=bbox$ymax+.04*height, label=&amp;quot;More&amp;quot;,
    alpha=1, size=2.5, hjust=0.5)+
  annotate(&amp;quot;text&amp;quot;, x=bbox$xmax-.75*width, y=bbox$ymax+.08*height,
  label=str_wrap(&amp;quot;Line thickness shows current number relative to county population&amp;quot;,35),
  alpha=1, size=3.5, hjust=0)+
  # Title.
  annotate(&amp;quot;text&amp;quot;, x=bbox$xmax-.5*width, y=bbox$ymax+.15*height,
  label=&amp;quot;Change in reported cases since May 3&amp;quot;, alpha=1, size=5)+
  ...
  ...
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Data are deeply embedded in most domains and in how we approach most of the world’s problems. Communicating with data is, however, not an easy undertaking. Difficult decisions must be made around how much important detail to sacrifice in favour of clarity and simplicity of message. Visual approaches can help here; giving cues that order and prioritise information and that build explanatory narratives using metaphor and other rhetorical devices. There are stellar examples of this from in-house Data Journalism teams, most obviously in recent evidence-based stories around the pandemic. We have considered some of these and the careful design decisions made when communicating data-driven stories in light of data, audience and intended purpose. Many leading data journalists use &lt;code&gt;ggplot2&lt;/code&gt; as their visulization toolkit of choice and in the technical session we demonstrated how more deliberatively designed graphics can be generated. This somewhat fiddly approach to creating graphics is different from the style of workflow envisaged for &lt;code&gt;ggplot2&lt;/code&gt; in the session on &lt;a href=&#34;../class/04-class/&#34;&gt;Exploratory Data Analysis&lt;/a&gt;. &lt;!-- Thinking in terms of coordinate space in which graphical elements can be variously added is more similar to that required by low-level visualization frameworks such as [processing](https://processing.org/) and [d3](https://d3js.org/).  --&gt; However, as demonstrated through John Burn-Murdoch’s excellent work and also the &lt;a href=&#34;https://bbc.github.io/rcookbook/&#34;&gt;BBC graphics team&lt;/a&gt;, &lt;code&gt;ggplot2&lt;/code&gt;
&lt;em&gt;can&lt;/em&gt; be used for visualization design, making control over annotations, text labels, and embedded graphics important skills to develop.&lt;/p&gt;
&lt;!-- Thinking in terms of coordinate space in which graphical elements can be variously added is more similar to that required by low-level visualization frameworks such as [processing](https://processing.org/) and [d3](https://d3js.org/).


* Examples here were quite selective -- approaches to looking at cases/hospitalisation -- honourable mention to [Hary Stevens](https://www.washingtonpost.com/graphics/2020/world/corona-simulator/) explaining exponential doubling and thus the argument for resitrictions.
 --&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;!-- Concerns around the _reproducibility crisis_ are not simply a function of transparency in methodology and research design. Rather, they relate to a culture and incentive structure whereby scientific claims are conferred with authority when reported within the (slightly backwards) logic of Null Hypothesis Significance Testing (NHST) and _p-values_. We will cover a little on this in session 7 and 8, but for an accessible read on the phenomenon of _p-hacking_ (with interactive graphic) see [this article](https://fivethirtyeight.com/features/science-isnt-broken/#part1) from the excellent [FiveThirtyEight](http://fivethirtyeight.com) website. Again, the upshot of all this introspection is a rethinking of the way in which Statistics is taught in schools and universities, with greater emphasis on understanding through computational approaches rather than traditional equations, formulas and probability tables. Where does `R` fit within this? Simply put: `R` is far better placed than traditional software tools and point-and-click paradigms for supporting computational approaches to statistics -- with a set of methods and libraries for performing simulations and permutation-based tests. --&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-beecham_on_2021&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R., J. Dykes, L. Hama, and N. Lomax. 2021. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;On the use of &lt;span&gt;‘glyphmaps’&lt;/span&gt; for analysing Covid-19 reported cases&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;ISPRS International Journal of Geo-Information&lt;/em&gt; 10 (4).
&lt;/div&gt;
&lt;div id=&#34;ref-beecham_design_2020&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R., J. Dykes, C. Rooney, and W. Wong. 2020. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Design Exposition Discussion Documents for Rich Design Discourse in Applied Visualization&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/span&gt;&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-gleicher_visual_2011&#34; class=&#34;csl-entry&#34;&gt;
Gleicher, Albers, M., and J. Roberts. 2011. &lt;span&gt;“Visual Comparison for Information Visualization. Information Visualization.”&lt;/span&gt; &lt;em&gt;Information Visualization&lt;/em&gt; 10 (4): 289–309.
&lt;/div&gt;
&lt;div id=&#34;ref-healy_data_2018&#34; class=&#34;csl-entry&#34;&gt;
Healy, K. 2018. &lt;em&gt;Data Visualization: A Practical Introduction&lt;/em&gt;. Princeton: Princeton University Press.
&lt;/div&gt;
&lt;div id=&#34;ref-henry_data_2018&#34; class=&#34;csl-entry&#34;&gt;
Henry-Riche, N., C. Hurter, N. Diakopoulos, and S. Carpendale, eds. 2018. In &lt;em&gt;Data-Driven Storytelling&lt;/em&gt;. Boca Raton.
&lt;/div&gt;
&lt;div id=&#34;ref-roth_cartographic_2020&#34; class=&#34;csl-entry&#34;&gt;
Roth, R. 2020. &lt;span&gt;“Cartographic Design as Visual Storytelling: Synthesis and Review of Map-Based Narratives, Genres, and Tropes.”&lt;/span&gt; &lt;em&gt;The Cartographic Journal&lt;/em&gt; 0 (0). Taylor &amp;amp; Francis: 1–32.
&lt;/div&gt;
&lt;div id=&#34;ref-tufte_visual_1983&#34; class=&#34;csl-entry&#34;&gt;
Tufte, E. 1983. &lt;em&gt;The Visual Display of Quantitative Information&lt;/em&gt;. Cheshire, CT: Graphics Press.
&lt;/div&gt;
&lt;div id=&#34;ref-wood_design_2018&#34; class=&#34;csl-entry&#34;&gt;
Wood, J., A. Kachkaev, and J. Dykes. 2018. &lt;span&gt;“Design Exposition with Literate Visualization.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/span&gt;&lt;/em&gt; 25 (1). IEEE: 759–68.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualization for uncertainty analysis</title>
      <link>/class/08-class/</link>
      <pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate>
      <guid>/class/08-class/</guid>
      <description>
&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;script src=&#34;../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;h2&gt;Contents&lt;/h2&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concepts&#34;&gt;Concepts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#uncertainty-visualization&#34;&gt;Uncertainty visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#frequency-framing&#34;&gt;Frequency framing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantifying-uncertainty-in-frequencies&#34;&gt;Quantifying uncertainty in frequencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visualizing-uncertainty-in-frequencies&#34;&gt;Visualizing uncertainty in frequencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-comparisons&#34;&gt;Multiple comparisons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#techniques&#34;&gt;Techniques&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#import&#34;&gt;Import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-icon-arrays&#34;&gt;Plot icon arrays&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generate-bootstrap-estimates-of-parameter-uncertainty&#34;&gt;Generate bootstrap estimates of parameter uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-parameter-estimates-with-uncertainty-information&#34;&gt;Plot parameter estimates with uncertainty information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ensemble-plots-and-hypothetical-outcome-plots&#34;&gt;Ensemble plots and hypothetical outcome plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;knowledge&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Appreciate&lt;/strong&gt; the main challenges and objectives of uncertainty representation.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Learn&lt;/strong&gt; how visualization techniques can be used to support ‘frequency framing’ – the perception of probability and risk.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Understand&lt;/strong&gt; how parameter uncertainty can be estimated computationally.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;practical skills&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Create&lt;/strong&gt; uncertainty visualizations in &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Generate&lt;/strong&gt; estimates of parameter uncertainty using bootstrap resampling.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Apply&lt;/strong&gt; &lt;a href=&#34;https://purrr.tidyverse.org/articles/other-langs.html&#34;&gt;functional-style programming&lt;/a&gt; for working over bootstrap resamples.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Uncertainty is a key preoccupation of those working in statistics and data analysis. A lot of time is spent providing estimates for it, reasoning about it and trying to take it into account when making evidence-based claims and decisions. There are many ways in which uncertainty can enter a data analysis and many ways in which it can be conceptually represented. This session focuses mainly on parameter uncertainty – that is, quantifying and conveying the different possible values that a quantity of interest might take. Intuitively, visualization should help with providing support here. We can use visuals to represent these different values and give greater emphasis to those for which we have greater certainty – to communicate or &lt;em&gt;imply&lt;/em&gt; levels of uncertainty in the background. This is, however, quite challenging. In &lt;a href=&#34;../class/03-class/&#34;&gt;session 3&lt;/a&gt;, we learnt that there is often a gap between the visual encoding of data and its perception and the tendency in standard data graphics to imbue data with marks that over-imply precision. We will consider research in Cartography and Information Visualization on representing uncertainty information, before exploring and applying techniques for quantifying and visually representing parameter uncertainty. We will do so using &lt;a href=&#34;https://data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data&#34;&gt;STATS19&lt;/a&gt; road safety data, exploring how injury severity rates in pedestrian-vehicle crashes vary over time and by geographic area.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    Watch Matt Kay’s OpenVis Conf 2018 talk on quantifying and visualizing uncertainty (and in &lt;code&gt;R&lt;/code&gt;). You may also wish to consult Matt and colleagues’ &lt;a href=&#34;https://mucollective.github.io/visualization/&#34;&gt;probabilistic visualization collection&lt;/a&gt; as you consider the materials in this session.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;https://www.youtube.com/embed/vqzO-9LSoG4&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concepts&lt;/h2&gt;
&lt;div id=&#34;uncertainty-visualization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Uncertainty visualization&lt;/h3&gt;
&lt;p&gt;Cartographers, and more recently those working in Information Visualization, have been concerned for some time with &lt;em&gt;visual variables&lt;/em&gt;, or adopting &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;’s vocabulary &lt;em&gt;visual channels&lt;/em&gt;, that might be used to represent uncertainty information. Figure &lt;a href=&#34;#fig:uncertainty-variables&#34;&gt;1&lt;/a&gt; is adapted from &lt;span class=&#34;citation&#34;&gt;Kinkeldey, MacEachren, and Schiewe (&lt;a href=&#34;#ref-kinkeldey_how_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; and displays visual variables that could be used to encode uncertainty information. Ideally, visual variables for representing uncertainty should be intuitive, logically related to notions of precision and accuracy, whilst also allowing sufficient discriminative power when deployed in data dense visualizations.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Kinkeldey, MacEachren, and Schiewe (&lt;a href=&#34;#ref-kinkeldey_how_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; provides an overview of empirical research (controlled experiments) that explore the effectiveness of proposed visual variables against these criteria. As intuitive signifiers of uncertainty, or lack of precision, &lt;em&gt;fuzziness&lt;/em&gt; and &lt;em&gt;location&lt;/em&gt; have been shown to work well. Slightly less intuitive, but nevertheless successful in terms of discrimination are &lt;em&gt;size&lt;/em&gt;, &lt;em&gt;transparency&lt;/em&gt; and &lt;em&gt;colour value&lt;/em&gt;. &lt;em&gt;Sketchiness&lt;/em&gt; is another intuitive signifier, proposed in &lt;span class=&#34;citation&#34;&gt;Boukhelifa et al. (&lt;a href=&#34;#ref-boukhelifa_evaluating_2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Wood et al. (&lt;a href=&#34;#ref-wood_sketchy_2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;. As with many visual variables, sketchiness is probably best considered as an ordinal visual variable to the extent that there is a limited range of sketchiness levels that can be discriminated. An additional feature of sketchiness is that, different from the other visual variables, it is related to informality – this may be desirable in certain contexts, less so in others &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-wood_sketchy_2012&#34; role=&#34;doc-biblioref&#34;&gt;Wood et al. 2012&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:uncertainty-variables&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/08-class_files/uncertainty_variables.png&#34; alt=&#34;Visual variables that can be used to represent levels of uncertainty information.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Visual variables that can be used to represent levels of uncertainty information.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From within the Uncertainty Visualization literature, a key maxim that consistently appears for successful uncertainty visualization is that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Things that are not precise should not be encoded with symbols that look precise.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The go-to example, which appears in most primers on uncertainty visualization, is the &lt;a href=&#34;https://www.nhc.noaa.gov/refresh/graphics_at1+shtml/030119.shtml?cone#contents&#34;&gt;National Hurricane Center’s&lt;/a&gt; &lt;em&gt;Cone of Uncertainty&lt;/em&gt; graphic (Figure &lt;a href=&#34;#fig:hurricane-vis&#34;&gt;2&lt;/a&gt;) used to represent major storm events. The cone starts at the storm’s current location and spreads out to represent the &lt;em&gt;projected path&lt;/em&gt; of the storm as determined by National Hurricane Center’s modelling. The main problem is that the cone implies that the storm is growing as we move away from its current location, when in fact this is not the case. Instead there is &lt;em&gt;more uncertainty&lt;/em&gt; in the areas that could be affected by the storm the further away those areas are from the storm’s current location. The second problem is that the cone uses strong lines that imply precision. The temptation is to think that anything contained by the cone is unsafe and anything outside of it is safe. This is of course not what is suggested by the model; rather that areas beyond the cone fall outside some chosen threshold probability.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://www.gicentre.net/jwo/index&#34;&gt;Jo Wood’s&lt;/a&gt; redesign, &lt;em&gt;colour value&lt;/em&gt; is used to represent four binned categories of storm probability suggested by the model. In representing the cone of uncertainty on a map, and incorporating geographic context – administrative boundaries – it is also natural to make quite specific and binary judgements about response based on this context. For example, if the cone is &lt;em&gt;close to&lt;/em&gt; but not overlapping a state boundary, a decision might be made not to mount a response/prepare for the hurricane. Jo’s approach is to use curve schematisation &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-van_exploring_2014&#34; role=&#34;doc-biblioref&#34;&gt;Van Goethem et al. 2014&lt;/a&gt;)&lt;/span&gt; to symbolise states using a single line. This provides context but in a way that may discourage binary thinking; precise inferences of location are not possible as the area and state borders are very obviously not exact.&lt;/p&gt;
&lt;!-- 1. Intrinsic vs. Extrinsic. Intrinsic techniques alter existing symbols used to represent uncertainty through manipulation of visual variables. Extrinsic approaches add new objects to the display to depict uncertainty – for example using glyphs or grids.
2. Coincident vs. Adjacent. This describes the organisation of views: whether or not uncertainty is represented in an integrated view (coincident) or in separate views (adjacent).
3. Static vs. Dynamic. Uncertainty might be represented dynamically, for example by alternating between visual representations of a dataset and depictions of its uncertainty.
Kinkeldey et al. (2014) provide a survey of studies that have variously evaluated these visual variables for uncertainty representation.
 --&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:hurricane-vis&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/08-class_files/hurricane_vis.png&#34; alt=&#34;*Cone of Uncertainty* produced by [National Hurricane Center](https://www.nhc.noaa.gov/refresh/graphics_at1+shtml/030119.shtml?cone#contents) and example re-design by [Jo Wood](https://www.gicentre.net/jwo/index).&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: &lt;em&gt;Cone of Uncertainty&lt;/em&gt; produced by &lt;a href=&#34;https://www.nhc.noaa.gov/refresh/graphics_at1+shtml/030119.shtml?cone#contents&#34;&gt;National Hurricane Center&lt;/a&gt; and example re-design by &lt;a href=&#34;https://www.gicentre.net/jwo/index&#34;&gt;Jo Wood&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;!-- The extreme abstraction of glyphs limits interprettion to generalised regional spatial patterns that reflect the uncertainty in the data. More precise inferences of location are not possible as area and state borders are not exactly described. --&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    See &lt;a href=&#34;https://www.gicentre.net/jsndyks&#34;&gt;Jason Dykes’s&lt;/a&gt; &lt;a href=&#34;https://www.gicentre.net/blog/2015/7/17/sketchyuncertainty&#34;&gt;blog post&lt;/a&gt; for discussion of how sketchiness might be applied to some interesting candidate Social Science datasets.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;frequency-framing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Frequency framing&lt;/h3&gt;
&lt;p&gt;The visual variables in Figure &lt;a href=&#34;#fig:uncertainty-variables&#34;&gt;1&lt;/a&gt; could be used to represent different categories of uncertainty information, not just those associated with parameters – for example, locational or temporal uncertainty at data collection. The rest of the session addresses techniques for quantifying, and charting idioms &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;Munzner 2014&lt;/a&gt;)&lt;/span&gt; for representing, parameter uncertainty: estimated injury severity rates for pedestrian-vehicle crashes in our STATS19 road crash dataset.&lt;/p&gt;
&lt;p&gt;Often parameters are represented as probabilities, or &lt;em&gt;relative frequencies&lt;/em&gt; – ratios or percentages describing the probability of some event happening. It is notoriously difficult to develop intuition around these sorts of relative frequencies. In the STATS19 dataset, we might wish to compare the injury severity rate of pedestrain-vehicle road crashes – the proportion of all crashes that resulted in a serious injury or fatality (KSI) – taking place between two local authority areas, say Bristol and Sheffield. There is in fact quite a difference in the injury severity rate between these two local authority areas in 2019: 15% for Bristol (35 out of 228 reported crashes were KSI) versus 50% for Sheffield (124 out of 248 reported crashes were KSI).&lt;/p&gt;
&lt;p&gt;This feels like quite a large difference, but it is difficult to imagine or experience these differences in probabilities when written down or encoded visually using say bar length. &lt;a href=&#34;https://mucollective.github.io/visualization/icon-array/&#34;&gt;Icon arrays&lt;/a&gt; are used extensively in health communication and have been demonstrated to be effective at communicating probabilities of event outcomes. They offload the thinking that happens when comparing ratios – the internal weighing up of numerators and denominators. In the example in Figure &lt;a href=&#34;#fig:icon-arrays-injury&#34;&gt;3&lt;/a&gt;, icon arrays are used to compare the two injury severity rates for Bristol and Sheffield. Each crash is a square and crashes are coloured according to whether they resulted in a serious injury or fatality (KSI, dark red) or slight injury (light red).&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:icon-arrays-injury&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/08-class_files/icon-arrays.png&#34; alt=&#34;Icon array displaying injury severity rates for Pedestrian-Vehicle crashes.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Icon array displaying injury severity rates for Pedestrian-Vehicle crashes.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There are compelling examples of icon arrays being used in data journalism, most obviously to support communication of probabilities in political polling. You might remember that at the time of 2016 US Presidential Election, there was much criticism levelled at pollsters, even the excellent &lt;a href=&#34;https://fivethirtyeight.com/features/why-fivethirtyeight-gave-trump-a-better-chance-than-almost-anyone-else/&#34;&gt;FiveThirtyEight&lt;/a&gt;, for not correctly calling the result. Huffpost gave Trump a 2% chance of winning the election, The New York Times 15% and FiveThirtyEight 28%. Clearly, the Huffpost estimate was really quite off, but thinking about FiveThirtyEight’s prediction, how surprised should we be if an outcome does in fact occur, which is predicted to happen with a probability of almost a third?&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://mucollective.github.io/visualization/risk-theatre/&#34;&gt;risk theatre&lt;/a&gt; (Figure &lt;a href=&#34;#fig:risk-theatre&#34;&gt;4&lt;/a&gt;) is a variant of an icon array. In this case it represents polling probabilities as seats of a theatre – a dark seat represents a Trump victory. If you imagine buying a theatre ticket and being randomly allocated to a seat, how confident would you be about not sitting in a “Trump” seat in the FiveThirtyEight image? The distribution of dark seats suggests that the 28% risk of a Trump victory according to the model is not negligible.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:risk-theatre&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/08-class_files/risk-theatre.jpg&#34; alt=&#34;Risk theatre of different election eve forecasts by Justin Gross as it appears in [Washington Post](https://mucollective.github.io/visualization/risk-theatre/)&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Risk theatre of different election eve forecasts by Justin Gross as it appears in &lt;a href=&#34;https://mucollective.github.io/visualization/risk-theatre/&#34;&gt;Washington Post&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    For an excellent example of icon arrays applied to road safety, read Jo Wood’s blog post on &lt;a href=&#34;https://www.gicentre.net/blog/2013/11/24/risk-cycling-and-denominator-neglect&#34;&gt;risk, cycling and denominator neglect&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantifying-uncertainty-in-frequencies&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quantifying uncertainty in frequencies&lt;/h3&gt;
&lt;p&gt;In the icon arrays above I made little of the fact that the sample size varies between the two recorded crash rates. Partly this was because the differences were reasonably small. When looking at injury severity rates across all local authorities in 2019, however, there is substantial variation in the rates and the sample size. Bromsgrove has a very low injury severity rate based on a small sample size (4%, or one out of 27 crashes resulting in KSI); Cotswold has a very high injury severity rate based on a small sample size (75%, or 14 out of 19 crashes resulting in KSI). With some prior knowledge of these areas, one might expect the difference in KSI rates to be in this direction, but would we expect the difference to be of this order of magnitude? Just eight more KSIs recorded in Bromsgrove makes its KSI rate equivalent to that of Bristol’s.&lt;/p&gt;
&lt;p&gt;Although STATS19 is a population dataset to the extent that it contains data on every crash recorded by the Police, it makes sense that the more data on which our KSI rates are based, the more certainty we have in them being reliable estimates of injury severity – ones that might be used to predict injury severity in future years. So we can treat our observed injury rates as being derived from samples of an (&lt;em&gt;unobtainable&lt;/em&gt;) population. Our calculated injury severity rates are &lt;em&gt;parameters&lt;/em&gt; that try to represent, or estimate, this population.&lt;/p&gt;
&lt;p&gt;Although this formulation might seem unnecessary, from here we can apply some statistical concepts to quantify uncertainty around our parameter estimates. We assume:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The variable of interest, KSI rate, has an unobtainable population mean and standard deviation.&lt;/li&gt;
&lt;li&gt;That a &lt;em&gt;sample&lt;/em&gt; will consist of a set of observations from this unobtainable population and that these samples could vary in size.&lt;/li&gt;
&lt;li&gt;From any &lt;em&gt;sample&lt;/em&gt; we can calculate a mean and standard deviation, which will differ from the population mean and standard deviation.&lt;/li&gt;
&lt;li&gt;That we can derive a &lt;em&gt;sampling distribution&lt;/em&gt; and generate an array of estimates that could be obtained from repeating the sampling process many, many times.&lt;/li&gt;
&lt;li&gt;This range of the &lt;em&gt;sampling distribution&lt;/em&gt; could then be used to quantify how precise are the estimates. Generally the larger the sampling distribution, the more precise – the less uncertain – the estimate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In &lt;a href=&#34;../class/06-class/&#34;&gt;session 6&lt;/a&gt; and &lt;a href=&#34;../class/07-class/&#34;&gt;session 7&lt;/a&gt;, we used Confidence Intervals to estimate uncertainty around regression coefficients. These described the range of the sampling distribution – the range of values that coefficients estimated from a large number of resamples could take. From early stats courses, you might have learnt how these can be calculated using statistical theory. Better still, we can derive these empirically via &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)&#34;&gt;bootstrapping&lt;/a&gt;. A bootstrap resample involves taking a random sample with replacement from the original sample and of the same size as the original sample. From this resample, a parameter estimate can be derived, in this case the KSI rate. And this process can be repeated many times to generate an empirical &lt;em&gt;sampling distribution&lt;/em&gt; for the parameter. The standard error can be calculated from the standard deviation of the sampling distribution. There are several reasons why bootstrapping is a really useful procedure: it can be applied to almost any sample statistic, makes no distributional assumptions and can work on quite complicated sampling designs.&lt;/p&gt;
&lt;p&gt;Presented in Figure &lt;a href=&#34;#fig:bootstrap-selected&#34;&gt;5&lt;/a&gt; are KSI rates with error bars used to display 95% Confidence Intervals generated from a bootstrap procedure in which 1000 resamples were taken with replacement. Upper and lower limits were lifted from .025 and .975 percentile positions of the bootstrap sampling distribution. Assuming that the observed data are drawn from a wider (unobtainable) population, the 95% Confidence Intervals demonstrate that whilst Cotswold recorded a very large KSI rate, sampling variation means that this figure could be much lower (or higher), whereas for Bristol and Sheffield, where our KSI rate is derived from more data, the range of plausible values that the KSI rate might take due to sampling variation is much smaller – there is less uncertainty associated with their KSI rates.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:bootstrap-selected&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/08-class_files/bootstrap-selected.png&#34; alt=&#34;KSI rates for pedestrian-vehicle crashes in selected local authorities with 95% CIs (derived from 1000 resample bootstraps).&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: KSI rates for pedestrian-vehicle crashes in selected local authorities with 95% CIs (derived from 1000 resample bootstraps).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-uncertainty-in-frequencies&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualizing uncertainty in frequencies&lt;/h3&gt;
&lt;p&gt;Error bars are a space-efficient way of conveying parameter uncertainty. However, remembering our maxim for uncertainty visualization – that &lt;em&gt;things that are not precise should not be encoded with symbols that look precise&lt;/em&gt; – they do have problems. The hard borders can lead to a binary/categorical thinking &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-correl_error_2014&#34; role=&#34;doc-biblioref&#34;&gt;Correll and Gleicher 2014&lt;/a&gt;)&lt;/span&gt;. Certain values within a Confidence Interval are more probable than others and so we should endeavour to use a visual encoding that reflects this. Matt Kay’s excellent &lt;a href=&#34;https://mjskay.github.io/ggdist/&#34;&gt;&lt;code&gt;ggdist&lt;/code&gt;&lt;/a&gt; package extends &lt;code&gt;ggplot2&lt;/code&gt; with a range of chart idioms for representing these sorts of intervals. In Figure &lt;a href=&#34;#fig:selected-uncertainty&#34;&gt;6&lt;/a&gt; I have replaced the error bars with &lt;em&gt;half eye plots&lt;/em&gt; and &lt;em&gt;interval bars&lt;/em&gt;, which give greater visual saliency to parameter estimates that are more likely.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:selected-uncertainty&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/08-class_files/selected-uncertainty.png&#34; alt=&#34;KSI rates for pedestrian-vehicle crashes in selected local authorities with uncertainty estimates.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: KSI rates for pedestrian-vehicle crashes in selected local authorities with uncertainty estimates.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The STATS19 dataset is released annually, and given the wide uncertainty bands for certain local authorities, it might be instructive to explore the stability of local authority KSI rates year-on-year. In Figure &lt;a href=&#34;#fig:bootstrap-selected&#34;&gt;5&lt;/a&gt; these KSI rates are represented with the bold line and the faint lines are superimposed bootstrap resamples. The overall line probably most clearly demonstrates volatility in the KSI rates for Cotswold and Bromsgrove due to small sample sizes. The observed increase in KSI rates for Sheffield since 2015 does appear to be a genuine one, although may also be affected by uncertainty around data collection – &lt;a href=&#34;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/904698/rrcgb-provisional-results-2019.pdf&#34;&gt;changes to how police record injury severity&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:temporal-uncertainty&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/08-class_files/temporal-uncertainty.png&#34; alt=&#34;Year-on-yar KSI rates for pedestrian-vehicle crashes in selected local authorities with bootstrap resamples superimposed.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: Year-on-yar KSI rates for pedestrian-vehicle crashes in selected local authorities with bootstrap resamples superimposed.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The superimposed lines in the figure above are a form of ensemble visualization. An alternative approach might have been to animate over the bootstrap resamples to generate a Hypoethetical Outcome Plot (HOP) &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hullman_hypothetical_2015&#34; role=&#34;doc-biblioref&#34;&gt;Hullman, Resnick, and Adar 2015&lt;/a&gt;)&lt;/span&gt;. HOPs convey a sense of uncertainty by animating over random draws of a distribution. As there is no single outcome to anchor to (although I have actually included one below), HOPs force viewers to account for uncertainty, recognising that less probable outcomes may be possible – essentially to think distributionally (e.g. Figure &lt;a href=&#34;#fig:hop-uncertainty&#34;&gt;8&lt;/a&gt;).&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:hop-uncertainty&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/08-class_files/temporal-uncertainty.gif&#34; alt=&#34;HOP of year-on-yar KSI rates for pedestrian-vehicle crashes in selected local authorities.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: HOP of year-on-yar KSI rates for pedestrian-vehicle crashes in selected local authorities.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    For an excellent discussion of HOPs, with a popular example that originally appeared in New York Times looking at survey sampling error, I highly recommend &lt;a href=&#34;https://medium.com/@uwdata/hypothetical-outcome-plots-hops-help-users-separate-signal-from-noise-870d4e2b75d7&#34;&gt;Alex Kale and Jess Hullman’s blogpost&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-comparisons&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiple comparisons&lt;/h3&gt;
&lt;p&gt;In road safety monitoring, a common ambition is to compare crash rates across local authorities on a map. We might represent injury severity rates as Risk Ratios (RR) comparing the observed injury severity in each local authority to a benchmark, say the injury severity rate we would expect to see nationally. RRs are an intuitive measure of effect size: &lt;span class=&#34;math inline&#34;&gt;\(RRs &amp;gt;1\)&lt;/span&gt; indicate that the injury severity rate is greater than the national average; &lt;span class=&#34;math inline&#34;&gt;\(RRs &amp;lt;1\)&lt;/span&gt; that it is less than the national average. As they are a ratio of ratios, and therefore agnostic to sample size, RRs can nevertheless be unreliable. Two ratios might be compared that have very different sample sizes and no compensation is made for the one that contains more data. We know this to be a problem in our dataset and so we will want to generate confidence intervals for our RRs.&lt;/p&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:rrs&#34;&gt;9&lt;/a&gt;, RRs for each local authority is represented with a &lt;span style=&#34;font-weight:bolder&#34;&gt;|&lt;/span&gt; icon, which is then encoded according RR values by continuously varying icon angles: &lt;span class=&#34;math inline&#34;&gt;\(RRs &amp;gt;1\)&lt;/span&gt; are angled to the right &lt;span style=&#34;font-weight:bold&#34;&gt;/&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(RRs &amp;lt;1\)&lt;/span&gt; to the left &lt;span style=&#34;font-weight:bolder&#34;&gt;\&lt;/span&gt;. You might remember this approach being used to represent electoral Swing in &lt;a href=&#34;../class/03-class/&#34;&gt;session 3&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:rrs&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/08-class_files/rrs.png&#34; alt=&#34;Risk Ratios comparing local authority KSI rates to the GB average in 2019. Significance based on 90% bootstrap CI.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: Risk Ratios comparing local authority KSI rates to the GB average in 2019. Significance based on 90% bootstrap CI.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Whilst this is likely successful at conveying these effect sizes, we will want to de-emphasise those for which there is less certainty. One way of doing this would be to vary the lightness of the icons according to estimated standard errors. In Epidemiology and other domains, more common practice is to use null hypothesis testing (NHST). Here we set up an assumption (null hypothesis) that the difference in the injury severity rate for any local authority and the national average equals zero (&lt;span class=&#34;math inline&#34;&gt;\(RR = 1.0\)&lt;/span&gt;). Our bootstrap Confidence Interval is then used to derive probabilities that random sampling generate data similar to what was observed if this null hypothesis were true. The confidence interval can be used to derive this probability. If a 95% confidence interval excludes the expected parameter value under the null hypothesis (i.e. &lt;span class=&#34;math inline&#34;&gt;\(RR \not= 1.0\)&lt;/span&gt;), then the null hypothesis is rejected and the observed RR is deemed statistically significant. In Figure &lt;a href=&#34;#fig:rrs&#34;&gt;9&lt;/a&gt; RRs &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;1\)&lt;/span&gt; that are statistically significant coloured red &lt;span style=&#34;color:#DB534D;font-weight:bold&#34;&gt;/&lt;/span&gt;, RRs &lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;1\)&lt;/span&gt; that are statistically significant coloured red are coloured blue &lt;span style=&#34;color:#3879A1;font-weight:bolder&#34;&gt;\&lt;/span&gt; and those that are not statistically significant are coloured grey (&lt;span style=&#34;color:#aeaeae;font-weight:bold&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#aeaeae;font-weight:bolder&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#aeaeae;font-weight:bolder&#34;&gt;\&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;A problem with this approach is one familiar to Statisticians, but that is rarely addressed in visual data analyses: the &lt;a href=&#34;https://en.wikipedia.org/wiki/Multiple_comparisons_problem&#34;&gt;multiple comparison problem&lt;/a&gt;. Whenever a NHST is made, there is a chance that the result observed is in fact a false alarm. In the plot above which uses a 90% confidence level, the “false positive rate” is expected to be 10% or 1/10. When many tests are considered simultaneously, as in Figure &lt;a href=&#34;#fig:rrs&#34;&gt;9&lt;/a&gt;, the number of these false alarms begins to accumulate. There are corrections that can be applied for this: test statistics can be adjusted and made more conservative. But these corrections have consequences. Too severe a correction can result in statistical tests that are underpowered and result in an elevated false negative rate, where a statistical test fails to detect an effect that truly exists. See &lt;span class=&#34;citation&#34;&gt;Brunsdon and Charlton (&lt;a href=&#34;#ref-brunsdon_an_2011&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt; for an interesting discussion of this in the context of mapping crime rates.&lt;/p&gt;
&lt;p&gt;So there is no canonical solution to this multiple comparison problem, which I see a lot in Geography where health and other outcomes are mapped visually. Presenting the RRs in their spatial context, and providing full information around RRs that are not significant, helps with making uncertainty type judgements. For example, we can attach more certainty to RRs that are labelled statistically significant and whose direction is consistent with its neighbours than those that are exceptional from their neighbours. But this is certainly an interesting area for those working in visual data analysis.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Again, for an excellent discussion of the multiple comparison problem and visualization, read &lt;a href=&#34;https://medium.com/hci-design-at-uw/multiple-perspectives-on-the-multiple-comparisons-problem-in-visual-analysis-df7493818bbd&#34;&gt;this post by Jess Hullman and Jeff Heer&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;techniques&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Techniques&lt;/h2&gt;
&lt;p&gt;The technical element demonstrates how the plots and uncertainty estimates of STATS19 road crash data introduced in the session can be produced. We will again make use of &lt;a href=&#34;https://adv-r.hadley.nz/fp.html#:~:text=Functional&#34;&gt;functional programming&lt;/a&gt; approaches via the &lt;a href=&#34;https://purrr.tidyverse.org/&#34;&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/a&gt; package, most obviously for generating and working over boostrap resamples.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the &lt;a href=&#34;../homework/08-homework_files/08-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 08-template.Rmd&lt;/a&gt; file for this session and save it to the &lt;code&gt;reports&lt;/code&gt; folder of your &lt;code&gt;vis-for-gds&lt;/code&gt; project.&lt;/li&gt;
&lt;li&gt;Open your &lt;code&gt;vis-for-gds&lt;/code&gt; project in RStudio and load the template file by clicking &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Open File ...&lt;/code&gt; &amp;gt; &lt;code&gt;reports/08-template.Rmd&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;import&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Import&lt;/h3&gt;
&lt;p&gt;The template file lists the required packages – &lt;code&gt;tidyverse&lt;/code&gt;, &lt;code&gt;sf&lt;/code&gt;, &lt;code&gt;tidymodels&lt;/code&gt; (for working with the bootstraps), &lt;a href=&#34;https://mjskay.github.io/ggdist/&#34;&gt;&lt;code&gt;ggdist&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://pkg.mitchelloharawild.com/distributional/&#34;&gt;&lt;code&gt;distributional&lt;/code&gt;&lt;/a&gt; for generating plots of parameter uncertainty and &lt;a href=&#34;https://gganimate.com/index.html&#34;&gt;&lt;code&gt;gganimate&lt;/code&gt;&lt;/a&gt; for the hypothetical outcome plot. You may not have saved the dataset of STATS19 pedestrian-vehicle road crashes generated in session 4 (&lt;code&gt;ped_veh&lt;/code&gt;) and so I have placed this on a repo for download. Code for loading these data into your session is in the &lt;a href=&#34;../homework/08-homework_files/08-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 08-template.Rmd&lt;/a&gt; file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-icon-arrays&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot icon arrays&lt;/h3&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:icon-arrays-technical&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/08-class_files/icon-arrays-technical.png&#34; alt=&#34;Icon array displaying injury severity rates for Pedestrian-Vehicle crashes.&#34; width=&#34;70%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: Icon array displaying injury severity rates for Pedestrian-Vehicle crashes.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I quickly explored extensions to &lt;code&gt;ggplot2&lt;/code&gt; for generating icon arrays, but found no stand-out package. They can be generated reasonably easily using &lt;code&gt;geom_tile()&lt;/code&gt; and some standard data generation functions. In the example in Figure &lt;a href=&#34;#fig:icon-arrays-injury&#34;&gt;3&lt;/a&gt; the icon arrays varied with sample size of the two Local Authorities being compared. A more generalisable approach would be for icon arrays to be regularly sized and used to represent probabilities of, in this case injury severity, if a stated rate of crashes occurred (e.g. Figure &lt;a href=&#34;#fig:icon-arrays-technical&#34;&gt;10&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We need to generate the array data: a data frame of array locations (candidate crashes) with values representing whether the crash is slight or KSI depending on the observed KSI rate. In the code below, we set up a 10x10 grid of row and column locations and populate these with values for the selected local authorities (Oxford and Fareham) using base R’s &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-patched/library/base/html/sample.html&#34;&gt;&lt;code&gt;sample()&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;array_data &amp;lt;- tibble(
  row=rep(1:10, times=1, each=10),
  col=rep(1:10, times=10, each=1),
  Oxford=sample(c(FALSE,TRUE), size=100, replace=TRUE, prob=c(.83,.17)),
  Fareham=sample(c(FALSE,TRUE), size=100, replace=TRUE, prob=c(.59,.41))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;array_data %&amp;gt;%
  pivot_longer(cols=c(Oxford,Fareham), names_to=&amp;quot;la&amp;quot;, values_to=&amp;quot;is_ksi&amp;quot;) %&amp;gt;%
  ggplot(aes(x=row,y=col, fill=is_ksi)) +
  geom_tile(colour=&amp;quot;#ffffff&amp;quot;, size=1) +
  scale_fill_manual(values=c(&amp;quot;#fee0d2&amp;quot;,&amp;quot;#de2d26&amp;quot;), guide=FALSE)+
  facet_wrap(~la)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot specification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: The array data, with &lt;code&gt;pivot_longer()&lt;/code&gt; so that we can facet by local authority.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: x- and y-position according to the array locations and filled on whether the sampled crash is KSI or slight.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_tile()&lt;/code&gt; for drawing square icons.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt;: &lt;code&gt;scale_fill_manual()&lt;/code&gt; is supplied with values that are dark (KSI) and light (slight) red.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Facets&lt;/strong&gt;: &lt;code&gt;facet_wrap()&lt;/code&gt; for faceting on local authority.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Tiles are given large, white borders (&lt;code&gt;geom_tile(colour=&#34;#ffffff&#34;, size=1)&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;generate-bootstrap-estimates-of-parameter-uncertainty&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generate bootstrap estimates of parameter uncertainty&lt;/h3&gt;
&lt;p&gt;The code for generating bootstrap resamples, stored in &lt;code&gt;rate_boots&lt;/code&gt;, initially looks formidable. It is a template that is nevertheless quite generalisable, and so once learnt can be extended and applied to suit different use cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rate_boots &amp;lt;- ped_veh %&amp;gt;%
  mutate(
    is_ksi=accident_severity!=&amp;quot;Slight&amp;quot;,
    year=lubridate::year(date)
  ) %&amp;gt;%
  filter(year==2019,
         local_authority_district %in% c(&amp;quot;Bristol, City of&amp;quot;, &amp;quot;Sheffield&amp;quot;, &amp;quot;Bromsgrove&amp;quot;, &amp;quot;Cotswold&amp;quot;)
         ) %&amp;gt;%
  select(local_authority_district, is_ksi) %&amp;gt;%
  nest(-local_authority_district) %&amp;gt;%
  mutate(la_boot=map(data, bootstraps, times=1000, apparent=TRUE)) %&amp;gt;%
  select(-data) %&amp;gt;%
  unnest(la_boot) %&amp;gt;%
  mutate(
    is_ksi=map(splits, ~ analysis(.) %&amp;gt;% pull(is_ksi)),
    ksi_rate=map_dbl(is_ksi, ~mean(.x)),
    sample_size=map_dbl(is_ksi, ~length(.x))
  ) %&amp;gt;%
  select(-c(splits, is_ksi))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Code description:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Setup&lt;/strong&gt;: The first mutate is straightforward – we generate a binary &lt;code&gt;is_ksi&lt;/code&gt; variable identifying whether the crash resulted in a KSI and extract the year from the crash &lt;code&gt;date&lt;/code&gt;. We then filter on crashes that took place in 2019 and the four comparator local authorities. In order to generate bootstrap resamples for each local authority, we need to &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyr/vignettes/nest.html&#34;&gt;&lt;code&gt;nest()&lt;/code&gt;&lt;/a&gt; on local authority. You will remember that &lt;code&gt;nest()&lt;/code&gt; creates a special type of column (a &lt;code&gt;list-column&lt;/code&gt;) in which the values of the column is a list of data frames – in this case the crash data for each local authority. So running the code up to and including the &lt;code&gt;nest()&lt;/code&gt;, you will notice that a data frame is returned which contains four rows, corresponding to the filtered local authorities, and a &lt;code&gt;list-column&lt;/code&gt; called &lt;code&gt;data&lt;/code&gt;, each element of which contains a data frame of varying dimensions (lengths), according to the number of crashes recorded in each local authority.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generate bootstraps resamples&lt;/strong&gt;: In the &lt;code&gt;mutate()&lt;/code&gt; that follows &lt;code&gt;purrr&lt;/code&gt;’s &lt;a href=&#34;https://purrr.tidyverse.org/reference/map.html&#34;&gt;&lt;code&gt;map()&lt;/code&gt;&lt;/a&gt; function is used to iterate over the list of datasets and the &lt;a href=&#34;https://rsample.tidymodels.org/reference/bootstraps.htmlfit&#34;&gt;&lt;code&gt;bootstraps()&lt;/code&gt;&lt;/a&gt; function is used to generate 1000 bootstrap resamples for each nested dataset. The new column &lt;code&gt;la_boot&lt;/code&gt; is a &lt;code&gt;list-column&lt;/code&gt; this time containing a list of bootstrap datasets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculate sample estimates&lt;/strong&gt;: We &lt;code&gt;unnest()&lt;/code&gt; the &lt;code&gt;la_boot&lt;/code&gt; column to return a dataset with a row for each boostrap resample and a &lt;code&gt;list-column&lt;/code&gt; named &lt;code&gt;splits&lt;/code&gt; which contains the bootstrap data. Again we &lt;code&gt;map()&lt;/code&gt; over each element of &lt;code&gt;splits&lt;/code&gt; to calculate the &lt;code&gt;ksi_rate&lt;/code&gt; for each of the bootstrap datasets. The first call to &lt;code&gt;map()&lt;/code&gt; extracts the &lt;code&gt;is_ksi&lt;/code&gt; variable; the second is just a convenient way of calculating a rate from this (remembering that &lt;code&gt;is_ksi&lt;/code&gt; is a binary variable); the third collects the sample size for each of the bootstraps which of course is the same as the number of crashes recorded for each local authority.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-parameter-estimates-with-uncertainty-information&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot parameter estimates with uncertainty information&lt;/h3&gt;
&lt;p&gt;With &lt;a href=&#34;https://mjskay.github.io/ggdist/&#34;&gt;&lt;code&gt;ggdist&lt;/code&gt;&lt;/a&gt;, the code for generating the plots of KSI rates with estimates of parameter uncertainty is straightforward and very similar to the &lt;a href=&#34;../class/06-class/#extracting-and-representing-model-parameters-1&#34;&gt;error bar plots&lt;/a&gt; in the previous two sessions.&lt;/p&gt;
&lt;p&gt;Plot code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rate_boots %&amp;gt;%
  group_by(local_authority_district) %&amp;gt;%
  mutate(std.error=sd(ksi_rate)) %&amp;gt;%
  filter(id==&amp;quot;Apparent&amp;quot;) %&amp;gt;%
  ggplot(aes(x=reorder(local_authority_district, ksi_rate), y=ksi_rate)) +
  stat_dist_gradientinterval(
    aes(dist = dist_normal(mu=ksi_rate, sigma=std.error)),
    point_size = 1.5
    ) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot specification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: The &lt;code&gt;rate_boots&lt;/code&gt; data frame is grouped by local authority and in the &lt;code&gt;mutate()&lt;/code&gt; we calculate an estimate of bootstrap standard error, the standard deviation of the sampling distribution, and filter all rows where &lt;code&gt;id==&#34;Apparent&#34;&lt;/code&gt; – this contains the KSI rate for the observed (unsampled) data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: x- position varies according to local authority and y-position according to KSI rate. The estimated KSI rate and bootsrtap standard error is also passed to &lt;code&gt;stat_dist_gradientinterval()&lt;/code&gt;, the &lt;code&gt;ggdist&lt;/code&gt; function for producing gradient plots.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;stat_dist_gradientinterval()&lt;/code&gt; for drawing the gradients and point estimates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: &lt;code&gt;coord_flip()&lt;/code&gt; for easy reading of local authority names.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;ensemble-plots-and-hypothetical-outcome-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ensemble plots and hypothetical outcome plots&lt;/h3&gt;
&lt;p&gt;To generate bootstrap resamples on local authority and year, necessary for the year-on-year analysis, we can use the same template for &lt;code&gt;rate_boots&lt;/code&gt;, the only difference is that we &lt;code&gt;select()&lt;/code&gt; and &lt;code&gt;nest()&lt;/code&gt; on the &lt;code&gt;year&lt;/code&gt; as well as &lt;code&gt;local_authority_district&lt;/code&gt; column. You may also not wish to resample 1000 times for each year and local authority as this tends to slow down the computation (for obvious reasons).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rate_boots_temporal &amp;lt;- ped_veh %&amp;gt;%
  ...
  ... %&amp;gt;%
  select(local_authority_district, is_ksi, year) %&amp;gt;%
  nest(-c(local_authority_district, year)) %&amp;gt;%
  ...
  ...
  ..&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ensemble plot is again reasonably straightforward:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rate_boots_temporal %&amp;gt;%
  ggplot(aes(x=year, y=ksi_rate)) +
  geom_line(data=. %&amp;gt;%  filter(id==&amp;quot;Apparent&amp;quot;), aes(group=id), size=.7) +
  geom_line(data=. %&amp;gt;%  filter(id!=&amp;quot;Apparent&amp;quot;), aes(group=id), alpha=.05, size=.2) +
  facet_wrap(~local_authority_district)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot specification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: The &lt;code&gt;rate_boots_temporal&lt;/code&gt; data frame. Note that we include two line layers, one with the observed data (&lt;code&gt;data=. %&amp;gt;%  filter(id==&#34;Apparent&#34;&lt;/code&gt;), one with the bootsrap data (&lt;code&gt;data=. %&amp;gt;%  filter(id!=&#34;Apparent&#34;&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: x- position varies according to year, y-position according to KSI rate.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_line&lt;/code&gt; for drawing lines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Facets&lt;/strong&gt;: &lt;code&gt;facet_wrap()&lt;/code&gt; for faceting on local authority.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: The bootstrap lines are de-emphasised by making the &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt; channels very small.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Hypothetical Outcome Plot (HOP) can be easily created using the &lt;a href=&#34;https://gganimate.com/index.html&#34;&gt;&lt;code&gt;gganimate&lt;/code&gt;&lt;/a&gt; package, simply by adding a call to &lt;a href=&#34;https://gganimate.com/reference/transition_states.html&#34;&gt;&lt;code&gt;transition_states()&lt;/code&gt;&lt;/a&gt; at the end of the plot specification:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rate_boots_temporal %&amp;gt;%
  filter(id!=&amp;quot;Apparent&amp;quot;) %&amp;gt;%
  ggplot(aes(x=year, y=ksi_rate)) +
  geom_line(aes(group=id), colour=site_colours$primary, size=.6) +
  transition_states(id, 0,1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Uncertainty is fundamental data analysis. Most statisticians and data scientists work on approaches to providing quantitive estimates of uncertainty, reasoning about uncertainty and communicating uncertainty so that it can be taken into account when making evidence-based claims and decisions. Through an analysis of injury severity in the STATS19 road crash dataset, this session has introduced techniques for quantifying and visually representing parameter uncertainty. There has been much activity in the Information Visualization and Data Journalism communities focussed on uncertainty communication – on developing approaches that promote intuition and allow users to experience uncertainty. We have covered some of these and demonstrated how they could be incorporated into our road safety analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-boukhelifa_evaluating_2012&#34; class=&#34;csl-entry&#34;&gt;
Boukhelifa, N., A. Bezerianos, T. Isenberg, and J. Fekete. 2012. &lt;span&gt;“Evaluating Sketchiness as a Visual Variable for the Depiction of Qualitative Uncertainty.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/em&gt; 18 (12): 2769–78.
&lt;/div&gt;
&lt;div id=&#34;ref-brunsdon_an_2011&#34; class=&#34;csl-entry&#34;&gt;
Brunsdon, C., and M. Charlton. 2011. &lt;span&gt;“An Assessment of the Effectiveness of Multiple Hypothesis Testing for Geographical Anomaly Detection.”&lt;/span&gt; &lt;em&gt;Environment and Planning B: Planning and Design&lt;/em&gt; 38 (2): 216–30.
&lt;/div&gt;
&lt;div id=&#34;ref-correl_error_2014&#34; class=&#34;csl-entry&#34;&gt;
Correll, M., and M. Gleicher. 2014. &lt;span&gt;“Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/em&gt; 20 (12): 2142–51.
&lt;/div&gt;
&lt;div id=&#34;ref-hullman_hypothetical_2015&#34; class=&#34;csl-entry&#34;&gt;
Hullman, J., P. Resnick, and E. Adar. 2015. &lt;span&gt;“Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering.”&lt;/span&gt; &lt;em&gt;PLOS ONE&lt;/em&gt; 10 (11).
&lt;/div&gt;
&lt;div id=&#34;ref-kinkeldey_how_2014&#34; class=&#34;csl-entry&#34;&gt;
Kinkeldey, C., A. MacEachren, and J/ Schiewe. 2014. &lt;span&gt;“How to Assess Visual Communication of Uncertainty? A Systematic Review of Geospatial Uncertainty Visualisation User Studies.”&lt;/span&gt; &lt;em&gt;The Cartographic Journal&lt;/em&gt; 51 (4). Taylor &amp;amp; Francis: 372–86.
&lt;/div&gt;
&lt;div id=&#34;ref-munzner_visualization_2014&#34; class=&#34;csl-entry&#34;&gt;
Munzner, T. 2014. &lt;em&gt;Visualization Analysis and Design&lt;/em&gt;. AK Peters Visualization Series. Boca Raton, FL: CRC Press.
&lt;/div&gt;
&lt;div id=&#34;ref-van_exploring_2014&#34; class=&#34;csl-entry&#34;&gt;
Van Goethem, A., W. Meulemans, B. Speckmann, and J. Wood. 2014. &lt;span&gt;“Exploring Curved Schematization.”&lt;/span&gt; &lt;em&gt;IEEE Pacific Visualization Symposium&lt;/em&gt;. IEEE Computer Society, 1–8.
&lt;/div&gt;
&lt;div id=&#34;ref-wood_sketchy_2012&#34; class=&#34;csl-entry&#34;&gt;
Wood, J., P. Isenberg, T. Isenberg, J. Dykes, N. Boukhelifa, and A. Slingsby. 2012. &lt;span&gt;“Sketchy Rendering for Information Visualization.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/em&gt; 18 (12): 2749–58.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualization for model building 2: Expose, estimate, evaluate</title>
      <link>/class/07-class/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      <guid>/class/07-class/</guid>
      <description>
&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;script src=&#34;../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;h2&gt;Contents&lt;/h2&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concepts&#34;&gt;Concepts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#geographic-dependency-and-non-stationarity&#34;&gt;Geographic dependency and non-stationarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#representing-geographic-context&#34;&gt;Representing geographic context&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geographic-context-as-grouped-nuisance-term&#34;&gt;Geographic context as grouped nuisance term&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geographic-context-as-grouped-effects&#34;&gt;Geographic context as grouped effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#addressing-estimate-volatility&#34;&gt;Addressing estimate volatility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geographic-context-as-continuous-effects&#34;&gt;Geographic context as continuous effects&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#techniques&#34;&gt;Techniques&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#import&#34;&gt;Import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-fe&#34;&gt;Model FE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-interaction&#34;&gt;Model Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-tidy-models&#34;&gt;Plot tidy models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;knowledge&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Understand&lt;/strong&gt; two categories of geographic effect in regression modelling: geographical &lt;em&gt;dependence&lt;/em&gt; in values and &lt;em&gt;non-stationarity&lt;/em&gt; in processes.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Learn&lt;/strong&gt; how linear regression models can be updated to account for and explore these two effects.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;practical skills&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Update&lt;/strong&gt; linear regression models in R with Fixed Effect (FE) and Interaction terms.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Extract&lt;/strong&gt; model outputs and diagnostics in a &lt;a href=&#34;https://vita.had.co.nz/papers/tidy-data.pdf&#34;&gt;&lt;code&gt;tidy&lt;/code&gt;&lt;/a&gt; manner.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Apply&lt;/strong&gt; &lt;a href=&#34;https://purrr.tidyverse.org/articles/other-langs.html&#34;&gt;functional-style programming&lt;/a&gt; for working over multiple model outputs.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The previous session finished by identifying geographic patterning in the residuals of our multivariate regression model that attempts to explain variation in constituency-level Leave voting. Geographic patterning in residuals is common – and expected – in area-level analysis, but is problematic as it indicates &lt;em&gt;bias&lt;/em&gt; – that Leave voting is better represented in certain areas than in others because the model ignores some systematic grouping context. This session is again presented as a narrated data analysis – the &lt;a href=&#34;../class/06-class/#concepts-1/&#34;&gt;Concepts&lt;/a&gt; element is structured around the data analysis and the &lt;a href=&#34;../class/06-class/#techniques-1/&#34;&gt;Techniques&lt;/a&gt; on working with regression models in R. We will extend our regression model to adjust for geographic context, using visualization to represent and evaluate model outputs.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Watch &lt;a href=&#34;https://www.youtube.com/watch?v=uw1Tag08dK4&#34;&gt;Heather Krause’s&lt;/a&gt; excellent OpenVisConf 2018 talk on, amongst other things, why it is important to account for this sort of grouping context.&lt;/p&gt;&lt;/p&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;https://www.youtube.com/embed/uw1Tag08dK4&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concepts&lt;/h2&gt;
&lt;div id=&#34;geographic-dependency-and-non-stationarity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Geographic dependency and non-stationarity&lt;/h3&gt;
&lt;p&gt;In the last session we identified two ways in which geographic grouping in residuals can be understood:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Spatial dependence&lt;/em&gt; in &lt;strong&gt;variable values&lt;/strong&gt;. The geography of GB is quite socially distinctive, so it is reasonable to expect spatial dependence in observed demographic values. For example, the &lt;em&gt;range&lt;/em&gt; in variables measuring relative employment in &lt;em&gt;heavy industry&lt;/em&gt;, or residents that are &lt;em&gt;white&lt;/em&gt;, is likely to be bounded to economic regions and metropolitan-peripheral regional contexts.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Spatial non-stationarity&lt;/em&gt; in &lt;strong&gt;processes&lt;/strong&gt;. This is where associations between variables might be geographically grouped: that the associations vary for different parts of the country and so there is heterogeneity in process. For example, high levels of &lt;em&gt;EU-born&lt;/em&gt; migration might affect political attitudes, and thus area-level voting, differently in different parts of the country.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;‘Global’ models that do not take these types of structure and effect into account may misrepresent the processes they are trying to capture and hide more subtle insights into phenomena. And so this session tries to (briefly) address both.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;representing-geographic-context&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Representing geographic context&lt;/h3&gt;
&lt;p&gt;There are different ways of constituting geographic context. We have talked about patterning in residuals as being &lt;em&gt;spatial&lt;/em&gt;, with values varying smoothly and continuously depending on location. This might be the case, and spatial autocorrelation is present in almost all datasets (&lt;a href=&#34;https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography&#34;&gt;e.g. Tobler’s first law&lt;/a&gt;). But given the phenomena we are studying – variation in political voting behaviour and demographic composition of constituencies deliberately linked to &lt;a href=&#34;https://ppr.lse.ac.uk/articles/10.31389/lseppr.4/&#34;&gt;&lt;em&gt;places&lt;/em&gt;&lt;/a&gt; left-behind by structural-economic change – it also plausible that distinct contexts are linked to regions. The residuals in the &lt;a href=&#34;http://127.0.0.1:4321/class/06-class/#exploring-bias-1&#34;&gt;previous session’s Map LineUp&lt;/a&gt; – also below – do seem to be grouped by regional boundaries, particularly Scotland looks categorically different (for obvious reasons). This suggests that geographic context might be usefully represented as a &lt;em&gt;category&lt;/em&gt; rather than continuous variable (location in &lt;em&gt;x,y&lt;/em&gt;). For most of the session we will represent geographic context as a &lt;em&gt;regional&lt;/em&gt; grouping and cover approaches both to modelling &lt;em&gt;spatial dependence&lt;/em&gt; in &lt;strong&gt;values&lt;/strong&gt; and &lt;em&gt;spatial non-stationarity&lt;/em&gt; in &lt;strong&gt;processes&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:plot-lineup&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/06-class_files/lineups.png&#34; alt=&#34;Map LineUp of residuals in which the ‘real’ dataset is presented alongside 8 decoy plots generated by randomly permuting the observed residuals around constituencies.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Map LineUp of residuals in which the ‘real’ dataset is presented alongside 8 decoy plots generated by randomly permuting the observed residuals around constituencies.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;geographic-context-as-grouped-nuisance-term&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Geographic context as grouped nuisance term&lt;/h3&gt;
&lt;p&gt;A common approach to treating geographic &lt;em&gt;dependence&lt;/em&gt; in the &lt;strong&gt;values&lt;/strong&gt; of variables is to model geographic context as a Fixed Effect (FE).&lt;/p&gt;
&lt;p&gt;A dummy variable is created for each group (region in our case), and every region receives a constant; you can think of this as a separate intercept for its regression line. Any group-level sources of variation in the outcome are collapsed into the FE variable, which means that regression coefficients are not complicated by this more messy variation – they now capture the association between demographics and Leave after adjusting for systematic differences in the Leave vote due to region. So, for example, we know that Scotland is politically different from the rest of GB and that this appears to drag down the observed Leave vote for its constituencies, and so the constant term on region adjusts for this and prevents the estimated regression coefficients (inferred associations between variables) from being affected. The constant term also allows you to estimate the ‘base level’ of the outcome for each grouping variable – e.g. net of demographic composition, the expected Leave vote in a particular region.&lt;/p&gt;
&lt;p&gt;The linear regression model introduced last week can be easily extended with the FE term (&lt;span class=&#34;math inline&#34;&gt;\(\textcolor{highlight}{\gamma_{j}}\)&lt;/span&gt;). For a single variable model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
       y_{i}&amp;amp;= \textcolor{highlight}{\gamma_{j}}  + \beta_{1}x_{i1} + \varepsilon_{i}  \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we now estimate the Leave vote in each constituency (&lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;) as a function of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textcolor{highlight}{\gamma_{j}}\)&lt;/span&gt;, a constant term similar to an intercept for region &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}=\beta_{1}x_{i1}\)&lt;/span&gt;, the slope, indicating in which direction and to what extent some explanatory variable measured at constituency &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is associated with Leave, &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i}\)&lt;/span&gt;, the difference between &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; (the observed value) at constituency &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and the &lt;em&gt;unobservable&lt;/em&gt; true population value of the Leave vote in that constituency (statistical error)&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- To illustrate this, Figure \@ref(fig:scatters-fe) is an updated set of scatterplots now annotated with coefficients, $R^2$ and coefficient values for single variable linear regression models fit separately for each explanatory variable with a region FE. The $R^2$ is extremely high as we are collapsing variation assumed to be between regions into the FE term.

Plotting the virtual regression lines -- regression lines offset by the FE constant -- is interesting as it tells us a little about the nature of association in our explanatory variables. For the *degree-educated* and *professional* variables especially, regional *dependence* in Leave voting needs to be adjusted for -- once taken into account, there is a very strong association between *degree-education* and *Leave* -- most obvious in the lines for Scotland (red).  Although we are not yet modelling for this explicitly, eyeballing the chart, there is not obvious evidence for regional heterogeneity in the *associations* with Leave for *degree-educated* and so it might be regarded as a &#39;global&#39; variable -- a clear positive association with Leave irrespective of region. There may be regional specificity in the associations with Leave for the other explanatory variables. --&gt;
&lt;p&gt;To illustrate this, Figure &lt;a href=&#34;#fig:scatters-fe&#34;&gt;2&lt;/a&gt; is an updated set of scatterplots now annotated with ‘virtual’ regression lines for single variable linear regression models fit separately for each explanatory variable with a region FE. Plotting the virtual regression lines – regression lines offset by the FE constant – is interesting as it tells us a little about the nature of association in our explanatory variables. Given the way we have specified our model, the FE constants serve effectively as intercepts, &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; is no longer meaningful and so other measures of fit such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Akaike_information_criterion&#34;&gt;AIC&lt;/a&gt; should be used to evaluate relative fit.&lt;/p&gt;
&lt;p&gt;For the &lt;em&gt;degree-educated&lt;/em&gt; and &lt;em&gt;professional&lt;/em&gt; variables especially, regional &lt;em&gt;dependence&lt;/em&gt; in Leave voting needs to be adjusted for – once taken into account, there is a very strong association between &lt;em&gt;degree-education&lt;/em&gt; and &lt;em&gt;Leave&lt;/em&gt;, most obvious in the lines for Scotland (red). Although we are not yet modelling for this explicitly, eyeballing the chart, there is not obvious evidence for regional non-stationarity in the &lt;em&gt;associations&lt;/em&gt; with Leave for &lt;em&gt;degree-educated&lt;/em&gt; and so it might be regarded as a ‘global’ variable – a clear positive association with Leave irrespective of region. There may be regional specificity in the associations with Leave for the other explanatory variables.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:scatters-fe&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/07-class_files/scatters-fe.png&#34; alt=&#34;Scatterplots of constituency Leave vote against candidate explanatory variables, annotated with FE &#39;virtual&#39; regression lines, coefficients and model fit (AIC -- the lower the value, better the model). Scotland is highlighted red.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Scatterplots of constituency Leave vote against candidate explanatory variables, annotated with FE ‘virtual’ regression lines, coefficients and model fit (AIC – the lower the value, better the model). Scotland is highlighted red.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Presented in Figure &lt;a href=&#34;#fig:plot-outputs-fe&#34;&gt;3&lt;/a&gt; are updated regression coefficients for the model fit with a FE on region. In the left panel are the FE constants. Together these capture the variance in Leave vote between regions after accounting for demographic composition. These coefficients have interesting properties: they are the estimated size of the Leave vote for a constituency in a region &lt;em&gt;net&lt;/em&gt; of demographic composition. London is in interesting here. When initially analysing variation in the vote, constituencies in Scotland and London were distinctive in voting in much smaller proportions than the rest of the country for Leave. Given the associations we observe with Leave voting and demographic composition, however, if we were to randomly sample two constituencies that contain the same demographic characteristics, one in London and one in another region (say North West), on average we’d expect Leave for the constituency in London to be higher (~60%) than that sampled from North West (~51%). A separate, and more anticipated pattern is that Scotland would have a lower Leave vote (~38%) – that is, net of demographics there is some additional context in Scotland that means Leave is lower than in other regions.&lt;/p&gt;
&lt;p&gt;In the right panel are the regression coefficients net of this between-region variation. Compared to the model specified without FEs, the coefficients are estimated with less uncertainty (tighter standard errors). In the previous session, the &lt;em&gt;white&lt;/em&gt; variable was shown counterintuitively to have a slight negative association with Leave (although there was high uncertainty here). Now the &lt;em&gt;white&lt;/em&gt; variable has a direction of effect that conforms to expectation – net of variation in other demographics increased proportions of &lt;em&gt;white&lt;/em&gt; residents is associated with increased Leave voting. For the other variable with a counterintuitive effect – &lt;em&gt;EU born&lt;/em&gt; – the coefficient still suggests a positive association with Leave.&lt;/p&gt;
&lt;!-- Standard practice for FE dummy variables is to hold one as a &#34;reference&#34; variable for comparison.   --&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:plot-outputs-fe&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/07-class_files/plot-outputs-fe.png&#34; alt=&#34;Output from multiple regression model of Leave vote by demographic composition of constituency with FE on region.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Output from multiple regression model of Leave vote by demographic composition of constituency with FE on region.
&lt;/p&gt;
&lt;/div&gt;
&lt;!--
* Each group receives a separate intercept for its (virtual) regression line, while the relationships between all other predictors are the same (i.e., parallel regression lines)
* Including the region FE, we are allowing for separate regression lines, where each has its own intercept.
* By including the region FE, we are accounting for any group-level sources of variation in the outcome you’re trying to explain - sources of variation that are not captured in your regression otherwise.
* FEs allow you to estimate the “base level” of your outcome of interest for each group -- e.g. net of demographics the Leave vote in ?? is ?? much higher/lower than ??.
* we are estimating the relationship between having adjusted. for systematic differences in outcome and explanatory variables across regions --&gt;
&lt;/div&gt;
&lt;div id=&#34;geographic-context-as-grouped-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Geographic context as grouped effects&lt;/h3&gt;
&lt;p&gt;The benefit of the FE adjustment is that it provides coefficient estimates that are not affected by between-region variation. The derived FE constants themselves also allow group differences to be quantified net of differences in demographics. However, they simply identify the fact that this variation exists – they do not permit non-stationarity in &lt;em&gt;process&lt;/em&gt;. It is conceivable that the strength and direction of association between Leave and the candidate demographic variables may vary between regions. For example, that increased levels of &lt;em&gt;EU-born&lt;/em&gt; (non-UK) residents might affect area-level voting differently in certain regions than others.&lt;/p&gt;
&lt;p&gt;Rather than simply allowing a constant term to vary, we can update the linear regression model with an &lt;a href=&#34;https://en.wikipedia.org/wiki/Interaction_(statistics)&#34;&gt;interaction term&lt;/a&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\textcolor{highlight}{\beta_{1j}}{x_{i1}}\)&lt;/span&gt;) that allows the coefficient estimates to vary depending on region. This means we get a separate constant term and coefficient estimate of the effect of each variable on Leave for every region – it provides a framework for exploring regionally-distinct effects.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
       y_{i}&amp;amp;= \textcolor{highlight}{\gamma_{j}}  + \textcolor{highlight}{\beta_{1j}}x_{i1} + \varepsilon_{i}  \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textcolor{highlight}{\gamma_{j}}\)&lt;/span&gt;, a constant term similar to an intercept for region &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textcolor{highlight}{\beta_{1j}}x_{i1}\)&lt;/span&gt;, the region-specific slope, indicating in which direction and to what extent some demographic variable at constituency &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and in region &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is associated with Leave, &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i}\)&lt;/span&gt;, the difference between &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; (the observed value) at constituency &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and the &lt;em&gt;unobservable&lt;/em&gt; true ‘population’ value of the Leave vote in that constituency (statistical error)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:scatters-interaction&#34;&gt;4&lt;/a&gt; is an updated set of scatterplots again annotated with ‘virtual’ regression lines, now with slopes that vary. Again the structure of these slopes is interesting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;parallel slopes with large variation in vertical position suggest that there is variation in the outcome (Leave) between regions, but a &lt;em&gt;consistent association&lt;/em&gt; between variables exists;&lt;/li&gt;
&lt;li&gt;substantial changes in slope (cluttered display) suggests that the pattern of &lt;em&gt;association switches between regions&lt;/em&gt;. Most obvious here is the &lt;em&gt;EU-born&lt;/em&gt; and &lt;em&gt;no-car&lt;/em&gt; variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:scatters-interaction&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/07-class_files/scatters-interaction.png&#34; alt=&#34;Scatterplots of constituency Leave vote against candidate explanatory variables, annotated with FE regression lines for interaction on region. Scotland is highlighted red.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Scatterplots of constituency Leave vote against candidate explanatory variables, annotated with FE regression lines for interaction on region. Scotland is highlighted red.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:plot-outputs-interaction&#34;&gt;5&lt;/a&gt; are region-specific coefficients derived from a multivariate model with an interaction term introduced on region. In each region, &lt;em&gt;degree-educated&lt;/em&gt; has a negative coefficient and with reasonably tight uncertainty estimates, or at least CIs that do not cross 0. The other variables are subject to more uncertainty. The &lt;em&gt;no-car&lt;/em&gt; variable is also negatively associated with Leave, a variable we thought may separate metropolitan versus peripheral contexts, but the strength of negative association, after controlling for variation in other demographic factors, does vary by region. The &lt;em&gt;heavy industry&lt;/em&gt; variable, previously identified as being strongly associated with Leave (e.g. &lt;a href=&#34;../06-class/#quantifying-and-exploring-co-variation&#34;&gt;previous session&lt;/a&gt;), has a clear positive association only for London and to a much lesser extent for North West and Wales (small coefficients). The EU born variable is again the least consistent as it flips between positive and negative association when analysed at the regional-level: after controlling for variation in other demographic characteristics it is positively associated with Leave for North West, Scotland, South West, but negatively associated with Leave for the North East (though with coefficients that are subject to much variation).&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:plot-outputs-interaction&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/07-class_files/plot-outputs-interaction-fe.png&#34; alt=&#34;Output from multiple regression model of Leave vote by demographic composition of constituency with FE and interaction on region.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Output from multiple regression model of Leave vote by demographic composition of constituency with FE and interaction on region.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;addressing-estimate-volatility&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Addressing estimate volatility&lt;/h3&gt;
&lt;p&gt;Given the reasonably large standard errors, it is difficult to make strong claims about regional nonstationarity in &lt;em&gt;process&lt;/em&gt; form the models presented above. This might be due to overfitting, caused by the fact that we introduce additional terms to the regression model (region as an interaction) without adding data – our coefficients may begin to fit noise rather than true effects.&lt;/p&gt;
&lt;p&gt;Given the fact that our data are hierarchically structured (constituencies sit within regions) hierarchical &lt;a href=&#34;https://en.wikipedia.org/wiki/Multilevel_model&#34;&gt;multi-level modelling&lt;/a&gt; may be more appropriate to modelling this sort of regional grouping. Multi-level modelling uses &lt;a href=&#34;https://solomonkurz.netlify.app/post/stein-s-paradox-and-what-partial-pooling-can-do-for-you/&#34;&gt;partial pooling&lt;/a&gt; to make estimated coefficients more conservative where there are comparatively few observations in particular groupings. There are numerous resources on multi-level modelling in R (see &lt;a href=&#34;https://bookdown.org/roback/bookdown-BeyondMLR/&#34;&gt;Roback &amp;amp; Legler 2021&lt;/a&gt; for an in-progress example). The reason for the FE approach here is a prosaic one: I wanted to use model-building functions in R that worked well with &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;&lt;code&gt;tidymodels&lt;/code&gt;&lt;/a&gt;, although annoyingly I’ve recently noticed that a &lt;code&gt;package&lt;/code&gt; for combining multi-level modelling with &lt;code&gt;tidymodels&lt;/code&gt; is &lt;a href=&#34;https://github.com/tidymodels/multilevelmod&#34;&gt;under development here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Alternatively, it may be useful to generate different model formulations for different regions – e.g. particular combinations of explanatory variable. &lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_(mathematics)&#34;&gt;Penalised regression&lt;/a&gt; provides a principled means of automatic variable selection and has been applied to generate regional and state area-level model specifications for the Brexit &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-beecham_locally_2018&#34; role=&#34;doc-biblioref&#34;&gt;Beecham, Slingsby, and Brunsdon 2018&lt;/a&gt;)&lt;/span&gt; and Trump &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-beecham_regionally_2020&#34; role=&#34;doc-biblioref&#34;&gt;Beecham, Williams, and Comber 2020&lt;/a&gt;)&lt;/span&gt; votes, again with more reliably coefficient estimates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;geographic-context-as-continuous-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Geographic context as continuous effects&lt;/h3&gt;
&lt;!-- Previously, we identified two ways in which geographic context might be represented: geography as categorical variable (region) and geography as continuous variable (location in *x,y*).  --&gt;
&lt;p&gt;Geographically-weighted (GW) statistics provides a mechanism for exploring the extent to which values and associations vary &lt;em&gt;continuously&lt;/em&gt; over space &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brunsdon_geographically_2002&#34; role=&#34;doc-biblioref&#34;&gt;Brunsdon, Fortheringham, and Charlton 2002&lt;/a&gt;)&lt;/span&gt;. This involves generating local statistics, and in the case of &lt;a href=&#34;https://journals.sagepub.com/doi/10.1068/a301905&#34;&gt;Geographically Weighted Regression&lt;/a&gt; (GWR) local regression coefficients, for each spatial unit. If applied to our dataset, separate regression coefficients for each constituency are estimated that take into account observed values for Leave and the demographic variables in nearby constituencies. GW-statistics enables spatial non-stationarity in process to be flexibly explored and characterised. As GWR involves generating many hundreds of parameter estimates, visual analysis are often used in its interpretation &lt;span class=&#34;citation&#34;&gt;(e.g. &lt;a href=&#34;#ref-dykes_geographically_2007&#34; role=&#34;doc-biblioref&#34;&gt;Dykes and Brunsdon 2007&lt;/a&gt;)&lt;/span&gt;. I don’t want to burden you with yet another modelling paradigm, but &lt;a href=&#34;https://www.roger-beecham.com/data-science-practicals/&#34;&gt;this repo&lt;/a&gt; contains some (rather dated) materials I previously prepared on GWR with the Brexit dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;techniques&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Techniques&lt;/h2&gt;
&lt;p&gt;The technical element to this session demonstrates how linear regression models can be updated with Fixed Effects and Interaction Terms. Approaches for extracting model summaries and diagnostics introduced in the previous session will be used again.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the &lt;a href=&#34;../homework/07-homework_files/07-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 07-template.Rmd&lt;/a&gt; file for this session and save it to the &lt;code&gt;reports&lt;/code&gt; folder of your &lt;code&gt;vis-for-gds&lt;/code&gt; project.&lt;/li&gt;
&lt;li&gt;Open your &lt;code&gt;vis-for-gds&lt;/code&gt; project in RStudio and load the template file by clicking &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Open File ...&lt;/code&gt; &amp;gt; &lt;code&gt;reports/07-template.Rmd&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;import&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Import&lt;/h3&gt;
&lt;p&gt;The template file lists the required packages: &lt;code&gt;tidyverse&lt;/code&gt;, &lt;code&gt;sf&lt;/code&gt;, &lt;code&gt;parlitools&lt;/code&gt; and also the &lt;code&gt;tidymodels&lt;/code&gt; package for extracting model outputs. You will also need to re-load the datasets that you created in the previous session:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cons_outline&lt;/code&gt; : a data frame loaded from a &lt;code&gt;.geojson&lt;/code&gt; file containing boundaries for drawing constituencies.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data_for_models&lt;/code&gt; : data frame containing the outcome variable and z-score transformed explanatory variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model FE&lt;/h3&gt;
&lt;p&gt;To include a FE on region, we convert &lt;code&gt;region&lt;/code&gt; to a factor variable (this effectively creates dummies) and include it as an additional term in &lt;code&gt;lm()&lt;/code&gt;. By default a “reference” region appears and the FE regression coefficients (those for region) describe the effect on the outcome of a constituency being in a given region relative to the “reference” region. So the reference region (intercept) in the model below is the East Midlands – the first in the factor to appear alphabetically. The signed coefficient estimates for regions identifies whether, after controlling for variation in demographics, the Leave vote for a particular region is expected to be higher or lower than this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- lm(leave ~ region +  degree  + eu_born + white  + no_car +
                      not_good_health + heavy_industry, data=data_for_models)

summary(model)

Call:
lm(formula = leave ~ region + degree + eu_born + white + no_car +
    not_good_health + heavy_industry, data = data_for_models)

Residuals:
      Min        1Q    Median        3Q       Max
-0.198730 -0.020953  0.001834  0.023086  0.115457

# Coefficients:
#                                  Estimate Std. Error t value Pr(&amp;gt;|t|)
# (Intercept)                     0.5896429  0.0330259  17.854  &amp;lt; 2e-16 ***
# regionEast of England           0.0036335  0.0078731   0.462  0.64459
# regionLondon                    0.0654256  0.0094823   6.900 1.30e-11 ***
# regionNorth East                0.0048168  0.0094500   0.510  0.61044
# regionNorth West               -0.0200108  0.0072752  -2.751  0.00612 **
# regionScotland                 -0.1452946  0.0084285 -17.238  &amp;lt; 2e-16 ***
# regionSouth East                0.0037722  0.0075166   0.502  0.61595
# regionSouth West               -0.0232999  0.0078890  -2.953  0.00326 **
# regionWales                    -0.0547155  0.0085991  -6.363 3.87e-10 ***
# regionWest Midlands             0.0236169  0.0074457   3.172  0.00159 **
# regionYorkshire and The Humber  0.0112335  0.0076235   1.474  0.14112
# degree                         -0.0092785  0.0004078 -22.753  &amp;lt; 2e-16 ***
# eu_born                         0.0060143  0.0011372   5.289 1.72e-07 ***
# white                           0.0017054  0.0001765   9.662  &amp;lt; 2e-16 ***
# no_car                         -0.0029004  0.0002519 -11.513  &amp;lt; 2e-16 ***
# not_good_health                 0.0030277  0.0009866   3.069  0.00224 **
# heavy_industry                  0.0030330  0.0006117   4.958 9.23e-07 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#
# Residual standard error: 0.03713 on 615 degrees of freedom
# Multiple R-squared:  0.8973,  Adjusted R-squared:  0.8946
# F-statistic: 335.9 on 16 and 615 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want our model to represent a dummy for every area and so we add &lt;code&gt;-1&lt;/code&gt; to the specification. Doing this removes the intercept (reference region), and this is why &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; is no longer meaningful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Include `-1` to include a dummy for every area.
model &amp;lt;- lm(leave ~ region +  degree  + eu_born + white  + no_car +
                      not_good_health + heavy_industry -1, data=data_for_models)

summary(model)

# Coefficients:
#                                  Estimate Std. Error t value Pr(&amp;gt;|t|)
# regionEast Midlands             0.5896429  0.0330259  17.854  &amp;lt; 2e-16 ***
# regionEast of England           0.5932764  0.0309206  19.187  &amp;lt; 2e-16 ***
# regionLondon                    0.6550685  0.0309627  21.157  &amp;lt; 2e-16 ***
# regionNorth East                0.5944597  0.0342229  17.370  &amp;lt; 2e-16 ***
# regionNorth West                0.5696321  0.0329834  17.270  &amp;lt; 2e-16 ***
# regionScotland                  0.4443483  0.0314911  14.110  &amp;lt; 2e-16 ***
# regionSouth East                0.5934151  0.0308656  19.226  &amp;lt; 2e-16 ***
# regionSouth West                0.5663430  0.0320231  17.685  &amp;lt; 2e-16 ***
# regionWales                     0.5349274  0.0340264  15.721  &amp;lt; 2e-16 ***
# regionWest Midlands             0.6132598  0.0326154  18.803  &amp;lt; 2e-16 ***
# regionYorkshire and The Humber  0.6008764  0.0325957  18.434  &amp;lt; 2e-16 ***
# degree                         -0.0092785  0.0004078 -22.753  &amp;lt; 2e-16 ***
# eu_born                         0.0060143  0.0011372   5.289 1.72e-07 ***
# white                           0.0017054  0.0001765   9.662  &amp;lt; 2e-16 ***
# no_car                         -0.0029004  0.0002519 -11.513  &amp;lt; 2e-16 ***
# not_good_health                 0.0030277  0.0009866   3.069  0.00224 **
# heavy_industry                  0.0030330  0.0006117   4.958 9.23e-07 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#
# Residual standard error: 0.03713 on 615 degrees of freedom
# Multiple R-squared:  0.9953,  Adjusted R-squared:  0.9951
# F-statistic:  7625 on 17 and 615 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, it is useful to extract details of fit, residuals and other diagnostics in a &lt;code&gt;tidymodel&lt;/code&gt; way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- data_for_models %&amp;gt;%
  mutate(type=&amp;quot;full_dataset&amp;quot;, region=as.factor(region)) %&amp;gt;%
  nest(data=-type) %&amp;gt;%
  mutate(
    # Include `-1` to eliminate the constant term and include a dummy for every area.
    model=map(data, ~lm(leave ~ region +  degree  + eu_born + white  + no_car +
                          not_good_health + heavy_industry -1, data=.x)),
    # glance() for each model fit
    fits = map(model, glance),
    # tidy() for coefficients
    coefs = map(model, tidy),
    # augment() for predictions/residuals
    values=map(model, augment)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-interaction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Interaction&lt;/h3&gt;
&lt;p&gt;To include an Interaction on region, we need to set a variable that will be used to represent these regional constants (&lt;code&gt;cons&lt;/code&gt;), and the Interaction is added with the notation &lt;code&gt;:&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- data_for_models %&amp;gt;%
  mutate(
    type=&amp;quot;full_dataset&amp;quot;,
    # Convert to factor variable.
    region=as.factor(region),
    # We need to explicitly set a constant term that we can allow to vary on region.
    cons=1
    ) %&amp;gt;%
  nest(data=-type) %&amp;gt;%
  mutate(
    # `:` Notation implies interaction variables
    model=map(data, ~lm(leave ~ 0 +  (cons + degree  + eu_born + white  + no_car +
                          not_good_health + heavy_industry):(region), data=.x)),
    # glance() for each model fit
    fits = map(model, glance),
    # tidy() for coefficients
    coefs = map(model, tidy),
    # augment() for predictions/residuals
    values=map(model, augment)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-tidy-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot tidy models&lt;/h3&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:plot-outputs-interaction&#34;&gt;5&lt;/a&gt; is an extension of the model outputs plot in the previous session.&lt;/p&gt;
&lt;p&gt;The code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model %&amp;gt;%
  unnest(cols = coefs) %&amp;gt;% # unnest output from glance
  select(-c(data, model, fits, model, values)) %&amp;gt;%
  separate(term, into= c(&amp;quot;term&amp;quot;, &amp;quot;region&amp;quot;), sep=&amp;quot;:&amp;quot;) %&amp;gt;%
  mutate(
    region=str_remove(region,&amp;quot;region&amp;quot;)
  ) %&amp;gt;%
  filter(term!=&amp;quot;cons&amp;quot;) %&amp;gt;%
  ggplot() +
  geom_col(aes(x=reorder(term, -estimate), y=estimate), alpha=.3)+
  geom_pointrange(aes(x=reorder(term, -estimate),
                      y=estimate,ymin=estimate-1.96*std.error, ymax=estimate+1.96*std.error)) +
  geom_hline(yintercept = 0, size=.2)+
  facet_wrap(~region) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot specification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: A data frame of model coefficients extracted from the multivariate model object using &lt;code&gt;tidy&lt;/code&gt;. To make clean plot labels, we need to remove unnecessary text in the &lt;code&gt;term&lt;/code&gt; variable (e.g. “cons:regionEast Midlands”). &lt;a href=&#34;https://tidyr.tidyverse.org/reference/separate.html&#34;&gt;&lt;code&gt;separate()&lt;/code&gt;&lt;/a&gt; allows us to split this column on &lt;code&gt;:&lt;/code&gt; and then &lt;code&gt;str_remove()&lt;/code&gt; is quite obvious. We do not wish to plot the FE constants and so &lt;code&gt;filter()&lt;/code&gt; them out.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: y-position varies according to the size of the coefficient estimate and the 95% confidence intervals, defined using &lt;code&gt;ymin&lt;/code&gt; and &lt;code&gt;ymax&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_pointrange()&lt;/code&gt;, which understands &lt;code&gt;ymin&lt;/code&gt; and &lt;code&gt;ymax&lt;/code&gt;, for the dots with Confidence Intervals. I have also included light bars in the background (&lt;code&gt;geom_col()&lt;/code&gt;) as I think this aids interpretation of the direction and size of the coefficients.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: &lt;code&gt;coord_flip()&lt;/code&gt; to make variable names easier to read.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Geographic grouping of residuals is a common feature of area-level analysis, but should not be ignored as it suggests that there is some systematic context that is not captured well by the model. There are two different classes of approach to addressing geographic context: those that treat geographic dependence in the values of variables as a nuisance term that is to be quantified/captured and controlled away; and those that explicitly try to model for geographic grouping in processes. Through our analysis of 2016 EU Referendum dataset this session introduced techniques for dealing with both, treating geography as a &lt;em&gt;categorical&lt;/em&gt; variable: introducing a Fixed Effect term to assess regional dependence and Interaction term to assess regional non-stationarity. It also reused some of the &lt;code&gt;dplyr&lt;/code&gt; and &lt;a href=&#34;https://purrr.tidyverse.org/&#34;&gt;functional programming&lt;/a&gt; code templates from the previous session that are instrumental for working over models, and as we will see in the next session, for modern computational data analysis.&lt;/p&gt;
&lt;!-- The last two sessions have used a regression framework to explore constituency-level variation in 2016 EU Referendum. The pattern of residuals in Figure \@ref(fig:plot-lineup) it is likely that there is some important context at play in determining the Leave vote in certain regions, most obviously Scotland, that we are not accounting for in our model.  --&gt;
&lt;!-- This session dependence in values and non-stationarity in process is common in area-level regression analysis. This session introduced approaches to quantifying and exploring this, treating geography as a *categorical* variable.


 Given the fact that our  constituency-level voting in the UK’s 2016 EU Referendum

introduced a linear regression modelling framework with the explicit aim of analysing whether variation in constituency-level voting in the UK’s 2016 EU Referendum varies systematically with the demographic composition of constituencies. Visual approaches were used to explore associations between constituency-level voting and demographics and also to characterise bias in the specified models – to identify potential (geographic and regional) groupings that our models ignore.

Given the pattern of residuals in Figure \@ref(fig:plot-lineup) it is likely that there is some important context at play in determining the Leave vote in certain regions, most obviously Scotland, that we are not accounting for in our model.

We can control for these unobserved factors by modelling Region as a fixed effect, using traditional dummy variables,  -- allowing a constant term to vary on Region. --&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-beecham_locally_2018&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R., A. Slingsby, and C. Brunsdon. 2018. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Locally-varying explanations behind the United Kingdom’s vote to leave the European Union &lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;Journal of Spatial Information Science&lt;/span&gt;&lt;/em&gt; 16: 117–36.
&lt;/div&gt;
&lt;div id=&#34;ref-beecham_regionally_2020&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R., N. Williams, and L. Comber. 2020. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Regionally-structured explanations behind area-level populism: An update to recent ecological analyses&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;PLOS One&lt;/span&gt;&lt;/em&gt; 15 (3): e0229974.
&lt;/div&gt;
&lt;div id=&#34;ref-brunsdon_geographically_2002&#34; class=&#34;csl-entry&#34;&gt;
Brunsdon, C., M. Fortheringham, and M. Charlton. 2002. &lt;span&gt;“Geographically Weighted Summary Statistics: A Framework for Localised Exploratory Data Analysis.”&lt;/span&gt; &lt;em&gt;Computers, Environment and Urban Systems&lt;/em&gt; 26: 501–24.
&lt;/div&gt;
&lt;div id=&#34;ref-dykes_geographically_2007&#34; class=&#34;csl-entry&#34;&gt;
Dykes, J., and C. Brunsdon. 2007. &lt;span&gt;“Geographically Weighted Visualization: Interactive Graphics for Scale-Varying Exploratory Analysis.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/em&gt; 13 (6): 1161–68.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualization for model building 1: Expose, estimate, evaluate</title>
      <link>/class/06-class/</link>
      <pubDate>Mon, 13 Jun 2022 00:00:00 +0000</pubDate>
      <guid>/class/06-class/</guid>
      <description>
&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;script src=&#34;../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;../rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;../rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;h2&gt;Contents&lt;/h2&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#session-outcomes&#34;&gt;Session outcomes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concepts&#34;&gt;Concepts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#quantifying-and-exploring-variation&#34;&gt;Quantifying and exploring variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantifying-and-exploring-co-variation&#34;&gt;Quantifying and exploring co-variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modelling-for-co-variation&#34;&gt;Modelling for co-variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extracting-and-representing-model-parameters&#34;&gt;Extracting and representing model parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-bias&#34;&gt;Exploring bias&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#techniques&#34;&gt;Techniques&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#import&#34;&gt;Import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transform&#34;&gt;Transform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model&#34;&gt;Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidy-models&#34;&gt;Tidy models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-tidy-models&#34;&gt;Plot tidy models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-residuals&#34;&gt;Plot residuals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;session-outcomes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session outcomes&lt;/h2&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;knowledge&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Be &lt;strong&gt;reminded&lt;/strong&gt; of the basics of linear regression modelling.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Appreciate&lt;/strong&gt; problems of model bias.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;practical skills&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Specify&lt;/strong&gt; linear regression models in R.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Extract&lt;/strong&gt; model outputs and diagnostics in a &lt;a href=&#34;https://vita.had.co.nz/papers/tidy-data.pdf&#34;&gt;&lt;code&gt;tidy&lt;/code&gt;&lt;/a&gt; manner.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Apply&lt;/strong&gt; &lt;a href=&#34;https://purrr.tidyverse.org/articles/other-langs.html&#34;&gt;functional-style programming&lt;/a&gt; for working over multiple model outputs.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;So far the analysis presented in this module has been very data-driven. We have demonstrated how, having &lt;a href=&#34;../class/02-class/&#34;&gt;described data&lt;/a&gt; in a consistent way, heuristics around &lt;a href=&#34;../class/03-class/&#34;&gt;visual approaches and techniques&lt;/a&gt; can be applied to usefully expose structure in datasets. The session on &lt;a href=&#34;../class/04-class/&#34;&gt;exploratory data analysis&lt;/a&gt; involved some model building, but these were largely value-free models derived from contingency tables, based on few prior assumptions.&lt;/p&gt;
&lt;p&gt;In the next two sessions we will work on a dataset with a more explicit, and theoretically-informed, motivation. We will explore &lt;em&gt;variation&lt;/em&gt; in voting behaviour in the UK’s 2016 referendum on leaving the EU. You might remember that whilst there was a slight majority for Leave (51.9%), the vote varied between different parts of the country. And there were many theories and explanations offered around the time for why particular places voted the way they did – related to the demographic composition of those areas. We will explore whether the sorts of compositional demographic factors discussed – &lt;a href=&#34;https://www.theguardian.com/politics/2016/jun/25/left-behind-eu-referendum-vote-ukip-revolt-brexit&#34;&gt;left behind places&lt;/a&gt; – vary systematically with area-level Leave voting. We will use a regression framework to model the relative effect of each of these compositional factors in structuring variation in the vote and visualization techniques to support estimation of model parameters and evaluation of model bias. Different from the previous sessions, the &lt;a href=&#34;../class/06-class/#concepts-1/&#34;&gt;Concepts&lt;/a&gt; element is structured more as a narrated data analysis and the &lt;a href=&#34;../class/06-class/#techniques-1/&#34;&gt;Techniques&lt;/a&gt; element more of a how-to for working with regression models in R.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This session assumes some basic familiarity with linear regression modelling. There are entire courses on the topic, so it’s not possible to cover the fundamentals in the next two sessions alone. For a clear and detailed overview, with excellent social science examples, I would recommend &lt;span class=&#34;citation&#34;&gt;Bartholomew et al. (&lt;a href=&#34;#ref-bartholomew_analysis_2008&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concepts&lt;/h2&gt;
&lt;div id=&#34;quantifying-and-exploring-variation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quantifying and exploring variation&lt;/h3&gt;
&lt;p&gt;Variation is central to most data analysis, and certainly regression modelling: quantifying variation, exploring how it is structured and accounting for (or explaining) it using a combination of empirical data and prior theory/knowledge.&lt;/p&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:map-uniform&#34;&gt;1&lt;/a&gt; is a map and bar chart displaying total variation in vote shares for Leave in Great Britain (GB), estimated at Parliamentary Constituency level &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-hanretty_areal_2017&#34; role=&#34;doc-biblioref&#34;&gt;Hanretty 2017&lt;/a&gt;)&lt;/span&gt;. The values themselves are the difference in estimated vote shares from an expectation that the Leave vote for a constituency (&lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;), our &lt;em&gt;outcome&lt;/em&gt; of interest, is the same as the national (GB) average of 51.9% (&lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt;). Although a slightly contrived formulation, we could express this as an intercept-only linear regression model, where &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;, the estimated &lt;strong&gt;slope&lt;/strong&gt; is ‘turned off’ (takes the value &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;), and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;, the &lt;strong&gt;intercept&lt;/strong&gt;, is &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt;, the GB average vote share for Leave:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
       y_{i}= \beta_{0} + \beta_{1} + \varepsilon_{i}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we estimate the Leave vote in each constituency (&lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;) as a function of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;, the intercept, the GB average vote share (&lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}=0\)&lt;/span&gt;, a negated slope, &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i}\)&lt;/span&gt;, a statistical error term capturing the difference between &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; (the observed value) and the &lt;em&gt;unobservable&lt;/em&gt; &lt;em&gt;true&lt;/em&gt; ‘population’ value of the Leave vote in each constituency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How does this relate to the idea of characterising variation? The length and colour of each bar in Figure &lt;a href=&#34;#fig:map-uniform&#34;&gt;1&lt;/a&gt; is scaled according to model &lt;strong&gt;residuals&lt;/strong&gt;: estimates of the statistical errors (&lt;span class=&#34;math inline&#34;&gt;\(\hat{\varepsilon}_{i}\)&lt;/span&gt;), the difference between &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; (the observed value) and the &lt;em&gt;unobservable&lt;/em&gt; expected value of the Leave vote in our dataset (&lt;span class=&#34;math inline&#34;&gt;\(\hat{y}_{i}\)&lt;/span&gt;). The sum of these bar lengths (residuals) is therefore the total &lt;strong&gt;variance&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\((\frac{\sum({y_{i...n}-\bar{y}}^2}{n-1})\)&lt;/span&gt; – variance that we later try to reduce, or explain, by updating our regression model to generate new expected values using information on the demographic composition of constituencies.&lt;/p&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:map-uniform&#34;&gt;1&lt;/a&gt; is similar to the maps that were &lt;a href=&#34;https://www.theguardian.com/politics/2016/jun/24/eu-voting-map-lays-bare-depth-of-division-across-britain&#34;&gt;published widely in press reports&lt;/a&gt; in the aftermath of the vote, and demonstrates that there is indeed substantial variation in Leave voting between different parts of the country. The uniform model consistently underestimates Leave in Scotland and most of London. Outside of this, constituencies voting in smaller proportions than would be expected for Leave are distributed more in pockets around the country: the dark red dot with surrounding red area in the east of the country is Cambridge and Cambridgeshire, constituencies in Bristol (south west), Manchester and Liverpool (north west), Brighton (south), are also reasonably strong red.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:map-uniform&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/06-class_files/map-uniform.png&#34; alt=&#34;Residuals from uniform model comparing constituency Leave vote to GB average.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Residuals from uniform model comparing constituency Leave vote to GB average.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;When &lt;strong&gt;evaluating&lt;/strong&gt; the effectiveness of modelled values, there are various checks that can be performed. An obvious one here is wether there is bias in the residuals – whether they have any underlying structure that suggests that they are grouped in a way not captured by the model. Given the motivation behind our analysis, it is no surprise that there is a geographic pattern to the residuals in Figure &lt;a href=&#34;#fig:map-uniform&#34;&gt;1&lt;/a&gt;, but also a histogram of the residuals (Figure &lt;a href=&#34;#fig:hist-uniform&#34;&gt;2&lt;/a&gt;) shows a slight left skew. There are more constituencies with positive values than negative – the Leave vote is underestimated by the uniform model for 57% of constituencies – and certain constituencies with extreme negative values – the strongest vote for Leave was Boston and Skegness (76%) but the strongest for Remain was Hackney North and Stoke Newington (80%).&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:hist-uniform&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/06-class_files/hist-uniform.png&#34; alt=&#34;Histogram of residuals from uniform model comparing constituency Leave vote to GB average.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Histogram of residuals from uniform model comparing constituency Leave vote to GB average.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;quantifying-and-exploring-co-variation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quantifying and exploring co-variation&lt;/h3&gt;
&lt;p&gt;More interesting is whether the pattern of variation in Figure &lt;a href=&#34;#fig:map-uniform&#34;&gt;1&lt;/a&gt; is &lt;em&gt;correlated&lt;/em&gt; with compositional factors, &lt;a href=&#34;https://www.theguardian.com/news/datablog/2016/jun/24/the-areas-and-demographics-where-the-brexit-vote-was-won&#34;&gt;identified in press and other reporting&lt;/a&gt; that we think explain this variation; and also whether bias or structure in residuals exists even after accounting for these compositional factors.&lt;/p&gt;
&lt;p&gt;In Table &lt;a href=&#34;#tab:variables&#34;&gt;1&lt;/a&gt; is a list of candidate explanatory variables describing the demographic composition of constituencies, selected based on the narrative around ‘left-beind’ places. Each variable is expressed as a proportion of the constituency’s population. So the &lt;em&gt;degree educated&lt;/em&gt; variable describes the proportion of residents in the constituency educated at least to degree-level. Comparison across these variables is challenging due to the fact that their ranges differ: the &lt;em&gt;EU-born&lt;/em&gt; variable ranges from 0.6% to 17%; the &lt;em&gt;white&lt;/em&gt; variable from 14% to 98%. There are also obvious ceilings that limit how successful explanatory variables are likely to be at discriminating variation. Common practice for addressing the range problem is to &lt;a href=&#34;https://en.wikipedia.org/wiki/Standard_score&#34;&gt;z-score transform&lt;/a&gt; the variables, so that each value is expressed in terms of standard deviation units from that variable’s mean, as in Figure &lt;a href=&#34;#fig:hist-expl&#34;&gt;3&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:variables&#34;&gt;Table 1: &lt;/span&gt;Selected 2011 Census variables.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Census variable
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;4&#34;&gt;
&lt;td colspan=&#34;1&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;post-industrial / knowlegde economy&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
degree-educated
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
professional occupations
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
younger adults
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
heavy industry
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;4&#34;&gt;
&lt;td colspan=&#34;1&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;diversity/values/outcomes&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
not good health
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
white British/Irish
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
Christian
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
EU-born (not UK)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;1&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;metropolitan / ‘big city’&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
own home
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
don’t own car
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:hist-expl&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/06-class_files/hist-expl.png&#34; alt=&#34;Histograms of candidate explanatory variables measuring demographic composition of constituencies.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Histograms of candidate explanatory variables measuring demographic composition of constituencies.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To explore whether these demographics vary systematically with Leave voting in each constituency, Figure &lt;a href=&#34;#fig:scatters&#34;&gt;4&lt;/a&gt; presents scatterplots from which the extent of linear association can be inferred. Each dot is a constituency, arranged on the x-axis according to value of each candidate explanatory variable and the y-axis according to the share of Leave vote. The scatterplots are faceted by explanatory variable and ordered left-to-right and top-to-bottom according to correlation coefficient. The variable most heavily correlated with Leave voting is that measuring levels of &lt;em&gt;degree&lt;/em&gt; education: as the share of a constituency’s population educated at least to &lt;em&gt;degree-level&lt;/em&gt; increases, the share of Leave vote in that constituency decreases. An association in the same direction, but to a lesser extent, is observed for variables representing similar concepts: &lt;em&gt;professional occupations&lt;/em&gt;, &lt;em&gt;younger adults&lt;/em&gt;, &lt;em&gt;EU-born&lt;/em&gt;, &lt;em&gt;no-car&lt;/em&gt; and the reverse for &lt;em&gt;Christian&lt;/em&gt;, &lt;em&gt;not-good health&lt;/em&gt; and &lt;em&gt;heavy industry&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:scatters&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/06-class_files/scatters.png&#34; alt=&#34;Scatterplots of constituency Leave vote against candidate explanatory variables.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Scatterplots of constituency Leave vote against candidate explanatory variables.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;You will remember from introductory stats courses that that the correlation coefficient can be used to summarise the strength of linear association between two variables. It is a quantity that ranges from perfect negative correlation, -1 – as one value increases another decreases in the same proportion – to perfect positive correlation, +1 – as one value increases another increases in the same proportion. A value of 0 indicates no association – the values increase and decrease independently of each other.&lt;/p&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:correlations&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/06-class_files/correlations.png&#34; alt=&#34;Scatterplots of synthetic bivariate data with extent of correlation coefficient systematically varied.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Scatterplots of synthetic bivariate data with extent of correlation coefficient systematically varied.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;!-- * To look at this in multivariate space -- PCPs. --&gt;
&lt;/div&gt;
&lt;div id=&#34;modelling-for-co-variation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modelling for co-variation&lt;/h3&gt;
&lt;p&gt;Linear regression provides a framework for explicitly describing these linear associations. The ultimate objective is to quantify how much of the variation in an outcome variable, summarised in Figure &lt;a href=&#34;#fig:map-uniform&#34;&gt;1&lt;/a&gt;, can be explained using information on other variables, the candidate demographic variables in Figure &lt;a href=&#34;#fig:scatters&#34;&gt;4&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To express this in equation form, we can update the uniform model such that Leave vote is a &lt;em&gt;function&lt;/em&gt; of the candidate explanatory variables. For single-variable linear regression, we could update with the proportion of residents educated at least to &lt;em&gt;degree-level&lt;/em&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\textcolor{highlight}{d_{i1}}\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
       y_{i}&amp;amp;= \beta_{0} + \beta_{1}\textcolor{highlight}{d_{i1}} + \varepsilon_{i}  \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we now estimate the Leave vote in each constituency (&lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;) as a function of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;, the intercept, the GB average vote share (&lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}=\beta_{1}\textcolor{highlight}{d_{i1}}\)&lt;/span&gt;, the slope, indicating in which direction and to what extent &lt;em&gt;degree-educated&lt;/em&gt; is associated with Leave, &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i}\)&lt;/span&gt;, the difference between &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; (the observed value) and the &lt;em&gt;unobservable&lt;/em&gt; true ‘population’ value of the Leave vote in that constituency (statistical error)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are different algorithms that can be used to estimate these parameters. Most obvious is ordinary least squares (OLS), which aims to minimise &lt;span class=&#34;math inline&#34;&gt;\(\sum{\hat{\varepsilon}_{i...n}}\)&lt;/span&gt;, the sum of the (squared) residuals between the observed Leave vote in a constituency, &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;, and that expected, &lt;span class=&#34;math inline&#34;&gt;\(\hat{y_{i}}\)&lt;/span&gt;, given the association with the &lt;em&gt;degree-educated&lt;/em&gt; explanatory variable &lt;span class=&#34;math inline&#34;&gt;\(d_{i1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To explore the associations further, we could update the scatterplots with regression lines, fit via OLS, modelling the Leave vote separately as a linear function of each explanatory variable. As well as the regression line, observations are now coloured according to the size and direction of their residuals. The plots are also annotated according to the coefficient of determination (&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;) – the proportion of the total constituency-level variation in the Leave vote explained by each model formulation. This tells us how much better than the uniform model, which captures the total variation (Figure &lt;a href=&#34;#fig:map-uniform&#34;&gt;1&lt;/a&gt;), is each of the single variable explanatory models.&lt;/p&gt;
&lt;p&gt;Colouring dots representing constituencies by residuals is instructive. In most of the scatterplots there seems to be a grouping of large negative residuals (dark red) where, after taking into account the association between Leave voting and demographics across all constituencies in GB, the Leave vote is consistently &lt;em&gt;underrepresented&lt;/em&gt;. You might be able to guess at where these are located, certainly generating maps of these residuals may expose whether they are grouped in a particular way.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:scatters-resids&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/06-class_files/scatters-resids.png&#34; alt=&#34;Scatterplots of constituency Leave vote against candidate explanatory variables, annotated with regression lines.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Scatterplots of constituency Leave vote against candidate explanatory variables, annotated with regression lines.
&lt;/p&gt;
&lt;/div&gt;
&lt;!-- * Linear regression is termed parametric as the outputs are parameters that summarise relationships: --&gt;
&lt;/div&gt;
&lt;div id=&#34;extracting-and-representing-model-parameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extracting and representing model parameters&lt;/h3&gt;
&lt;p&gt;It is of course possible, and likely, that some of these variables account for different elements of the variation in the Leave vote than others. You will be aware that the linear regression model can be extended to include many explanatory variables:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
      y_{i}&amp;amp;= \beta_{0} +\beta_{1}x_{i1} + ... + \beta_{k}x_{ik} + \varepsilon_{i}  \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So this results in &lt;em&gt;separate&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\beta_{k}\)&lt;/span&gt; coefficients for separate explanatory variables. These coefficients can be interpreted as the degree of association between the explanatory variable &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and the outcome variable, keeping all the other explanatory variables constant – or the distinct correlation between an explanatory variable &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and the outcome variable, net of the other correlated variables.&lt;/p&gt;
&lt;p&gt;&lt;!-- which allows us to isolate the distinct effect that a single variable has on the outcome. --&gt;&lt;/p&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:plot-outputs&#34;&gt;7&lt;/a&gt; are regression coefficients (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{k}\)&lt;/span&gt;) from a multiple regression model with &lt;em&gt;degree-educated&lt;/em&gt;, &lt;em&gt;no car&lt;/em&gt;, &lt;em&gt;white&lt;/em&gt;, &lt;em&gt;heavy industry&lt;/em&gt;, &lt;em&gt;EU-born&lt;/em&gt; and &lt;em&gt;not good health&lt;/em&gt; selected as explanatory variables. Coefficients are reported as dots with estimates of uncertainty represented as lines, displaying 95% &lt;a href=&#34;http://www.sumsar.net/blog/2013/12/an-animation-of-the-construction-of-a-confidence-interval/&#34;&gt;confidence intervals&lt;/a&gt;.
&lt;!-- the range of values the true (*unobservable*) coefficient is likely to take --&gt;&lt;/p&gt;
&lt;p&gt;Most variables’ coefficients are in the direction that would be expected given the associations in Figure &lt;a href=&#34;#fig:scatters-resids&#34;&gt;6&lt;/a&gt;. Net of variation in the other compositional factors, increased levels of &lt;em&gt;degree education&lt;/em&gt; in a constituency have the effect of reducing the Leave vote. The two exceptions are &lt;em&gt;EU-born&lt;/em&gt; and &lt;em&gt;white&lt;/em&gt;: after controlling for variation in the other demographic variables, increased proportions of residents identifying as &lt;em&gt;white&lt;/em&gt; has the effect of reducing the Leave vote and increased proportions of residents that are &lt;em&gt;EU-born&lt;/em&gt; has the effect of increasing the Leave vote. Since the confidence interval for &lt;em&gt;white&lt;/em&gt; crosses zero, this coefficient is subject to much uncertainty. In the next session we will discuss some approaches to exploring whether these sorts of counter-intuitive effects are genuine or as a result of a poorly-specified model.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:plot-outputs&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/06-class_files/plot-outputs.png&#34; alt=&#34;Output from multiple regression model of Leave vote by demographic composition of constituency.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: Output from multiple regression model of Leave vote by demographic composition of constituency.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;Again, you might remember that issues of multicollinearity, amongst other things, mean that variable selection in multiple regression requires a little thought. Ideally you want model specifications that are easy to interpret (without variable redundancy) and that explain variation in the outcome reasonably well but reliably. As explanatory variables are added, model fit can increase slightly or substantially, but will never decrease. A common scenario is that a model specified with many explanatory variables fits the data well, but contains coefficients that are inflated in magnitude and that likely vary between different model realisations. These problems are typically addressed in Social Science applications by judicious variable selection and computing coefficient &lt;a href=&#34;https://en.wikipedia.org/wiki/Variance_inflation_factor&#34;&gt;Variance Inflation Factors&lt;/a&gt; (VIF). The variables here were selected using this &lt;em&gt;de facto&lt;/em&gt; approach, but see &lt;span class=&#34;citation&#34;&gt;Beecham, Williams, and Comber (&lt;a href=&#34;#ref-beecham_regionally_2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; for an application of a common machine learning approach to automatic variable selection applied to (almost) the same dataset, and that could be used to explore the sorts of counter-intuitive directions in the coefficients for &lt;em&gt;EU-born&lt;/em&gt; and &lt;em&gt;white&lt;/em&gt; above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimating uncertainty&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given the spirit of this module, you might have wondered about the reasonably abbreviated discussion of techniques for representing model outputs and their uncertainty estimates (via Confidence Intervals). I am deliberately reserving more involved coverage of this for &lt;a href=&#34;&#34;&gt;session 08&lt;/a&gt;.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-bias&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploring bias&lt;/h3&gt;
&lt;p&gt;The multivariate model explains a reasonably large share (c.80%) of the variation in constituency-level Leave voting. However, our analysis becomes more interesting when we start to explore and characterise model &lt;em&gt;bias&lt;/em&gt;: any underlying structure to the observations which are better or less-well accounted for by the model. Especially for area-level regression models, it is usual for residuals to contain &lt;a href=&#34;https://rspatial.org/raster/analysis/3-spauto.html&#34;&gt;spatial autocorrelation&lt;/a&gt;. For certain parts of a country, a model will overestimate an outcome given the relationship implied by associations between explanatory and outcome variables; for other parts, the outcome will be underestimated. This might occur due to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Spatial dependence&lt;/em&gt; in &lt;strong&gt;variable values&lt;/strong&gt; over space. We know that the geography of GB is quite socially distinctive, so it is reasonable to expect, for example, the range in variables like &lt;em&gt;heavy industry&lt;/em&gt; and &lt;em&gt;white&lt;/em&gt; to be bounded to economic regions and metropolitan-peripheral regional contexts.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Spatial nonstationarity&lt;/em&gt; in &lt;strong&gt;processes&lt;/strong&gt; over space. It is possible that associations between variables might be grouped over space – that the associations vary for different parts of the country. For example, high levels of &lt;em&gt;EU-born&lt;/em&gt; migration might affect political attitudes, and thus area-level voting, differently in different parts of a country.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can test for and characterise &lt;a href=&#34;https://rspatial.org/raster/analysis/3-spauto.html&#34;&gt;spatial autocorrelation&lt;/a&gt; by performing a graphical inference test. a Map LineUp &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-beecham_maplineups_2017&#34; role=&#34;doc-biblioref&#34;&gt;Beecham et al. 2017&lt;/a&gt;; &lt;a href=&#34;#ref-wickham_inference_2010&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al. 2010&lt;/a&gt;)&lt;/span&gt;, against a null hypothesis of &lt;em&gt;complete spatial randomness&lt;/em&gt; in residuals. Graphical LineUp tests are visual equivalents of test statistics. A plot of real data is hidden amongst a set of decoys generated under a null hypothesis. If the real can be correctly identified from the decoys, then this lends statistical credibility to the claim that the observed data are not consistent with the specified null &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wickham_inference_2010&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al. 2010&lt;/a&gt;)&lt;/span&gt;. Graphical LineUp tests have been used in various domains, also to test regression assumptions &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-loy_model_2017&#34; role=&#34;doc-biblioref&#34;&gt;Loy, Hofmann, and Cook 2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Map LineUp in Figure &lt;a href=&#34;#fig:plot-lineup&#34;&gt;8&lt;/a&gt;, constructed by randomly permuting observed residuals around constituencies, demonstrates that there &lt;em&gt;is&lt;/em&gt; obvious spatial (and &lt;em&gt;regional&lt;/em&gt;) autocorrelation in residuals. In the next session, we will cover approaches to dealing with this – accounting for &lt;em&gt;spatial dependence&lt;/em&gt; in &lt;strong&gt;values&lt;/strong&gt; and exploring &lt;em&gt;spatial nonstationarity&lt;/em&gt; in &lt;strong&gt;processes&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:plot-lineup&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/06-class_files/lineups.png&#34; alt=&#34;Map LineUp of residuals in which the ‘real’ dataset is presented alongside 8 decoy plots generated by randomly permuting the observed residuals around constituencies.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: Map LineUp of residuals in which the ‘real’ dataset is presented alongside 8 decoy plots generated by randomly permuting the observed residuals around constituencies.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;In this session, I have focussed mostly on one assumption of linear regression: that of &lt;em&gt;independence of errors (residuals)&lt;/em&gt;. Consult &lt;span class=&#34;citation&#34;&gt;Bartholomew et al. (&lt;a href=&#34;#ref-bartholomew_analysis_2008&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt; for a thorough discussion of these matters, but others not mentioned (and of varying importance) include: &lt;em&gt;linearity&lt;/em&gt; in the relationship between outcome and explanatory variables; &lt;em&gt;equal variance&lt;/em&gt; and &lt;em&gt;normality&lt;/em&gt; in the residuals.&lt;/p&gt;&lt;/p&gt;
&lt;!-- *Normality* in residuals.
* I: The errors are independent—there’s no connection between how far any two points lie from the regression line,
* N: The responses are normally distributed at each level of X, and
* E: The variance or, equivalently, the standard deviation of the responses is equal for all levels of X. --&gt;
&lt;p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;!-- https://maczokni.github.io/crimemapping_textbook_bookdown/regression-analysis-a-refresher.html#homework-5 --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;techniques&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Techniques&lt;/h2&gt;
&lt;p&gt;The technical element to this session demonstrates how linear regression models can be specified in R, as well as approaches for extracting model summaries and diagnostics; and of course representing them visually. Data recording estimated vote shares for Leave by Parliamentary Constituency, as well as constituency-level 2011 Census demographics, are available via the &lt;code&gt;parlitools&lt;/code&gt; package used in &lt;a href=&#34;../class/03-class/&#34;&gt;session 3&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the &lt;a href=&#34;../homework/06-homework_files/06-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 06-template.Rmd&lt;/a&gt; file for this session and save it to the &lt;code&gt;reports&lt;/code&gt; folder of your &lt;code&gt;vis-for-gds&lt;/code&gt; project.&lt;/li&gt;
&lt;li&gt;Open your &lt;code&gt;vis-for-gds&lt;/code&gt; project in RStudio and load the template file by clicking &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Open File ...&lt;/code&gt; &amp;gt; &lt;code&gt;reports/06-template.Rmd&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;import&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Import&lt;/h3&gt;
&lt;p&gt;The template file lists the required packages: &lt;code&gt;tidyverse&lt;/code&gt;, &lt;code&gt;sf&lt;/code&gt;, &lt;code&gt;parlitools&lt;/code&gt; and also the &lt;code&gt;tidymodels&lt;/code&gt; package for extracting model outputs. The required datasets are loaded automatically when &lt;code&gt;library(parlitools)&lt;/code&gt; is called. There is code for loading a simplified shapefile representing constituencies and also for extracting the relevant 2011 Census demographics that form explanatory variables in our model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transform&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Transform&lt;/h3&gt;
&lt;p&gt;In the &lt;a href=&#34;../class/06-class/#quantifying-and-exploring-co-variation-1&#34;&gt;concepts section&lt;/a&gt;, I mentioned that explanatory variables are &lt;a href=&#34;https://en.wikipedia.org/wiki/Standard_score&#34;&gt;z-score transformed&lt;/a&gt;. Here, the distance between observed values for each 2011 Census variable is expressed in standard deviation units from the mean across constituencies for that variable. In the code below, also in the template, &lt;a href=&#34;https://dplyr.tidyverse.org/reference/across.html&#34;&gt;&lt;code&gt;across()&lt;/code&gt;&lt;/a&gt; is used to apply this formula to each explanatory variable.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;across()&lt;/code&gt; is a really useful &lt;code&gt;dplyr&lt;/code&gt; function. The first argument is the set of columns you would like the same function to be applied to and the second is the function you would like to apply. Remembering that &lt;code&gt;mutate()&lt;/code&gt; works over columns of a data frame, and that a single column of a dataframe is a vector of values, the notation &lt;code&gt;.x&lt;/code&gt; is used to access each element of the vector of values of the columns being worked across.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;explanatory_z_scores &amp;lt;- explanatory %&amp;gt;%
  mutate(
    across(
      c(younger:heavy_industry), ~(.x-mean(.x))/sd(.x)
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model&lt;/h3&gt;
&lt;p&gt;Linear models can be fit with the &lt;code&gt;lm()&lt;/code&gt; function and coefficients extracted with &lt;code&gt;summary()&lt;/code&gt;.&lt;/p&gt;
&lt;!-- To specify a linear regression model with a single predictor: --&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- lm(leave ~ degree, data=data_for_models)

summary(model)

# Call:
# lm(formula = leave ~ degree, data = data_for_models)
#
# Residuals:
#      Min       1Q   Median       3Q      Max
# -0.25521 -0.02548  0.01957  0.05143  0.11237
#
# Coefficients:
#               Estimate Std. Error t value Pr(&amp;gt;|t|)
# (Intercept)  0.8044108  0.0097570   82.44   &amp;lt;2e-16 ***
# degree      -0.0106109  0.0003483  -30.46   &amp;lt;2e-16 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#
# Residual standard error: 0.07279 on 630 degrees of freedom
# Multiple R-squared:  0.5956,  Adjusted R-squared:  0.595
# F-statistic: 927.9 on 1 and 630 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tidy-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tidy models&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;&lt;code&gt;tidymodels&lt;/code&gt;&lt;/a&gt;, and specifically the &lt;a href=&#34;https://broom.tidymodels.org/&#34;&gt;&lt;code&gt;broom&lt;/code&gt;&lt;/a&gt; package, provides a useful set of functions for extracting model outputs in a format that adheres to tidy data &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wickham_tidy_2014&#34; role=&#34;doc-biblioref&#34;&gt;Wickham 2014&lt;/a&gt;)&lt;/span&gt; – e.g. as a data frame.&lt;/p&gt;
&lt;p&gt;Some examples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# tidy() return estimated coefficients as a data frame
tidy(model)
# # A tibble: 2 x 5
#   term        estimate std.error statistic   p.value
#   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
# 1 (Intercept)   0.804   0.00976       82.4 0.
# 2 degree       -0.0106  0.000348     -30.5 5.67e-126


# glance() returns a single row containing summaries of model fit.
glance(model)
# # A tibble: 1 x 12
#   r.squared adj.r.squared  sigma statistic   p.value    df logLik    AIC    BIC
#       &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
# 1     0.596         0.595 0.0728      928. 5.67e-126     1   760. -1514. -1501.
# # … with 3 more variables: deviance &amp;lt;dbl&amp;gt;, df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;

# augment() returns a data frame of residuals and predictions (fitted values) for the model realisation.
augment(model)
# # A tibble: 632 x 8
#    leave degree .fitted   .resid    .hat .sigma     .cooksd .std.resid
#    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
#  1 0.601   16.6   0.628 -0.0269  0.00393 0.0728 0.000270       -0.370
#  2 0.522   27.1   0.517  0.00546 0.00159 0.0729 0.00000447      0.0750
#  3 0.431   29.0   0.497 -0.0662  0.00169 0.0728 0.000703       -0.910
# ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The advantage of generating model diagnostics and outputs that are tidy, is that this eases the process of working with many model realisations. This is a common requirement for modern data analysis, where statistical inferences are made empirically from resampling.&lt;/p&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:scatters-resids&#34;&gt;6&lt;/a&gt;, scatterplots are coloured according to model residuals and annotated with model diagnostics from single-variable linear regression models generated separately for each candidate explanatory variable. These models can be generated with reasonably little code, by making use of &lt;code&gt;broom&lt;/code&gt; and a style of &lt;a href=&#34;https://adv-r.hadley.nz/fp.html#:~:text=Functional&#34;&gt;functional programming&lt;/a&gt; in R, which is supported by the &lt;a href=&#34;https://purrr.tidyverse.org/&#34;&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Example code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;single_model_fits &amp;lt;- data_for_models %&amp;gt;%
  pivot_longer(cols=younger:heavy_industry, names_to=&amp;quot;expl_var&amp;quot;, values_to=&amp;quot;z_score&amp;quot;) %&amp;gt;%
  nest(data=-expl_var) %&amp;gt;%  # Nest to generate list-column by expl_var.
  mutate(
    # Use map() to iterate over the list of datasets.
    model = map(data, ~lm(leave ~ z_score, data = .x)),
    # glance() for each model fit.
    fits = map(model, glance),
    # tidy() for coefficients.
    coefs = map(model, tidy),
    # augment() for predictions/residuals.
    values=map(model, augment),
  )

  single_model_fits %&amp;gt;%
    unnest(cols = fits) %&amp;gt;% # unnest output from glance.
    select(-c(data, model)) # remove other list-columns.

# # A tibble: 10 x 15
#    expl_var     r.squared adj.r.squared  sigma statistic   p.value    df logLik    AIC
#    &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
#  1 younger          0.289         0.288 0.0965      257. 1.05e- 48     1   582. -1158.
#  2 own_home         0.185         0.184 0.103       143. 7.42e- 30     1   539. -1071.
#  3 no_car           0.157         0.155 0.105       117. 3.81e- 25     1   528. -1050.
#  4 white            0.169         0.168 0.104       128. 3.79e- 27     1   532. -1059.
#  5 eu_born          0.233         0.232 0.100       191. 3.42e- 38     1   558. -1110.
#  6 christian        0.238         0.236 0.100       196. 4.95e- 39     1   560. -1114.
#  7 professional     0.320         0.319 0.0944      296. 1.08e- 54     1   596. -1186.
#  8 degree           0.596         0.595 0.0728      928. 5.67e-126     1   760. -1514.
#  9 not_good_he…     0.316         0.315 0.0947      291. 5.93e- 54     1   594. -1182.
# 10 heavy_indus…     0.504         0.503 0.0806      640. 5.43e- 98     1   696. -1385.
# # … with 6 more variables: BIC &amp;lt;dbl&amp;gt;, deviance &amp;lt;dbl&amp;gt;, df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;,
# #   coefs &amp;lt;list&amp;gt;, values &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Code description (there is a little to get your head around here):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Setup&lt;/strong&gt;: In order to generate separate models for separate explanatory variables, we need to generate &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyr/vignettes/nest.html&#34;&gt;nested data frames&lt;/a&gt;. These data frames are stored in a special type of column (a &lt;code&gt;list-column&lt;/code&gt;) in which the values of the column is a list of data frames – in this case one for each explanatory variable that we would like to generate a model over. You can think of parameterising &lt;code&gt;nest()&lt;/code&gt; in a similar way to &lt;code&gt;group_by&lt;/code&gt;. We first &lt;code&gt;pivot_longer()&lt;/code&gt; to generate a data frame where each observation contains the recorded Leave vote for a constituency and its corresponding &lt;code&gt;z_score&lt;/code&gt; value for each explanatory variable. There are 10 explanatory variables and so &lt;code&gt;nest()&lt;/code&gt; returns a data frame with the dimensions &lt;code&gt;10x2&lt;/code&gt; – a variable identifying the explanatory variable on which the model is to be built (&lt;code&gt;expl_var&lt;/code&gt;) and a &lt;code&gt;list-column&lt;/code&gt;, each element containing a data frame with the dimensions &lt;code&gt;632x7&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build model&lt;/strong&gt;: In &lt;code&gt;mutate()&lt;/code&gt;, &lt;code&gt;purrr&lt;/code&gt;’s &lt;a href=&#34;https://purrr.tidyverse.org/reference/map.html&#34;&gt;&lt;code&gt;map()&lt;/code&gt;&lt;/a&gt; function is used to iterate over the list of datasets and fit a model to each nested dataset. The new column &lt;code&gt;model&lt;/code&gt; is a &lt;code&gt;list-column&lt;/code&gt; this time containing a list of model objects.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generate outputs&lt;/strong&gt;: Next, the different model outputs can be generated using &lt;code&gt;glance(), tidy(), augment()&lt;/code&gt;, with &lt;code&gt;map()&lt;/code&gt; to iterate over the list of model objects. The new columns are now &lt;code&gt;list-columns&lt;/code&gt; of data frames containing model outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extract outputs&lt;/strong&gt;: Finally, we will want to variously extract the values from these nested data. This can be achieved using &lt;code&gt;unnest()&lt;/code&gt;and supplying to the &lt;code&gt;cols&lt;/code&gt; argument the names of the &lt;code&gt;list-columns&lt;/code&gt; that we want to flatten over. &lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-tidy-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot tidy models&lt;/h3&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:scatters-resids&#34;&gt;6&lt;/a&gt; estimated regression coefficients are plotted for a multivariate linear regression model. The &lt;code&gt;ggplot&lt;/code&gt; specification is reasonably straightforward.&lt;/p&gt;
&lt;p&gt;The code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- lm(leave ~ degree  + eu_born + white  + no_car + christian +
       not_good_health + heavy_industry, data=data_for_models)
outputs &amp;lt;- tidy(model)
outputs %&amp;gt;%
  filter(term != &amp;quot;(Intercept)&amp;quot;) %&amp;gt;%
  ggplot(
    aes(x=reorder(term, -estimate),
        y=estimate,ymin=estimate-1.96*std.error, ymax=estimate+1.96*std.error)) +
  geom_pointrange() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot specification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: A data frame of model coefficients extracted from the multivariate model object using &lt;code&gt;tidy()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: y-position varies according to the size of the coefficient estimate and the 95% confidence intervals, defined using &lt;code&gt;ymin&lt;/code&gt; and &lt;code&gt;ymax&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_pointrange()&lt;/code&gt;, which understands &lt;code&gt;ymin&lt;/code&gt; and &lt;code&gt;ymax&lt;/code&gt;, for the dots with Confidence Intervals&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: &lt;code&gt;coord_flip()&lt;/code&gt; to make variable names easier to read.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-residuals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot residuals&lt;/h3&gt;
&lt;p&gt;In order to explore spatial autocorrelation in residuals, we used a Map LineUp test &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-beecham_maplineups_2017&#34; role=&#34;doc-biblioref&#34;&gt;Beecham et al. 2017&lt;/a&gt;; &lt;a href=&#34;#ref-wickham_inference_2010&#34; role=&#34;doc-biblioref&#34;&gt;Wickham et al. 2010&lt;/a&gt;)&lt;/span&gt;. Using the sorts of functional programming techniques mentioned above, these tests are actually reasonably straightforward to construct.&lt;/p&gt;
&lt;p&gt;First generate a model object and extract residuals from it, again making use of &lt;code&gt;nest()&lt;/code&gt;, &lt;code&gt;map()&lt;/code&gt; and &lt;code&gt;tidy()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Generate model object and extract residuals
model_values &amp;lt;- data_for_models %&amp;gt;%
  mutate(type=&amp;quot;full_dataset&amp;quot;,) %&amp;gt;%
  nest(data=-type) %&amp;gt;%
  mutate(
    model=map(data, ~lm(leave ~ degree  + eu_born + white  + no_car + christian +
                          not_good_health + heavy_industry, data=.x)),
    values=map(model, augment),
    resids=map(values, . %&amp;gt;% select(.resid))
  ) %&amp;gt;%
  unnest(cols = c(data, resids)) %&amp;gt;%
  select(-model)
# Store max value of residuals for setting limits in map colour scheme.
max_resid &amp;lt;- max(abs(model_values$.resid))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, create a function that generates in this case nine permutations of the same dataset. I am deliberately not discussing this in too much detail, but you might notice that each permutation is stored in separate columns and is generated using &lt;a href=&#34;http://www.rexamples.com/14/Sample()&#34;&gt;&lt;code&gt;sample()&lt;/code&gt;&lt;/a&gt; with one permutation randomly held back – this represents the &lt;em&gt;real&lt;/em&gt; dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function for generating random permutations + one real.
do_lineup &amp;lt;- function(data, col_offset) {
  real &amp;lt;- sample(1:9,1)
  for(i in 1:9) {
    if(i==real) {
      data &amp;lt;- cbind(data, data$value)
      colnames(data)[i+col_offset] &amp;lt;- paste0(&amp;quot;permutation&amp;quot;, i)
    }
    else {
      permutation &amp;lt;- sample(data$value,nrow(data))
      data &amp;lt;- cbind(data, permutation)
      colnames(data)[i+col_offset] &amp;lt;- paste0(&amp;quot;permutation&amp;quot;, i)
    }
  }
  return(data %&amp;gt;% select(-value) %&amp;gt;% mutate(real=paste0(&amp;quot;permutation&amp;quot;, real)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The permutations are simply constituency names that are randomly shuffled (permuted). To add residuals data to these, we again make use of &lt;code&gt;nest()&lt;/code&gt; to generate a &lt;code&gt;list-column&lt;/code&gt; of randomly permuted constituencies, and then for each permutation add a new column bringing in the residuals values from the &lt;code&gt;model_values&lt;/code&gt; data frame that is &lt;em&gt;not&lt;/em&gt; randomly permuted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create the lineup data: swaps for constituencies
lineup_permutations &amp;lt;- do_lineup(model_values %&amp;gt;% select(value=ons_const_id) %&amp;gt;% unique, 1) %&amp;gt;%
  pivot_longer(cols=(permutation1:permutation9),names_to=&amp;quot;perm&amp;quot;, values_to=&amp;quot;area_name&amp;quot;) %&amp;gt;%
  arrange(perm)

lineup_data &amp;lt;- lineup_permutations %&amp;gt;%
  nest(data=-perm) %&amp;gt;%
  mutate(
    resids=map(data, ~model_values %&amp;gt;% select(.resid))
  ) %&amp;gt;%
  unnest(c(data, resids))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, plot choropleths in the usual way, faceting according to the permutation variable &lt;code&gt;perm&lt;/code&gt; in &lt;code&gt;lineup_data&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cons_outline %&amp;gt;%
  inner_join(lineup_data, by=c(&amp;quot;pcon19cd&amp;quot;=&amp;quot;area_name&amp;quot;)) %&amp;gt;%
  ggplot() +
  geom_sf(aes(fill=.resid), colour=&amp;quot;#757575&amp;quot;, size=0.001)+
  coord_sf(crs=27700, datum=NA) +
  facet_wrap(~perm, ncol=5) +
  scale_fill_distiller(palette=&amp;quot;RdBu&amp;quot;, direction=1,
                       limits=c(-max_resid, max_resid))&lt;/code&gt;&lt;/pre&gt;
&lt;!--  * Models as functions
* Vocabulary: outcome variable (whose variation you are trying to understand), explanatory (other variables that you want to use to explain variation), predicted value (output o the model function -- expected outcome conditioing on explanatory variables), residuals (a measure of how far each case is from the predicted value)

Why models and not graphics
* Can reveal patterns that not evident in graphics -- in regression framework we can *condition* on explanatory variables and talk about the effect of single variables net of others.
* Deal with uncertainty and false discovery -- we infer/overinterpet from scatterplots

* https://wilkelab.org/SDS375/
* https://mjskay.github.io/ggdist/articles/freq-uncertainty-vis.html
Use tidymodels : https://cfss.uchicago.edu/notes/start-with-models/  --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Variation is central to most data analysis, and certainly regression modelling: quantifying variation, exploring how it is structured and accounting for (or explaining) it using a combination of data and prior theory/knowledge. This session introduced a linear regression modelling framework with the explicit aim of analysing whether variation in constituency-level voting in the UK’s 2016 EU Referendum varies systematically with the demographic composition of constituencies. Visual approaches were used to explore associations between constituency-level voting and demographics and also to characterise bias in the specified models – to identify potential (geographic and regional) groupings that our models ignore.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-bartholomew_analysis_2008&#34; class=&#34;csl-entry&#34;&gt;
Bartholomew, David J., Fiona Steele, J Galbraith, and Irini Moustaki. 2008. &lt;em&gt;Analysis of Multivariate Social Science Data&lt;/em&gt;. London, &lt;span&gt;UK&lt;/span&gt;: CRC Press.
&lt;/div&gt;
&lt;div id=&#34;ref-beecham_maplineups_2017&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R., J. Dykes, W. Meulemans, A. Slingsby, C. Turkay, and J. Wood. 2017. &lt;span&gt;“Map Line-Ups: Effects of Spatial Structure on Graphical Inference.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;IEEE&lt;/span&gt; Transactions on Visualization &amp;amp; Computer Graphics&lt;/em&gt; 23 (1): 391–400.
&lt;/div&gt;
&lt;div id=&#34;ref-beecham_regionally_2020&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R., N. Williams, and L. Comber. 2020. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Regionally-structured explanations behind area-level populism: An update to recent ecological analyses&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;PLOS One&lt;/span&gt;&lt;/em&gt; 15 (3): e0229974.
&lt;/div&gt;
&lt;div id=&#34;ref-hanretty_areal_2017&#34; class=&#34;csl-entry&#34;&gt;
Hanretty, C. 2017. &lt;span&gt;“Areal Interpolation and the UK’s Referendum on EU Membership.”&lt;/span&gt; &lt;em&gt;Journal of Elections, Public Opinion and Parties&lt;/em&gt; 37 (4): 466–83.
&lt;/div&gt;
&lt;div id=&#34;ref-loy_model_2017&#34; class=&#34;csl-entry&#34;&gt;
Loy, A., H. Hofmann, and D. Cook. 2017. &lt;span&gt;“Model Choice and Diagnostics for Linear Mixed-Effects Models Using Statistics on Street Corners.”&lt;/span&gt; &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 26 (3). Taylor &amp;amp; Francis: 478–92. doi:&lt;a href=&#34;https://doi.org/10.1080/10618600.2017.1330207&#34;&gt;10.1080/10618600.2017.1330207&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-wickham_tidy_2014&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. 2014. &lt;span&gt;“Tidy Data.”&lt;/span&gt; &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 59 (10): 1–23.
&lt;/div&gt;
&lt;div id=&#34;ref-wickham_inference_2010&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., D. Cook, H. Hofmann, and A. Buja. 2010. &lt;span&gt;“Graphical Inference for Infovis.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Visualization and Computer Graphics (Proc. InfoVis ’10)&lt;/em&gt; 16 (6): 973–79.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualization for exploring spatial networks: Containment and connection</title>
      <link>/class/05-class/</link>
      <pubDate>Mon, 06 Jun 2022 00:00:00 +0000</pubDate>
      <guid>/class/05-class/</guid>
      <description>
&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;script src=&#34;../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;../rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;../rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;h2&gt;Contents&lt;/h2&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concepts&#34;&gt;Concepts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#network-data-nodes-and-edges&#34;&gt;Network data: nodes and edges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#node-link-representations&#34;&gt;Node-link representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-representations&#34;&gt;Matrix representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#od-maps&#34;&gt;OD maps&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#techniques&#34;&gt;Techniques&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#import&#34;&gt;Import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analyse-over-nodes&#34;&gt;Analyse over nodes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analyse-over-edges&#34;&gt;Analyse over edges&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;knowledge&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; The special structure and vocabulary used to represent network data.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; The strengths, weaknesses and trade-offs of network visualizations and of incorporating geographic context.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;practical skills&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write &lt;code&gt;ggplot2&lt;/code&gt; specifications that use containment to analyse spatial network origin-destination data.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Networks are a special class of data used to represent things (entities) and how they relate to one another. Network data consist of two types of element: &lt;strong&gt;nodes&lt;/strong&gt;, the entities themselves, and &lt;strong&gt;edges&lt;/strong&gt;, the connections between nodes. Both nodes and edges can have additional information attached to them – counts, categories and directions. Network data are cumbersome to work with in R as they are not represented well by flat data frames. This is being improved by packages like &lt;a href=&#34;https://github.com/thomasp85/tidygraph&#34;&gt;tidygraph&lt;/a&gt;, but a common workflow is to split the data across two tables – one representing nodes and one representing edges &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wickham_ggplot2_2020&#34; role=&#34;doc-biblioref&#34;&gt;Wickham, Navarro, and Lin Pedersen 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A category of network data used heavily in geospatial analysis is origin-destination (OD) data describing, for example, flows of bikes &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-beecham_exploring_2014&#34; role=&#34;doc-biblioref&#34;&gt;Beecham and Wood 2014&lt;/a&gt;)&lt;/span&gt; and commuters &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-beecham_characterising_2019&#34; role=&#34;doc-biblioref&#34;&gt;Beecham and Slingsby 2019&lt;/a&gt;)&lt;/span&gt; around a city, or &lt;a href=&#34;https://public.tableau.com/profile/robradburn#!/vizhome/LiquidCapital/VirtualWater&#34;&gt;water&lt;/a&gt; around a country. These data consist of &lt;strong&gt;nodes&lt;/strong&gt;, origin and destination locations, and &lt;strong&gt;edges&lt;/strong&gt;, flows linking those origins and destinations. Whilst statistics common to &lt;a href=&#34;https://en.wikipedia.org/wiki/Network_science&#34;&gt;Network Science&lt;/a&gt; can and have been deployed in the analysis of geospatial OD data, visualization techniques provide much assistance in exposing the types of complex structural patterns and relations that result from locating OD flow data within geographic context.&lt;/p&gt;
&lt;p&gt;In this session we will work with an accessible and widely used origin-destination network dataset: 2011 Census travel-to-work data. This records counts of individuals commuting between locations (&lt;a href=&#34;https://www.ons.gov.uk/methodology/geography/ukgeographies/censusgeography#census-geography&#34;&gt;census geographies&lt;/a&gt;) of the UK by travel mode, occupation, sex and age.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    &lt;/p&gt;
&lt;p&gt;Read this &lt;a href=&#34;https://medium.com/@roger.beecham.231/upgrading-spatial-analysis-of-origin-destination-data-using-modern-vis-frameworks-part-1-of-2-ee1c6481a595&#34;&gt;short blog post&lt;/a&gt; describing some of the challenges of visually analysing OD data, and which introduces some of the techniques used in this session.&lt;/p&gt;
&lt;p&gt;Next read this &lt;a href=&#34;https://threadreaderapp.com/thread/1138746147269468161.html?refreshed=yes#&#34;&gt;twitter thread&lt;/a&gt; demonstrating how these techniques can be deployed in a data analysis.
&lt;a href=&#34;https://threadreaderapp.com/thread/1138746147269468161.html?refreshed=yes#&#34;&gt;&lt;img src=&#34;../class/05-class_files/thread.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;!-- Network data present numerous analysis challenges. They are structured in a way that is sometimes cumbersome to work with (at least in R); and
* Geographical analysis of network data remains upport insights into the types of structure, patterns and relations necessary for spatial analysis is : e.g. judgements around the magnitude and distribution of large numbers of in-flows and out-flows, embedded within their geographic context. In such analysis scenarios visual analysis approaches become necessary.
* In this session we&#39;ll demonstrate some of the challenges associated with spatially-referenced network data and how visualization approaches can be used to support their analysis.
* We will do so through an analysis of origin-destination data describing travel-to-work in London in the 2011 Census



&lt;!-- [Network Science](https://en.wikipedia.org/wiki/Network_science) is an academic discipline devoted to the study of networks, and which draws on Mathematics (graph theory), Physics and Computer Science to contribute a special set of data structures for representing network data and statistical procedures for summarising and making inferences over networks. --&gt;&lt;/p&gt;
&lt;!-- https://ggplot2-book.org/networks.html --&gt;
&lt;!-- https://cran.r-project.org/web/packages/stplanr/vignettes/stplanr-od.html --&gt;
&lt;/div&gt;
&lt;div id=&#34;concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concepts&lt;/h2&gt;
&lt;div id=&#34;network-data-nodes-and-edges&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Network data: nodes and edges&lt;/h3&gt;
&lt;p&gt;This session introduces techniques for visually representing networks and evaluates their usefulness through an analysis of 2011 Census OD travel-to-work data. We will focus on travel-to-work data in London – the &lt;strong&gt;nodes&lt;/strong&gt; in this dataset are London’s 33 boroughs and the &lt;strong&gt;edges&lt;/strong&gt; are directed OD pairs between these boroughs.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:nodes-data&#34;&gt;Table 1: &lt;/span&gt;Nodes table: London Boroughs.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
borough
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
out
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
in
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barking and Dagenham
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
54237
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
33605
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barnet
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
117657
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
72024
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bexley
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
77263
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
39232
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:edges-data&#34;&gt;Table 2: &lt;/span&gt;Edges table: OD pairs between London Boroughs.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
origin
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
destination
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barking and Dagenham
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barking and Dagenham
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14650
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barking and Dagenham
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barnet
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
280
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barking and Dagenham
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bexley
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
155
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:flows-bor&#34;&gt;1&lt;/a&gt;, frequencies of flows into- and out-of London boroughs (the nodes) are represented. Note that job-rich boroughs in central London – Westminster, City of London, Camden, Tower Hamlets – contain many more &lt;em&gt;workers&lt;/em&gt; commuting-in for work than &lt;em&gt;residents&lt;/em&gt; commuting out for work.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:flows-bor&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/05-class_files/flows-bor.png&#34; alt=&#34;Barchart of commutes in- and out- of London boroughs.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Barchart of commutes in- and out- of London boroughs.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A map below (Figure &lt;a href=&#34;#fig:bor-geos&#34;&gt;2&lt;/a&gt;) to help read the abbreviated labels for London boroughs and the semi-spatial orderings used in this session.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:bor-geos&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/05-class_files/geogs.png&#34; alt=&#34;Relaxed geospatial layout of London boroughs.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Relaxed geospatial layout of London boroughs.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;node-link-representations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Node-link representations&lt;/h3&gt;
&lt;!-- Standard data graphics -- bar charts, scatterplots etc. -- are often not sufficient for expressing the types of complex relations implied by network data. --&gt;
&lt;p&gt;The most common examples of network visualization that we could use to represent the data in Tables &lt;a href=&#34;#tab:nodes-data&#34;&gt;1&lt;/a&gt; and &lt;a href=&#34;#tab:edges-data&#34;&gt;2&lt;/a&gt; are node-link diagrams. These depict graphs in two dimensions as a force-directed layout. Nodes are positioned such that those sharing greater connection – edges with greater weights (frequencies) – are closer than those that are less well-connected – that do not share edges with such large weights. Edges are drawn as lines &lt;em&gt;connecting&lt;/em&gt; nodes.&lt;/p&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:flows-bor&#34;&gt;1&lt;/a&gt; uses a force-directed layout to represent the travel-to-work data as such a network graph. This shows large numbers of flows between job rich boroughs (particularly Westminster) and boroughs containing large resident populations: Wandsworth, Lambeth, Southwark (close to central London), Barnet, Brent, Haringey (further from central London). Note that although no geographic positioning is used, the layout suggests geographic dependency in the relations between nodes, and so the manner by which populations commute between boroughs for work. For example, Hillingdon (HDN), Hounslow (HNS) and Ealing (ELG) in the bottom left; Redbridge (RDB) and Walthamstow (WTH) in the bottom middle/right.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:node-link-bor&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/05-class_files/node-link-bor.png&#34; alt=&#34;Node-link layout of commutes between London boroughs.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Node-link layout of commutes between London boroughs.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You will have no doubt seen many node-link representations before. It is very easy to produce complex and beautiful-looking graphics using off-the-shelf node-link layouts and they are often evoked as “Data Visualization” branding. As always, it is necessary to think about the tasks that these visualizations support and their usefulness in advancing a data analysis. You might consider this as you browse the collection of network visualizations at &lt;a href=&#34;http://www.visualcomplexity.com/vc/&#34;&gt;visualcomplexity.com&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;If the aim is to compare relative frequencies of flows and &lt;em&gt;locate&lt;/em&gt; them in geographic context – to understand both the distribution and geography of travel-to-work in London, then we can make better use of &lt;em&gt;position&lt;/em&gt;. In Figure &lt;a href=&#34;#fig:lines-geog&#34;&gt;4&lt;/a&gt;, nodes (boroughs) are placed in their exact geographic position (geometric centroid of boroughs), and line width and transparency is used to encode flow frequency. In the plot on the bottom row, flow &lt;em&gt;direction&lt;/em&gt; is encoded by making flow lines asymmetric &lt;span class=&#34;citation&#34;&gt;(following &lt;a href=&#34;#ref-wood_visualizing_2011&#34; role=&#34;doc-biblioref&#34;&gt;Wood, Slingsby, and Dykes 2011&lt;/a&gt;)&lt;/span&gt;: the straight ends are origins, the curved ends destinations.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:lines-geog&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/05-class_files/lines-geog.png&#34; alt=&#34;Straight line showing commutes between London boroughs in spatial position; asymmetric lines encode direction.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Straight line showing commutes between London boroughs in spatial position; asymmetric lines encode direction.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The geophraphic positioning of nodes adds context and the encoding of direction provides further detail around, for example, the pattern of commuting into central London boroughs versus more peripheral boroughs (asymmetric flows into Westminster, more symmetric pattern between outer London boroughs like Brent and Enfield). However, there are problems that affect the usefulness of the graphic:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flows that start and end at the same location are not shown;&lt;/li&gt;
&lt;li&gt;Longer flows are more visually dominant than are shorter flows;&lt;/li&gt;
&lt;li&gt;The graphic is cluttered with a ‘hairball’ effect due to multiple overlapping lines;&lt;/li&gt;
&lt;li&gt;Aggregating to the somewhat arbitrary geometric centre of boroughs and drawing lines between these locations implies an undue level of spatial precision.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-representations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix representations&lt;/h3&gt;
&lt;p&gt;An alternative way to represent these edge frequencies is origin-destination matrices, as in Figure &lt;a href=&#34;#fig:matrices&#34;&gt;5&lt;/a&gt;. The columns are destinations, London boroughs to which residents commute for work; the rows are origins, London boroughs from which residents commute out for work. Edge frequencies are encoded using colour value – the darker the colour, the larger the number of commutes between those boroughs. Boroughs are ordered left-to-right and top-to-bottom according to the total number of jobs accessed in each borough.&lt;/p&gt;
&lt;p&gt;Whilst using colour rather than line width to show flow magnitude is a less effective encoding channel &lt;span class=&#34;citation&#34;&gt;(following &lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;Munzner 2014&lt;/a&gt;)&lt;/span&gt;, there are obvious advantages. The salience bias of longer flows is removed. Ordering cells of the matrix by destination size (number of jobs accessed in each borough) helps to emphasise patterns in the job-rich boroughs, but also encourages &lt;em&gt;within&lt;/em&gt; and &lt;em&gt;between&lt;/em&gt; borough comparison. For example, the lack of colour outside of the diagonals in the less job-rich boroughs, which also tend to be in outer London, suggests that labour markets there might be more self-contained: that is, people are less likely to commute-in from outside of those boroughs for work. The layout also supports ‘look-up’ type tasks – for example in the top-right plot, where the same OD commutes are removed, we can efficiently identify the boroughs importing the second (WNS) and third (LAM) largest number of workers to Westminster. By applying a local scaling on destination (bottom plot), whereby a separate colour scale is created for each destination borough, we can explore the theme of &lt;em&gt;self-containment&lt;/em&gt; more directly. The vertical blue bars for Hammersmith and Kingston suggest that there are comparatively large numbers commuting into- these boroughs for work, especially given their size in terms of available jobs.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:matrices&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/05-class_files/matrices.png&#34; alt=&#34;OD matrices showing commutes between London boroughs.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: OD matrices showing commutes between London boroughs.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;od-maps&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;OD maps&lt;/h3&gt;
&lt;p&gt;The OD matrices expose new structure that could not be so easily inferred from the (more flashy) network visualizations. Although their layout is space-efficient, clearly for phenomena such as commuting, &lt;strong&gt;geographic context&lt;/strong&gt; is highly relevant. It is possible to make better use of layout and position to support the spatial dimension of analysis. &lt;a href=&#34;https://xeno.graphics/od-map/&#34;&gt;OD Maps&lt;/a&gt; were proposed by &lt;span class=&#34;citation&#34;&gt;Wood, Dykes, and Slingsby (&lt;a href=&#34;#ref-wood_visualisation_2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; as a means of supporting such an analysis. They take a little to get your head around, but the idea is very elegant.&lt;/p&gt;
&lt;p&gt;You will recall that we previously used the &lt;a href=&#34;https://github.com/aftertheflood/londonsquared&#34;&gt;LondondSquared&lt;/a&gt; grid layout, also in Figure &lt;a href=&#34;#fig:bor-geos&#34;&gt;2&lt;/a&gt;, for arranging London boroughs of regular size and shape. The cells of each column or row of the matrix could also be &lt;strong&gt;re-ordered&lt;/strong&gt; according to this geographic layout, as in Figure &lt;a href=&#34;#fig:reordered-matrix&#34;&gt;6&lt;/a&gt;. So, for example, we may be interested in focussing on &lt;strong&gt;destination&lt;/strong&gt;, or workplace, boroughs. In the first highlighted example, commutes into Westminster are considered (the left-most column). Cells in the highlighted column are coloured according to the number of workers resident in each borough that travel into Westminster for work. These cells are then &lt;strong&gt;re-ordered&lt;/strong&gt; spatially, as in the inset map for Westminster in the figure. The geographic ordering allows us to see that residents access jobs in Westminster in large numbers from many boroughs in London, but especially so from Wandsworth, Lambeth and Southwark immediately to the south of Westminster (see real geography in Figure &lt;a href=&#34;#fig:bor-geos&#34;&gt;2&lt;/a&gt;). In the second example, we focus on &lt;strong&gt;origins&lt;/strong&gt;: commutes out of Hackney are considered (the middle row). Cells in the highlighted row are coloured according to the number of jobs accessed in each borough by residents travelling out of Hackney for work. Cells are again reordered in the inset map. This demonstrates that patterns of commuting are reasonably localised. The modal destination/workplace borough remains Westminster, but relatively large numbers of jobs are accessed in Camden, Islington, Tower Hamlets and the City of London.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:reordered-matrix&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/05-class_files/reordered-matrix.png&#34; alt=&#34;Highlighted columns (destinations/workplaces) and rows (origins/homeplaces) of the OD matrix with a spatial arrangement.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Highlighted columns (destinations/workplaces) and rows (origins/homeplaces) of the OD matrix with a spatial arrangement.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;OD maps extend this idea by displaying &lt;strong&gt;all cells&lt;/strong&gt; of the OD matrix with a geographic arrangement. This is achieved via a “map-within-map” layout. In the destination-focussed example in Figure &lt;a href=&#34;#fig:od-map-layout&#34;&gt;7&lt;/a&gt;, each larger (reference) cell identifies destinations and the smaller cells are coloured according to origins – the number of residents in each borough commuting into the reference cell for work.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:od-map-layout&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/05-class_files/d-od-map.png&#34; alt=&#34;Destination-focussed OD map of commutes between London boroughs&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: Destination-focussed OD map of commutes between London boroughs
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The first map uses a &lt;strong&gt;global&lt;/strong&gt; colour scaling and the second a &lt;strong&gt;local&lt;/strong&gt; colour scaling. Ordering cells geographically helps better appreciate this distinction. As demonstrated in Figure &lt;a href=&#34;#fig:flows-bor&#34;&gt;1&lt;/a&gt;, Westminster contains a large portion of jobs filled by London residents. It makes sense then that, outside of the reference cells (residents living and working in the same borough), the darkest colours corresponding with the largest flow counts are in the map where Westminster is the reference cell. Due to this dominant pattern, however, it is difficult to read too much into the patterns of commuting outside of Westminster. This is where a local scaling is useful. Flow counts are summarised over each reference borough (destination in this case) and each is normalised according to the &lt;em&gt;maximum&lt;/em&gt; flow count for that reference borough. Most often this maximum flow count (darkest blue) is the reference cell – residents living and working in the same borough. The City of London, which contains many jobs but few residents, is the obvious exception.&lt;/p&gt;
&lt;p&gt;The local scaling allows us to characterise the geography of commuting into boroughs in reasonable detail. The two job-rich boroughs – Westminster and City of London – clearly draw workers in large proportions across London boroughs and to a lesser extent this is the case for other central/inner boroughs such as Islington (ISL), Camden (CMD), Kingston (KNS). For many outer London boroughs, commuting patterns are very localised – large numbers of available jobs are filled by workers living in those boroughs or immediately neighbouring boroughs. Notice that for inner London boroughs south of the river – Lambeth (LAM), Wandsworth (WNS), Southwark (SWR) – they tend to draw workers in greater number from neighbouring boroughs that are also south of the river. There is more to unpack here. In the &lt;a href=&#34;#techniques-1&#34;&gt;Technical&lt;/a&gt; element to the class and &lt;a href=&#34;../homework/05-homework/&#34;&gt;Homework&lt;/a&gt; you will consider how this geography of travel-to-work varies by travel mode.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The analysis above might give the impression that OD maps should be used in preference of spatially arranged node-link diagrams. As always, this depends on dataset and analysis task. In Figure &lt;a href=&#34;#fig:flow-bikes&#34;&gt;8&lt;/a&gt; is a map displaying bikeshare flow data for the London Cycle Hire Scheme (LCHS), collected via the &lt;a href=&#34;https://docs.ropensci.org/bikedata/&#34;&gt;bikedata&lt;/a&gt; package. The LCHS consists of c.700 docking stations in London – so c. 700^2 grid cells if the grid-within-grid layout of an OD map were to be used – challenging, although possible &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-wood_visualizing_2011&#34; role=&#34;doc-biblioref&#34;&gt;Wood, Slingsby, and Dykes 2011&lt;/a&gt;)&lt;/span&gt;. If a synoptic overview of spatial patterns is necessary, the more intuitive node-line representation is perhaps more successful than the OD map. Additionally, different from the commuter dataset, there &lt;em&gt;is&lt;/em&gt; geographic precision in the spatial coordinates representing origins and destinations in the bikeshare dataset, and distance judgements seem important when exploring cycling trips – an argument against the spatial distortion required by OD maps.&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;In the graphic below, I’ve filtered on trips that take place in the morning peak. The dominant pattern is of flows from London’s main commuter rail terminals – King’s Cross and Waterloo – to central London and City of London respectively. The asymmetric bezier curves efficiently communicate this and the reverse pattern when trips in the evening peak are filtered.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:flow-bikes&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/05-class_files/flow-vis-gds.png&#34; alt=&#34;Flow map of London Cycle Hire Scheme trips in the weakday morning peak. Data by TfL, accessed via [bikedata](https://docs.ropensci.org/bikedata/); parks and river outline via [OSM](https://www.openstreetmap.org/).&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: Flow map of London Cycle Hire Scheme trips in the weakday morning peak. Data by TfL, accessed via &lt;a href=&#34;https://docs.ropensci.org/bikedata/&#34;&gt;bikedata&lt;/a&gt;; parks and river outline via &lt;a href=&#34;https://www.openstreetmap.org/&#34;&gt;OSM&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;techniques&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Techniques&lt;/h2&gt;
&lt;p&gt;The technical element to this session continues in our analysis of 2011 Census travel-to-work data. After importing the dataset, you will organise the flow data into nodes and edges. You will then create graphics that summarise over the nodes (London boroughs in this case) and reveal spatial structure in the edges (OD flows between boroughs). You will focus on how the geography of travel-to-work varies by travel mode.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the &lt;a href=&#34;../homework/05-homework_files/05-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 05-template.Rmd&lt;/a&gt; file for this session and save it to the &lt;code&gt;reports&lt;/code&gt; folder of your &lt;code&gt;vis-for-gds&lt;/code&gt; project.&lt;/li&gt;
&lt;li&gt;Open your &lt;code&gt;vis-for-gds&lt;/code&gt; project in RStudio and load the template file by clicking &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Open File ...&lt;/code&gt; &amp;gt; &lt;code&gt;reports/05-template.Rmd&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;import&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Import&lt;/h3&gt;
&lt;p&gt;The template file lists the required packages: &lt;code&gt;tidyverse&lt;/code&gt;, &lt;code&gt;sf&lt;/code&gt; and also the &lt;a href=&#34;https://itsleeds.github.io/pct/&#34;&gt;&lt;code&gt;pct&lt;/code&gt;&lt;/a&gt; package for downloading the 2011 Census travel-to-work data.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;pct&lt;/code&gt;’s &lt;a href=&#34;https://itsleeds.github.io/pct/reference/get_od.html&#34;&gt;get_od&lt;/a&gt; function, a &lt;code&gt;.csv&lt;/code&gt; file of travel-to-work data between Middle-Layer &lt;a href=&#34;https://www.ons.gov.uk/methodology/geography/ukgeographies/censusgeography#super-output-area-soa&#34;&gt;Super Output Areas&lt;/a&gt; (MSOA) can be dowloaded. The default dataset is &lt;a href=&#34;https://www.statistics.digitalresources.jisc.ac.uk/dataset/wu03uk-2011-sms-merged-lala-location-usual-residence-and-place-work-method-travel-work-6&#34;&gt;WU03UK&lt;/a&gt;, &lt;em&gt;location of usual residence and place of work by method of travel to work&lt;/em&gt;. When downloading, you will limit to London using the &lt;code&gt;region&lt;/code&gt; argument in &lt;code&gt;get_od&lt;/code&gt;. This will take a few seconds to execute.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Import OD by travel mode from 2011 Census.
od_pairs &amp;lt;- get_od(
  region = &amp;quot;london&amp;quot;,
  type = &amp;quot;within&amp;quot;,
  omit_intrazonal = FALSE,
  base_url = paste0(&amp;quot;https://s3-eu-west-1.amazonaws.com/&amp;quot;,
                    &amp;quot;statistics.digitalresources.jisc.ac.uk&amp;quot;, &amp;quot;/dkan/files/FLOW/&amp;quot;),
  filename = &amp;quot;wu03ew_v2&amp;quot;
)

# Import .geojson file with geometry data for LondonSquared and real layout of London boroughs.
london_grid_real &amp;lt;- st_read(&amp;quot;https://www.roger-beecham.com/datasets/london_grid_real.geojson&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The downloaded data have the following structure:&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:od-raw&#34;&gt;Table 3: &lt;/span&gt;Raw OD pairs retrieved from &lt;code&gt;get_od()&lt;/code&gt;.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
geo_code1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
geo_code2
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
all
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
train
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
bus
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
…cont
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
la_1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
la_2
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
EO2000001
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
EO2000014
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
City of London
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
City of London
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
EO2000001
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
EO2000016
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
City of London
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barking and Dagenham
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
origin msoa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
dest msoa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
origin bor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
dest bor
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the template file is code for aggregating these MSOA data to borough level to derive tables representing nodes and edges.&lt;/p&gt;
&lt;p&gt;First, we ensure that only commutes made by London residents between London boroughs are recorded and so we generate a look-up of London boroughs. When installing and loading the &lt;code&gt;pct&lt;/code&gt; package, a table matching Local Authorities with Regions (&lt;code&gt;pct_regions_lookup&lt;/code&gt;) was made available. We filter this on &lt;code&gt;region_name&lt;/code&gt; to extract a list of all local authorities (&lt;code&gt;lad16nm&lt;/code&gt;), London boroughs in this case, and store them as a vector using &lt;code&gt;pull()&lt;/code&gt;. This vector of London borough names (&lt;code&gt;london_las&lt;/code&gt;) is used to &lt;code&gt;filter()&lt;/code&gt; the raw &lt;code&gt;od_pairs&lt;/code&gt; data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Look-up of London boroughs.
london_las &amp;lt;- pct_regions_lookup %&amp;gt;% filter(region_name==&amp;quot;london&amp;quot;) %&amp;gt;%
  pull(lad16nm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To generate an &lt;strong&gt;edges&lt;/strong&gt; table, &lt;code&gt;od_pairs&lt;/code&gt; is grouped by the origin and destination borough (&lt;code&gt;la_1&lt;/code&gt;, &lt;code&gt;la_2&lt;/code&gt;) and flow counts are summed for each unique borough-borough OD pair. Note that we use &lt;code&gt;dplyr&lt;/code&gt;’s &lt;a href=&#34;https://dplyr.tidyverse.org/reference/across.html&#34;&gt;&lt;code&gt;across()&lt;/code&gt;&lt;/a&gt; function to summarise over the multiple travel mode columns. Our analysis focuses on trips made by &lt;code&gt;public_transport&lt;/code&gt;, &lt;code&gt;car&lt;/code&gt;, and &lt;code&gt;active&lt;/code&gt; transport (foot+bike) and so we generate new combined mode counts merging travel modes and also dropping those we do not wish to use, using &lt;a href=&#34;https://dplyr.tidyverse.org/reference/mutate.html&#34;&gt;&lt;code&gt;transmute&lt;/code&gt;()&lt;/a&gt;. The edges table, an OD dataset summarising commute counts between boroughs, contains 1089 rows (33^2 borough-borough commutes).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  edges &amp;lt;- od_pairs %&amp;gt;%
    # Filter only *within* London.
    filter(la_1 %in% london_las, la_2 %in% london_las) %&amp;gt;%
    group_by(la_1, la_2) %&amp;gt;%
    summarise(
      across(c(all:other), sum)
    ) %&amp;gt;%
    ungroup %&amp;gt;%
    transmute(
      o_bor=la_1, d_bor=la_2, all=all, public_transport=train+bus+light_rail,
      car=car_driver+car_passenger, active=bicycle+foot
    )&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:edges-table&#34;&gt;Table 4: &lt;/span&gt;Edges table.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
o_bor
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
d_bor
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
all
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
public_transport
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
car
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
active
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barking and Dagenham
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barnet
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
280
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
143
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
131
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barking and Dagenham
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bexley
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
155
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
41
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
109
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
origin bor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
dest bor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Nodes&lt;/strong&gt; in the dataset are the 33 London boroughs. We can express the 2.9m commutes between these nodes in different ways – according to whether nodes are destinations or origins. In the code below, two tables are generated with OD data grouped by destination (&lt;code&gt;nodes_d&lt;/code&gt;) and origin (&lt;code&gt;nodes_o&lt;/code&gt;) and commutes into- and out of- boroughs counted respectively. These two data sets are then joined with &lt;code&gt;bind_rows()&lt;/code&gt; and distinguished via the variable name &lt;code&gt;type&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Summarise over destinations: commutes into- boroughs.
nodes_d &amp;lt;- od_pairs %&amp;gt;%
  # Filter only *within* London.
  filter(la_1 %in% london_las, la_2 %in% london_las) %&amp;gt;%
  group_by(la_2) %&amp;gt;%
  summarise(
    across(c(all:other), sum)
  ) %&amp;gt;%
  ungroup %&amp;gt;%
  rename(bor = la_2) %&amp;gt;%
  transmute(
    bor=bor, type=&amp;quot;destination&amp;quot;, all=all, public_transport=train+bus+light_rail,
    car=car_driver+car_passenger, active=bicycle+foot
  )

nodes_o &amp;lt;- od_pairs %&amp;gt;%
  # Filter only *within* London.
  filter(la_1 %in% london_las, la_2 %in% london_las) %&amp;gt;%
  group_by(la_1) %&amp;gt;%
  summarise(
    across(c(all:other), sum)
  ) %&amp;gt;%
  ungroup %&amp;gt;%
  rename(bor = la_1) %&amp;gt;%
  transmute(
    bor=bor, type=&amp;quot;origin&amp;quot;, all=all, public_transport=train+bus+light_rail,
    car=car_driver+car_passenger, active=bicycle+foot
  )

nodes  &amp;lt;- nodes_o %&amp;gt;% bind_rows(nodes_d)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:nodes-table&#34;&gt;Table 5: &lt;/span&gt;Nodes table.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
bor
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
all
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
public_transport
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
car
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
active
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barking and Dagenham
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dest
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
33605
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
9483
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
19050
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
4644
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Barnet
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dest
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
72024
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
24838
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
36484
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
9890
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
bor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
origin|dest
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;
count
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;analyse-over-nodes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Analyse over nodes&lt;/h3&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:mode-analysis&#34;&gt;9&lt;/a&gt; are a set of plots summarising over the &lt;code&gt;nodes&lt;/code&gt; dataset and using colour to support comparison by travel mode. The first plot is a bar chart ordered left-to-right on commute frequency (by destination borough). Nodes are expressed as destinations on the left panel and origins on the right. This demonstrates a pattern similar to that in Figure &lt;a href=&#34;#fig:flows-bor&#34;&gt;1&lt;/a&gt; where there are many commutes into- job rich boroughs in central London. Note that the amount of red, representing commutes by car, increases left-to-right as we move away from job-rich central London boroughs. This is best expressed by the proportional/filled bar chart in the second plot. Hillingdon (HDN), a borough which contains a relatively large number of jobs, looks distinctive from its other job-rich neighbours in this plot (Lambeth LAM, Hammersmith &amp;amp; Fulham HMS) as it contains a reasonably large block of red (commuting by car). This is, however, consistent with other boroughs that are similarly “outer” London (Havering HVG, Bexley BEX, Enfield ENF). The third plot incorporates this geographic context by ordering the bars again using the &lt;a href=&#34;https://github.com/aftertheflood/londonsquared&#34;&gt;LondonSquared&lt;/a&gt; layout. Destination and origin summary bars are juxtaposed next to each other and heights are scaled locally by borough. This enables visual comparison of whether there is a net in- or out- flow of workers. Job-rich central London boroughs contain longer “destination” bars importing large numbers of workers from other London boroughs, outer-London boroughs generally have longer “origin” bars exporting large numbers of residents to work in other London boroughs.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:mode-analysis&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/05-class_files/mode-plots.png&#34; alt=&#34;Destination-focussed OD map of commutes between London boroughs&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: Destination-focussed OD map of commutes between London boroughs
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The code – bar charts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Vector of borough names for ordering.
bor_orders &amp;lt;- nodes %&amp;gt;%
  filter(type==&amp;quot;destination&amp;quot;) %&amp;gt;%
  arrange(-all) %&amp;gt;% pull(bor)

# Plot bars
nodes %&amp;gt;%
  pivot_longer(cols=c(active, public_transport, car), names_to=&amp;quot;mode&amp;quot;, values_to=&amp;quot;count&amp;quot;) %&amp;gt;%
  mutate(bor=factor(bor, levels=bor_orders)) %&amp;gt;%
  ggplot() +
  geom_col(aes(x=bor, y=count, fill=mode)) + # add argument position=&amp;quot;fill&amp;quot; for proportional bars.
  scale_fill_manual(values=c(&amp;quot;#2171b5&amp;quot;,&amp;quot;#cb181d&amp;quot;, &amp;quot;#238b45&amp;quot;))+
  guides(fill=FALSE)+
  labs(x=&amp;quot;&amp;quot;, y=&amp;quot;count&amp;quot;) +
  facet_wrap(~type, ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot specification – bar charts:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: Create a vector of borough names (&lt;code&gt;bor_orders&lt;/code&gt;) used to order bars according to total commute counts (by destination), casting to a factor variable, as in line 3 of the plot specification. As bars are coloured according to travel mode, the data frame must be made narrower and longer, such that each row (observation) represents a count for a single travel mode by borough over origin or destination. This is achieved with &lt;a href=&#34;https://tidyr.tidyverse.org/reference/pivot_longer.html&#34;&gt;&lt;code&gt;pivot_longer()&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: Bar length varies according to commute counts, so map the ordered factor variable (&lt;code&gt;bor&lt;/code&gt;) to the x-axis and commute counts (&lt;code&gt;count&lt;/code&gt;) to the y-axis. Bars are filled according to &lt;code&gt;mode&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_col()&lt;/code&gt; for bars.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt;: &lt;code&gt;scale_fill_manual()&lt;/code&gt; for differentiating &lt;code&gt;mode&lt;/code&gt; by colour hue. Colours are defined in &lt;a href=&#34;https://en.wikipedia.org/wiki/Web_colors#Hex_triplet&#34;&gt;hex space&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Facets&lt;/strong&gt;: &lt;code&gt;facet_wrap()&lt;/code&gt; on &lt;code&gt;type&lt;/code&gt; for generating juxtaposed plots with node counts summarised by “origin” and “destination”.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: For the proportional bases add &lt;code&gt;position=&#34;fill&#34;&lt;/code&gt; argument to &lt;code&gt;geom_col()&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code – spatially-arranged bars:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot bars
nodes %&amp;gt;%
  left_join(london_grid_real %&amp;gt;% st_drop_geometry() %&amp;gt;% filter(type==&amp;quot;grid&amp;quot;) %&amp;gt;%
              select(authority, BOR,x,y),by=c(&amp;quot;bor&amp;quot;=&amp;quot;authority&amp;quot;)) %&amp;gt;%
  pivot_longer(cols=c(active, public_transport, car), names_to=&amp;quot;mode&amp;quot;, values_to=&amp;quot;count&amp;quot;) %&amp;gt;%
  group_by(bor) %&amp;gt;%
  mutate(bor_total=sum(count)) %&amp;gt;%
  ggplot() +
  geom_col(aes(x=type, y=count/bor_total, fill=mode))+
  geom_text(data=. %&amp;gt;% filter(mode==&amp;quot;active&amp;quot;, type==&amp;quot;destination&amp;quot;), aes(x=1.5, y=1, label=BOR), vjust=&amp;quot;top&amp;quot;, hjust=&amp;quot;centre&amp;quot;)+
  geom_text(data=. %&amp;gt;% filter(mode==&amp;quot;active&amp;quot;, type==&amp;quot;destination&amp;quot;), aes(x=1.4, y=.85, label=&amp;quot;dest&amp;quot;), vjust=&amp;quot;top&amp;quot;, hjust=&amp;quot;right&amp;quot;, size=3)+
  geom_text(data=. %&amp;gt;% filter(mode==&amp;quot;active&amp;quot;, type==&amp;quot;origin&amp;quot;), aes(x=1.6, y=.85, label=&amp;quot;origin&amp;quot;), vjust=&amp;quot;top&amp;quot;, hjust=&amp;quot;left&amp;quot;, size=3)+
  scale_fill_manual(values=c(&amp;quot;#2171b5&amp;quot;,&amp;quot;#cb181d&amp;quot;, &amp;quot;#238b45&amp;quot;))+
  facet_grid(y~x)+
  guides(fill=FALSE)+
  theme(
    panel.spacing.y=unit(.2, &amp;quot;lines&amp;quot;), panel.spacing.x=unit(.2, &amp;quot;lines&amp;quot;),
    panel.background = element_rect(fill=&amp;quot;#ffffff&amp;quot;, colour=&amp;quot;#ffffff&amp;quot;),
    axis.title.x = element_blank(),axis.title.y = element_blank(),
    axis.text.x=element_blank(), axis.text.y = element_blank(),
    strip.text.x=element_blank(), strip.text.y = element_blank(),
    panel.grid=element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot specification – spatially-arranged bars:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: A few things to note here. First we must &lt;code&gt;left_join()&lt;/code&gt; on the dataset that gives us the grid locations of each borough (&lt;code&gt;london_grid_real&lt;/code&gt;) so that the bars can be spatially arranged (via &lt;code&gt;facet_grid()&lt;/code&gt;). We strip out the &lt;code&gt;geometry&lt;/code&gt; from this dataset (it is not required), and select only those variables that we need: the &lt;code&gt;x,y&lt;/code&gt; position in the grid, borough names for matching with our nodes dataset (“&lt;code&gt;bor&lt;/code&gt;”=“&lt;code&gt;authority&lt;/code&gt;”) and abbreviation for labelling (&lt;code&gt;BOR&lt;/code&gt;). We need to reshape the dataset in the same way as for the non-spatially-arranged bars – and so the call to &lt;code&gt;pivot_wider()&lt;/code&gt;. Also, we want to scale the height of bars for each borough such that the relative number of in- and out- commutes within-borough (commute &lt;code&gt;type&lt;/code&gt;) can be compared. To do this, we calculate a &lt;code&gt;bor_total&lt;/code&gt; variable, which is the total number of residents commuting out-, and jobs filled by workers commuting in-, for each borough. Note that there is double counting here where workers live and work in the same borough.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: Bar length (y-axis) varies according to commute counts normalised by the &lt;code&gt;bor_total&lt;/code&gt; variable. Separate bars are drawn depending on &lt;code&gt;type&lt;/code&gt;, summarised over “origin” or “destination”, so &lt;code&gt;type&lt;/code&gt; is mapped to the x-axis. Again bars are filled according to &lt;code&gt;mode&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_col()&lt;/code&gt; for bars.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt;: &lt;code&gt;scale_fill_manual()&lt;/code&gt; for differentiating &lt;code&gt;mode&lt;/code&gt; by colour hue. Colours are defined in &lt;a href=&#34;https://en.wikipedia.org/wiki/Web_colors#Hex_triplet&#34;&gt;hex space&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Facets&lt;/strong&gt;: &lt;code&gt;facet_grid()&lt;/code&gt; for laying out plots with a relaxed geographic (2D) arrangement. This is the &lt;a href=&#34;https://github.com/aftertheflood/londonsquared&#34;&gt;LondondSquared&lt;/a&gt; layout from the &lt;code&gt;london_grid_real&lt;/code&gt; table.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Various within &lt;code&gt;theme()&lt;/code&gt; to remove, for example, unnecessary axis and panel labels, which are effectively grid references for 2D arrangement. The plot is annotated with text labels (&lt;code&gt;geom_text()&lt;/code&gt;): borough names using the abbreviated &lt;code&gt;BOR&lt;/code&gt; variable and also the &lt;code&gt;type&lt;/code&gt; of bar (origin or destination). Labels are positioned within each facet panel by manually passing an &lt;code&gt;x,y&lt;/code&gt; location, and supplying a value for the &lt;code&gt;type&lt;/code&gt; label and using the &lt;code&gt;BOR&lt;/code&gt; variable for the borough label. We use &lt;code&gt;filter()&lt;/code&gt; here as otherwise a text label would appear for each row of the dataset. Try removing the call to &lt;code&gt;data=&lt;/code&gt; in the &lt;code&gt;geom_text&lt;/code&gt; lines to explore this.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;analyse-over-edges&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Analyse over edges&lt;/h3&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:mode-analysis-od-map&#34;&gt;10&lt;/a&gt; displays OD maps of commuting between boroughs using public transport. In a similar way to Figure &lt;a href=&#34;#fig:mode-analysis&#34;&gt;9&lt;/a&gt; maps are created separately with a destination and origin focus. The reference cells for the maps on the left are &lt;em&gt;destinations&lt;/em&gt; and the small cells are coloured according to number of commutes into- those destinations for work; the references cells for the maps on the right are origins and small cells are coloured according to number of commutes by residents out-of those boroughs for work. I won’t do any interpretation; that is reserved for this session’s &lt;a href=&#34;../homework/05-homework/&#34;&gt;Homework&lt;/a&gt;. A consolation is that there is no additional coding required for the homework.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:mode-analysis-od-map&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/05-class_files/od-maps-mode.png&#34; alt=&#34;OD map of commutes between London boroughs&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: OD map of commutes between London boroughs
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Temporary plot object of data joined to geom_sf geometries.
# D-OD map so geometries join on origin (edit this to switch between D-OD and O-DO matrix).
plot_data_temp &amp;lt;- london_grid_real %&amp;gt;% filter(type==&amp;quot;grid&amp;quot;) %&amp;gt;%
right_join(edges, by=c(&amp;quot;authority&amp;quot;=&amp;quot;o_bor&amp;quot;)) %&amp;gt;%
  mutate(o_bor=authority) %&amp;gt;% rename(o_fx=x, o_fy=y) %&amp;gt;%
  left_join(london_grid_real %&amp;gt;%
    filter(type==&amp;quot;grid&amp;quot;) %&amp;gt;% st_drop_geometry() %&amp;gt;% select(authority,x,y), by=c(&amp;quot;d_bor&amp;quot;=&amp;quot;authority&amp;quot;)
    ) %&amp;gt;%
  rename(d_fx=x, d_fy=y) %&amp;gt;%
  # Identify borough in focus (edit this to switch between D-OD and O-DO matrix).
  mutate(bor_label=if_else(o_bor==d_bor,d_bor,&amp;quot;&amp;quot;),
                                            bor_focus=if_else(o_bor==d_bor,1,0))

# Bounding box for positioning text labels.
bbox_grid &amp;lt;- st_bbox(london_grid_real %&amp;gt;% filter(type==&amp;quot;grid&amp;quot;))

# Draw plot -- simple.
plot_data_temp %&amp;gt;%
  ggplot()+
  geom_sf(aes(fill=public_transport), colour=&amp;quot;#616161&amp;quot;, size=0.15)+
  coord_sf(crs=st_crs(plot_data_temp), datum=NA)+
  guides(fill=FALSE)+
  facet_grid(d_fy~d_fx, shrink=FALSE)+
  scale_fill_distiller(palette=&amp;quot;Greens&amp;quot;, direction=1)

# Draw plot -- with text annotations.
plot_data_temp %&amp;gt;%
  ggplot()+
  geom_sf(aes(fill=public_transport), colour=&amp;quot;#616161&amp;quot;, size=0.15)+
  geom_sf(data=. %&amp;gt;% filter(bor_focus==1), fill=&amp;quot;transparent&amp;quot;,  colour=&amp;quot;#373737&amp;quot;, size=0.3)+
  geom_text(data=. %&amp;gt;% filter(bor_focus==1),  aes(x=east, y=north, label=str_sub(BOR,1,1)),
            colour=&amp;quot;#252525&amp;quot;, alpha=1.0, size=2, show.legend=FALSE, hjust=&amp;quot;centre&amp;quot;, vjust=&amp;quot;middle&amp;quot;)+
  geom_text(data=. %&amp;gt;% filter(bor_focus==1), aes(x=bbox_grid$xmax, y=bbox_grid$ymin, label=BOR),
            colour=&amp;quot;#252525&amp;quot;, alpha=0.6, size=2, show.legend=FALSE, hjust=&amp;quot;right&amp;quot;, vjust=&amp;quot;bottom&amp;quot;)+
  coord_sf(crs=st_crs(plot_data_temp), datum=NA)+
  guides(fill=FALSE)+
  facet_grid(d_fy~d_fx, shrink=FALSE)+
  scale_fill_distiller(palette=&amp;quot;Greens&amp;quot;, direction=1)+
  theme(
    panel.spacing=unit(0.1, &amp;quot;lines&amp;quot;),
    axis.title.x=element_blank(),axis.title.y=element_blank(),
    strip.text.x = element_blank(), strip.text.y = element_blank(),
    panel.background = element_rect(fill=&amp;quot;#ffffff&amp;quot;, colour=&amp;quot;#ffffff&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot specification:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1. Data&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;We create a temporary staging data frame for this plot (&lt;code&gt;plot_data_temp&lt;/code&gt;). This is our edges dataset, joined twice on &lt;code&gt;london_grid_real&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The first join. The code above is for generating a D-OD map, where the large cells are destinations and the small cells are origins. In the plot spec you will see, as in the spatially arranged bars, that for D-OD maps we facet on destination using the LondonSquared layout. This means that we want to bring in &lt;code&gt;geometry&lt;/code&gt; data for the origin borough in each OD pair and make this explicit in the join – &lt;code&gt;by=c(&#34;authority&#34;=&#34;o_bor&#34;)&lt;/code&gt;. We also &lt;code&gt;rename()&lt;/code&gt; the joined variables accordingly – &lt;code&gt;o_fx, o_fy&lt;/code&gt; refers to the LondonSquared grid cell references of the origins. If we wanted to use the LondonSquared layout only for faceting, and represent origin boroughs using their real geography &lt;span class=&#34;citation&#34;&gt;(as in &lt;a href=&#34;#ref-beecham_characterising_2019&#34; role=&#34;doc-biblioref&#34;&gt;Beecham and Slingsby 2019&lt;/a&gt;; &lt;a href=&#34;#ref-slingsby_od_2014&#34; role=&#34;doc-biblioref&#34;&gt;Slingsby, Kelly, and Dykes 2014&lt;/a&gt;)&lt;/span&gt; we could join on the real and not grid geometries – with &lt;code&gt;filter(type==&#34;real&#34;)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The second join is on destination (origin in the case of an O-OD map). We drop the geometry data from the joining London boundaries object, but keep the LondonSquared layout grid references and rename accordingly – e.g. &lt;code&gt;rename(d_fx=x, d_fy=y)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Finally in the &lt;code&gt;mutate()&lt;/code&gt; we generate a new variable identifying the borough in focus (&lt;code&gt;bor_focus&lt;/code&gt;), destination in this case, and a text label variable for annotating plots on this (&lt;code&gt;bor_label&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;An additional derived object contains coordinate pairs describing the geographic extent of our geometries data – see &lt;a href=&#34;https://r-spatial.github.io/sf/reference/st_bbox.html&#34;&gt;st_bbox()&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2. Encoding&lt;/strong&gt;: Cells are coloured according to commute count by public transport (&lt;code&gt;fill=public_transport&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3. Marks&lt;/strong&gt;: &lt;code&gt;geom_sf()&lt;/code&gt; – these are effectively spatially arranged choropleth maps. In the example we use a regular grid geometries, but we could easily use real geographies by filtering &lt;code&gt;london_grid_real&lt;/code&gt; on &lt;code&gt;type==&#34;real&#34;&lt;/code&gt; in the data staging code.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4. Scale&lt;/strong&gt;: &lt;code&gt;scale_fill_distiller()&lt;/code&gt; for a continuous colour scheme using the ColorBrewer &lt;code&gt;Greens&lt;/code&gt; palette.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;5. Facets&lt;/strong&gt;: &lt;code&gt;facet_grid()&lt;/code&gt; for laying out plots with a relaxed geographic (2D) arrangement. Again we facet on destination as this is a D-OD map.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;6. Setting&lt;/strong&gt;: Various within &lt;code&gt;theme()&lt;/code&gt; to remove, for example, unnecessary axis and panel labels. The plot is annotated with text labels (&lt;code&gt;geom_text()&lt;/code&gt;) – borough names using the abbreviated &lt;code&gt;BOR&lt;/code&gt; variable. The three letter abbreviations are positioned in &lt;code&gt;x,y&lt;/code&gt; to the bottom-right of the plot using the values in the bounding box object (&lt;code&gt;bbox_grid&lt;/code&gt;). The single letter abbreviation is positioned at the centroid of the grid geometries &lt;code&gt;east, north&lt;/code&gt;. Single letters are derived from the &lt;code&gt;BOR&lt;/code&gt; variable using &lt;a href=&#34;https://stringr.tidyverse.org/reference/str_sub.html&#34;&gt;str_sub()&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;/p&gt;
&lt;p&gt;Some of the code around the annotated text labels is a little hacky and not particularly elegant. However, with this removed, as in the block above, the plot specification is really quite simple and more importantly links to how OD maps are constructed – &lt;code&gt;fill&lt;/code&gt; cells according to a count variable and juxtapose or &lt;code&gt;facet&lt;/code&gt; according to the reference, or &lt;code&gt;focus&lt;/code&gt;, for comparison.&lt;/p&gt;
&lt;p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Network data – data which describe &lt;em&gt;relations&lt;/em&gt; between entities – are challenging to represent, work with and analyse. It is for this reason that visual approaches are often used in their analysis. Due to their complexity, many network visualizations fall into a common pitfall of simply re-presenting that complexity without exposing useful structure or insight into the phenomena being analysed. Through an analysis of 2011 Census travel-to-work data in London, we demonstrated some approaches to analysing and inferring structure in a category of network data common to Geographers: spatial origin-destination data. Spatially-arranged node-link diagrams are highly intuitive, support synoptic overview and detection of hierarchies in network (a theme not discussed in this session), but were of limited success in representing detailed patterns in travel-to-work within and between London boroughs. Instead we used matrix-based views, including spatially arranged matrices or &lt;a href=&#34;https://xeno.graphics/od-map/&#34;&gt;OD Maps&lt;/a&gt;. As ever, the appropriateness of either approach – node-link based or matrix-based representations – depends on data, analysis purpose and audience.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-beecham_characterising_2019&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R., and A. Slingsby. 2019. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Characterising labour market self-containment in London with geographically arranged small multiples&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;Environment and Planning A: Economy and Space&lt;/span&gt;&lt;/em&gt; 51 (6): 1217–24.
&lt;/div&gt;
&lt;div id=&#34;ref-beecham_exploring_2014&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R., and J. Wood. 2014. &lt;span&gt;“Exploring Gendered Cycling Behaviours Within a Large-Scale Behavioural Data-Set.”&lt;/span&gt; &lt;em&gt;Transportation Planning and Technology&lt;/em&gt; 37 (1). Taylor &amp;amp; Francis: 83–97.
&lt;/div&gt;
&lt;div id=&#34;ref-munzner_visualization_2014&#34; class=&#34;csl-entry&#34;&gt;
Munzner, T. 2014. &lt;em&gt;Visualization Analysis and Design&lt;/em&gt;. AK Peters Visualization Series. Boca Raton, FL: CRC Press.
&lt;/div&gt;
&lt;div id=&#34;ref-slingsby_od_2014&#34; class=&#34;csl-entry&#34;&gt;
Slingsby, A., M. Kelly, and J. Dykes. 2014. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;OD maps for showing changes inIrish female migration between 1851 and 1911&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;Environment and Planning A: Economy and Space&lt;/span&gt;&lt;/em&gt; 46 (12): 2795--2797.
&lt;/div&gt;
&lt;div id=&#34;ref-wickham_ggplot2_2020&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., D. Navarro, and T. Lin Pedersen. 2020. &lt;em&gt;Ggplot2: Elegant Graphics for Data Analysis&lt;/em&gt;. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-wood_visualisation_2010&#34; class=&#34;csl-entry&#34;&gt;
Wood, J., J. Dykes, and A. Slingsby. 2010. &lt;span&gt;“Visualisation of Origins, Destinations and Flows with OD Maps.”&lt;/span&gt; &lt;em&gt;The Cartographic Journal&lt;/em&gt; 47 (2): 117–29.
&lt;/div&gt;
&lt;div id=&#34;ref-wood_visualizing_2011&#34; class=&#34;csl-entry&#34;&gt;
Wood, J., A. Slingsby, and J. Dykes. 2011. &lt;span&gt;“Visualizing the Dynamics of London’s Bicycle-Hire Scheme.”&lt;/span&gt; &lt;em&gt;Cartographica: The International Journal for Geographic Information and Geovisualization&lt;/em&gt;, no. 4: 239–51.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualization for exploratory data analysis: using colour and layout for comparison</title>
      <link>/class/04-class/</link>
      <pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate>
      <guid>/class/04-class/</guid>
      <description>
&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;script src=&#34;../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;../rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;../rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;h2&gt;Contents&lt;/h2&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concepts&#34;&gt;Concepts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#exploratory-data-analysis-and-statistical-graphics&#34;&gt;Exploratory data analysis and statistical graphics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plots-for-continuous-variables&#34;&gt;Plots for continuous variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plots-for-categorical-variables&#34;&gt;Plots for categorical variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#supporting-comparison-with-layout-and-colour&#34;&gt;Supporting comparison with layout and colour&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#techniques&#34;&gt;Techniques&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#import&#34;&gt;Import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#explore&#34;&gt;Explore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;knowledge&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; An appreciation of the exploratory data analysis (EDA) worklow.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; The main chart types &lt;span class=&#34;citation&#34;&gt;(or &lt;em&gt;idioms&lt;/em&gt; &lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;Munzner 2014&lt;/a&gt;)&lt;/span&gt; for summarising variation within- and between- variables.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; The three strategies that can be used for supporting &lt;em&gt;comparison&lt;/em&gt; in EDA: juxtaposition, superposition and direct encoding &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gleicher_visual_2011&#34; role=&#34;doc-biblioref&#34;&gt;Gleicher and Roberts 2011&lt;/a&gt;)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;practical skills&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write &lt;code&gt;ggplot2&lt;/code&gt; specifications that use colour and layout to support comparison.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Exploratory Data Analysis (EDA) is a data-driven approach to analysis which aims to expose the properties and structure of a dataset, and from here suggest directions for analytic inquiry. In an EDA, relationships are quickly inferred, anomalies labelled, assumptions tested and new hypotheses and ideas are formulated. EDA relies heavily on visual approaches to analysis – it is common to generate many dozens of (often throwaway) data graphics.&lt;/p&gt;
&lt;p&gt;This session will demonstrate how the concepts and principles introduced previously – of data types and their visual encoding – can be applied to support EDA. It will do so by analysing &lt;a href=&#34;https://data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data&#34;&gt;STATS19&lt;/a&gt;, a dataset containing detailed information on every reported road traffic crash in Great Britain that resulted in personal injury. STATS19 is highly detailed, with many categorical variables. This session will start by revisiting commonly used chart idioms &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;Munzner 2014&lt;/a&gt;)&lt;/span&gt; for summarising within-variable variation and between-variable co-variation in a dataset. It will then focus more directly on the STATS19 case, and how detailed &lt;em&gt;comparison&lt;/em&gt; across many categorical variables can be effected using colour, layout and statistical computation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concepts&lt;/h2&gt;
&lt;div id=&#34;exploratory-data-analysis-and-statistical-graphics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory data analysis and statistical graphics&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The simple graph has brought more information to the data analyst’s mind than any other device.&lt;/p&gt;
&lt;p&gt;John Tukey&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In an Exploratory Data Analysis (EDA), graphical and statistical summaries are variously used to build knowledge and understanding of a dataset. The goal of EDA is to infer relationships, identify anomalies and test new ideas and hypotheses: it is a knowlegde-building activity. Rather than a formal set of techniques, EDA should be considered an &lt;em&gt;approach&lt;/em&gt; to analysis. It aims to reveal the underlying properties of variables in a dataset (central tendency and dispersion) and their structure (how variables relate to one another) and from there formulate hypotheses to be investigated.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Wickham and Grolemund (&lt;a href=&#34;#ref-wickham_r_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; identify two main questions that an EDA should address:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What type of &lt;strong&gt;variation&lt;/strong&gt; occurs &lt;strong&gt;within&lt;/strong&gt; variables of a dataset?&lt;/li&gt;
&lt;li&gt;What type of &lt;strong&gt;covariation&lt;/strong&gt; occurs &lt;strong&gt;between&lt;/strong&gt; variables of a dataset?&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:variable-types&#34;&gt;Table 1: &lt;/span&gt;Statistics and data graphics that can be used to summarise variation and covariation by variable type.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Measurement
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Statistics
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Chart idiom
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;3&#34; style=&#34;border-bottom: 0px solid;&#34;&gt;
Within-variable variation
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 8em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;code&gt;Nominal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
mode | entropy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bar charts, dot plots …
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 8em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;code&gt;Ordinal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
median | percentile
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bar charts, dot plots …
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 8em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;code&gt;Continuous&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
mean | variance
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
histograms, box plots, density plots …
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;3&#34; style=&#34;border-bottom: 0px solid;&#34;&gt;
Between-variable variation
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 8em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;code&gt;Nominal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
contingency tables
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
mosaic/spine plots …
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 8em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;code&gt;Ordinal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
rank correlation
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
slope/bump charts …
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 8em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;code&gt;Continuous&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
correlation
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
scatterplots, parallel coordinate plots …
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You will recall from early statistics classes, usually under the theme &lt;em&gt;descriptive statistics&lt;/em&gt;, that when summarising within-variable variation, we are interested in a variable’s spread or dispersion and its location within this distribution (central tendency). Different statistics can be applied, but a familiar distinction is between measures of central tendency that are or are not robust to outliers (e.g. mode and median versus mean). Correlation statistics are most obviously applied when studying between-variable covariation, but other statistics, such as odds ratios and chi-square tests are commonly used when studying covariation in &lt;a href=&#34;https://en.wikipedia.org/wiki/Contingency_table&#34;&gt;contingency tables&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Decisions around which statistic to use depend on a variable’s measurement-level (e.g. Table &lt;a href=&#34;#tab:variable-types&#34;&gt;1&lt;/a&gt;). As demonstrated by &lt;a href=&#34;https://en.wikipedia.org/wiki/Anscombe%27s_quartet&#34;&gt;Anscombe’s quartet&lt;/a&gt;, statistical summaries can hide important structure, or assume structure that doesn’t exist. None of the measures of central tendency in Table &lt;a href=&#34;#tab:variable-types&#34;&gt;1&lt;/a&gt; would expose whether a variable for instance is multi-modal and only when studying all measures of central tendency and dispersion together might it be possible to guess at the presence of outliers that could undermine statistical assumptions. It is for this reason that &lt;em&gt;data visualization&lt;/em&gt; is seen as intrinsic to EDA &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-tukey_exploratory_1977&#34; role=&#34;doc-biblioref&#34;&gt;Tukey 1977&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plots-for-continuous-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plots for continuous variables&lt;/h3&gt;
&lt;div id=&#34;within-variable-variation-histograms-desity-plots-boxplots&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Within-variable variation: histograms, desity plots, boxplots&lt;/h4&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:univariate-continuous&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/univariate-plots.png&#34; alt=&#34;Univariate plots of dispersion.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Univariate plots of dispersion.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:univariate-continuous&#34;&gt;1&lt;/a&gt; presents statistical graphics that are commonly used to display the distribution of continuous variables measured on a &lt;code&gt;ratio&lt;/code&gt; or &lt;code&gt;interval&lt;/code&gt; scale – in this instance the age of casualty for a random sample of Stast19 road crashes (&lt;code&gt;casualty_age&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In the bottom row is a &lt;strong&gt;strip-plot&lt;/strong&gt;. Every observation is displayed as a dot and mapped to &lt;code&gt;x-position&lt;/code&gt;, with transparency and a random vertical perturbation applied to resolve occlusion due to overlapping observations. Although strip-plots scale poorly, the advantage is that all observations are displayed without the need to impose an aggregation. It is possible to visually identify the &lt;em&gt;location&lt;/em&gt; of the distribution – denser dots towards 20-25 age range – but also that there is quite a degree of spread across the age values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Histograms&lt;/strong&gt; were used in the &lt;a href=&#34;../class/03-class/&#34;&gt;previous session&lt;/a&gt; when analysing the 2019 UK General Election dataset. Histograms partition continuous variables into equal-range bins and observations in each bin are counted. These counts are encoded on an aligned scale using bar height. Increasing the size of the bins increases the resolution of the graphical summary. If reasonable decisions are made around choice of bin, histograms give distributions a “shape” that is expressive. It is easy to identify the &lt;em&gt;location&lt;/em&gt; of a distribution and, in using length on aligned scale to encode frequency, estimate relative densities between different parts of the distribution. Different from the strip-plot, the histogram allows us to intuit that despite the heavy spread, the distribution of &lt;code&gt;casualty_age&lt;/code&gt; is right-skewed, and we’d expect this given the location of the mean (36 years) relative to the median (33 years).&lt;/p&gt;
&lt;p&gt;A problem with histograms is the potential for discontinuities and artificial edge-effects around the bins. &lt;strong&gt;Density plots&lt;/strong&gt; overcome this and can be thought of as smoothed histograms. They use &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34;&gt;kernel density estimation&lt;/a&gt; (KDE) to show the &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_density_function&#34;&gt;probability density function&lt;/a&gt; of a variable – the relative amount of probability attached to each value of &lt;code&gt;casualty_age&lt;/code&gt;. As with histograms, there are decisions to be made around how data are partitioned into bins (or KDE bandwidths). From glancing at the density plots an overall “shape” to the distribution can be immediately derived. It is also possible to infer statistical properties: the mode of the distribution (the highest density), the mean (by visual averaging) and median (finding the midpoint of the area under the curve). Density plots are better suited to datasets that contain a reasonably large number of observations and due to the smoothing function it is possible to generate a density plot that suggests nonsensical values (e.g. negative ages in this case if I hadn’t censored the plot range).&lt;/p&gt;
&lt;p&gt;Finally, &lt;strong&gt;boxplots&lt;/strong&gt;, or &lt;strong&gt;box and whisker&lt;/strong&gt; plots &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mcgill_variations_1978&#34; role=&#34;doc-biblioref&#34;&gt;McGill and Larsen 1978&lt;/a&gt;)&lt;/span&gt;, encode the statistical properties that we infer from strip-plots, histograms and density plots directly. The box is the interquartile range (&lt;span class=&#34;math inline&#34;&gt;\(IQR\)&lt;/span&gt;) of the &lt;code&gt;casualty_age&lt;/code&gt; variable, the vertical line splitting the box is the median, and the whiskers are placed at observations &lt;span class=&#34;math inline&#34;&gt;\(\leq 1.5*IQR\)&lt;/span&gt;. Whilst we lose information around the shape of a distribution, box-plots are space-efficient and useful for comparing many distributions at once.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;This discussion was a little prosaic – we haven’t made too many observations to advance our knowledge of the dataset. I was surprised at the low average age of road casualties and so quickly explored the distribution of &lt;code&gt;casualty_age&lt;/code&gt; conditioning on another variable and differentiating between variable values using colour. Figure &lt;a href=&#34;#fig:boxplots-class&#34;&gt;2&lt;/a&gt; displays boxplots of the location and spread in &lt;code&gt;casualty_age&lt;/code&gt; by vehicle type (left) and also by &lt;code&gt;casualty_class&lt;/code&gt; for all crashes involving pedestrians. A noteworthy pattern is that riders of bicycles and motorcycles tend to be younger than the pedestrians they are contacting with, whereas for buses, taxis, HGVs and cars the reverse is true. Pedestrians involved in crashes with cars are especially skewed towards the younger ages.&lt;/p&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:boxplots-class&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/boxplot-by-class.png&#34; alt=&#34;Boxplots of casualty age by vehicle type and class.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Boxplots of casualty age by vehicle type and class.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;between-variable-covariation-scatterplots&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Between-variable covariation: scatterplots&lt;/h4&gt;
&lt;p&gt;The &lt;a href=&#34;../class/03-class/&#34;&gt;previous session&lt;/a&gt; included several &lt;strong&gt;scatterplots&lt;/strong&gt; for displaying association between two quantitative variables. Scatterplots are used to check whether the association between variables is linear, as in &lt;a href=&#34;https://en.wikipedia.org/wiki/Anscombe%27s_quartet&#34;&gt;Anscombe’s quartet&lt;/a&gt;, but also to make inferences about the direction and intensity of linear correlation between variables – the extent to which &lt;strong&gt;values&lt;/strong&gt; in one variable depend on the values of another – and also around the nature of variation between variables – the extent to which &lt;strong&gt;variation&lt;/strong&gt; in one variable depends on another (&lt;a href=&#34;https://en.wikipedia.org/wiki/Heteroscedasticity&#34;&gt;heteroscedasticity&lt;/a&gt;). Although other &lt;em&gt;chart idioms&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;Munzner 2014&lt;/a&gt;)&lt;/span&gt; for displaying bivariate data exist, empirical studies in Information Visualization have demonstrated that aggregate correlation statistics can be reliably estimated from scatterplots &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rensink_perception_2010&#34; role=&#34;doc-biblioref&#34;&gt;Rensink and Baldridge 2010&lt;/a&gt;; &lt;a href=&#34;#ref-harrison_ranking_2014&#34; role=&#34;doc-biblioref&#34;&gt;Harrison et al. 2014&lt;/a&gt;; &lt;a href=&#34;#ref-kay_beyond_2016&#34; role=&#34;doc-biblioref&#34;&gt;Kay and Heer 2016&lt;/a&gt;; &lt;a href=&#34;#ref-correll_regression_2017&#34; role=&#34;doc-biblioref&#34;&gt;Correll and Heer 2017a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;There are few variables in the STATS19 dataset measured on a continuous scale, but based on the analysis above, we may wish to explore more directly whether an association exists between the age of pedestrians and drivers for pedestrian-vehicle crashes. The scatterplots in Figure &lt;a href=&#34;#fig:scatters-type&#34;&gt;3&lt;/a&gt; show that no obvious linear association exists.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:scatters-type&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/scatter-by-type.png&#34; alt=&#34;Scatterplots of pedestrian age by driver age and grouped by vehicle type.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Scatterplots of pedestrian age by driver age and grouped by vehicle type.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    It is common in an EDA to quickly compare associations between many quantitive variables in a dataset using &lt;a href=&#34;https://en.wikipedia.org/wiki/Scatter_plot#Scatter_plot_matrices&#34;&gt;scatterplot matrices&lt;/a&gt; or alternatively &lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_coordinates&#34;&gt;parallel coordinate plots&lt;/a&gt;. We will use both in sessions 6 and 7 when building models that attempt to formally structure and explain &lt;strong&gt;between-variable&lt;/strong&gt; covariation.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plots-for-categorical-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plots for categorical variables&lt;/h3&gt;
&lt;div id=&#34;within-variable-variation-bars-dotplots-heatmaps&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Within-variable variation: bars, dotplots, heatmaps&lt;/h4&gt;
&lt;p&gt;With categorical variables we are interested in exploring how relative frequencies distribute across the variable’s categories. Bar charts are most commonly used. As established in the previous session, length is an efficient visual channel for encoding quantities, counts in this case. Often it is useful to flip bar charts on their side so that category labels are arranged horizontally for ease of reading and, unless there is a natural ordering to categories, arrange the bars in descending order based on their frequency, as in Figure &lt;a href=&#34;#fig:bars&#34;&gt;4&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:bars&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/bars.png&#34; alt=&#34;Bars displaying crash frequencies by vehicle type.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Bars displaying crash frequencies by vehicle type.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Bar charts are effective at conveying frequencies where the number of categories is reasonably small. For summarising frequencies across many categories, alternatives chart types that minimise non-data-ink, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Dot_plot_(statistics)&#34;&gt;Cleveland dot plots&lt;/a&gt; and heatmaps may be appropriate. The left plot in Figure &lt;a href=&#34;#fig:dots-heatmap&#34;&gt;5&lt;/a&gt; displays crash counts for boroughs in London, ordered by crash frequency and grouped by whether boroughs are in inner- or outer- London. To the right is a heatmap with the same ordering and grouping of boroughs applied to the rows and with columns coloured according to crash frequencies by time period and further grouped by day of week. From scanning the graphic we can make several observations: that reported crashes tend to occur in the midday or evening peak (the middle two columns of a day are typically darker); that in relative terms there are more crashes recorded in outer London boroughs than inner London boroughs during the middle of the day on weekends (salient dark strips corresponding with midday Saturday and Sunday).&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:dots-heatmap&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/borough-freqs.png&#34; alt=&#34;[Cleveland dot plots](https://en.wikipedia.org/wiki/Dot_plot_(statistics)) and heatmaps summarising crash frequencies by London borough and period of day.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: &lt;a href=&#34;https://en.wikipedia.org/wiki/Dot_plot_(statistics)&#34;&gt;Cleveland dot plots&lt;/a&gt; and heatmaps summarising crash frequencies by London borough and period of day.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Remembering &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;’s ordering of visual channels, Cleveland dot plots, which use position on an aligned scale, are far better at encoding quantities than heatmaps, which use colour value. Stevens’s power estimates for the perception of changes in lightness were &lt;span class=&#34;math inline&#34;&gt;(n~0.5)&lt;/span&gt;. Despite this, I think the heatmap in Figure &lt;a href=&#34;#fig:dots-heatmap&#34;&gt;5&lt;/a&gt; is a successful data graphic. It displays 924 values in a reasonably compact space with cells arranged to emphasise comparison. This illustrates the point raised in the previous session around the trade-offs that must be negotiated when designing graphics for analysis. In this case we trade encoding effectiveness for data density.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;between-variable-covariation-contingency-tables-standardised-bars-mosaic-plots&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Between-variable covariation: contingency tables, standardised bars, mosaic plots&lt;/h4&gt;
&lt;p&gt;To study the association &lt;em&gt;between&lt;/em&gt; two categorical variables, &lt;a href=&#34;https://en.wikipedia.org/wiki/Contingency_table&#34;&gt;&lt;strong&gt;contingency tables&lt;/strong&gt;&lt;/a&gt; are routinely used. Each cell in a contingency table is the joint frequency of two category outcomes occurring together. The contingency table below (Table &lt;a href=&#34;#tab:tab-vehicle-severity&#34;&gt;2&lt;/a&gt;) looks at how crashes by vehicle type co-vary with injury severity. The question being posed is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are crashes involving certain vehicle associated with more/less severe injury than others?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The table presents counts by injury severity for pedestrians only; so these are pedestrians being hit by cars, vans, bicycles etc. Injury severity is either &lt;em&gt;slight&lt;/em&gt; or serious/fatal (&lt;em&gt;KSI&lt;/em&gt;).&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:tab-vehicle-severity&#34;&gt;Table 2: &lt;/span&gt;Pedestrian casualties by vehicle involved and injury severity.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Vehicle type
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
KSI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Slight
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Row Total
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Car
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42305
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
137924
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
180229
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Van
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3786
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11422
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
15208
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Taxi
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2580
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9188
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
11768
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bus
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2951
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8425
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
11376
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Motorcycle
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2137
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7102
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
9239
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HGV
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2030
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
5225
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1096
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3302
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
4398
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bicycle
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1033
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3184
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
4217
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Column Total
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
57918
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
183742
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;font-weight: bold;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To display these quantities graphically we could update our earlier bar chart to create &lt;em&gt;stacked&lt;/em&gt; bars that use colour to distinguish injury severity (Figure &lt;a href=&#34;#fig:bars-vehicle-injury&#34;&gt;6&lt;/a&gt;). Injury severity is an &lt;code&gt;ordinal&lt;/code&gt; variable and the choice of colour in Figure &lt;a href=&#34;#fig:bars-vehicle-injury&#34;&gt;6&lt;/a&gt; reflects this order – dark red for &lt;em&gt;KSI&lt;/em&gt;, light red for &lt;em&gt;slight&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:bars-vehicle-injury&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/bars-assoc.png&#34; alt=&#34;Bars (and Mosaic plot) displaying association between vehicle type and injury severity.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Bars (and Mosaic plot) displaying association between vehicle type and injury severity.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Cars are by far the dominant travel mode, causing the largest number of slight and serious injuries to pedestrians. Whether or not cars result in more severe injury rates than other travel modes is not clear from the left-most chart. Length effectively encodes &lt;em&gt;absolute&lt;/em&gt; crash counts but &lt;em&gt;relative&lt;/em&gt; comparison of injury severity between vehicle types is challenging. The graphic can be reconfigured to emphasise relative comparison – fixing the absolute length of bars and splitting the bars according to proportional injury severity (middle). Now we see that relative injury severity of pedestrians – KSI as a proportion of all crashes – varies slightly between modes (c.22% Taxis - c.26% Buses) except for HGVs where around 40% of crashes result in a serious injury or fatality to the pedestrian. However, we lose a sense of the absolute numbers involved.&lt;/p&gt;
&lt;p&gt;This can be problematic in an EDA when comparisons of absolute and relative frequencies over many category combinations are made. For combinations in a contingency table that are rare, for example bicycle-pedestrian casualties, it may only take a small number observations in a particular direction to change the KSI rate. Since proportional summaries are agnostic to sample size, they can induce false discoveries, overemphasising patterns that may be unlikely to replicate. It is sometimes desirable, then, to update &lt;em&gt;standardised&lt;/em&gt; bar charts so that they are weighted by frequency – to make more visually salient those categories that occur more often and visually downweight those that occur less often. This is possible using &lt;strong&gt;mosaic plots&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-friendly_mosaic_1992&#34; role=&#34;doc-biblioref&#34;&gt;Friendly 1992&lt;/a&gt;)&lt;/span&gt;. Bar widths and heights are allowed to vary; so bar area is proportional to &lt;em&gt;absolute&lt;/em&gt; number of observations and bars are further subdivided for &lt;em&gt;relative&lt;/em&gt; comparison across category values. Mosaic plots are useful tools for exploratory analysis. That they are space-efficient and regularly sized (squarified) means that that they can be laid out for comparison.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The Mosaic plot in Figure &lt;a href=&#34;#fig:bars-vehicle-injury&#34;&gt;6&lt;/a&gt; was generated using the &lt;a href=&#34;https://haleyjeppson.github.io/ggmosaic/&#34;&gt;&lt;code&gt;ggmosaic&lt;/code&gt;&lt;/a&gt; package, an extension to &lt;code&gt;ggplot2&lt;/code&gt;. We won’t cover the details of how to implement Mosaic plots in this session. Should you wish to apply them as part of the &lt;em&gt;data challenge&lt;/em&gt; element of the homework for this session, &lt;a href=&#34;https://haleyjeppson.github.io/ggmosaic/&#34;&gt;&lt;code&gt;ggmosaic&lt;/code&gt;&lt;/a&gt;’s documentation pages are an excellent resource.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deriving-effect-sizes-from-contingency-tables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Deriving effect sizes from contingency tables&lt;/h4&gt;
&lt;p&gt;When generating the contingency table on which these plots are based, there are some implied questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does casualty severity vary depending on the type of vehicle involved in the crash?&lt;/li&gt;
&lt;li&gt;For which vehicle types is injury severity the highest or lowest?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An imbalance is clearly to be expected, but we may start by assuming that injury severity rates, the proportion of pedestrian injuries that are KSI, is independent of vehicle type and look to quantify and locate any imbalance in these proportions. This happens automatically when comparing the relative widths of the dark red bars in Figure &lt;a href=&#34;#fig:bars-vehicle-injury&#34;&gt;6&lt;/a&gt;. Annotating with an expectation – e.g. the injury severity rate for all pedestrian casualties (middle plot) – helps to further locate differences from expectation for specific vehicles.&lt;/p&gt;
&lt;p&gt;Effect size estimates could also be computed directly using risk ratios (RR) – comparing the observed severity rates by vehicle type against an expectation of independence of severity rates by vehicle type. From this, we could find that the RR for HGVs is 1.64: a crash involving an HGV is 64% more likely to result in a fatality or serious injury to the pedestrian than one not involving an HGV. Alternatively, we could calculate signed chi-score residuals &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-visalingam_signed_1981&#34; role=&#34;doc-biblioref&#34;&gt;Visalingam 1981&lt;/a&gt;)&lt;/span&gt;, a measure of effect size that is sensitive both to absolute and relative differences from expectation. Expected values are &lt;em&gt;counts&lt;/em&gt; calculated for each observation (each cell in our contingency table). Observations (&lt;span class=&#34;math inline&#34;&gt;\(O_i ... O_n\)&lt;/span&gt;) are then compared to expected values (&lt;span class=&#34;math inline&#34;&gt;\(E_i ... E_n\)&lt;/span&gt;) as below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\chi=\frac{O_i - E_i}{\sqrt{E_i}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The way in which the differences (residuals) between observed and expected values are standardised in the denominator is important. If the denominator was simply the raw expected value, the residuals would express the proportional difference between each observation and its expected value. The denominator is instead transformed using the square root (&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E_i}\)&lt;/span&gt;), which has the effect of inflating smaller expected values and squashing larger expected values, thereby giving greater saliency to differences that are large in both relative and absolute number. The Mosaic plot sort of does this visually.&lt;/p&gt;
&lt;p&gt;Expected values are calculated in the same way as the standard &lt;a href=&#34;https://en.wikipedia.org/wiki/Chi-squared_test&#34;&gt;chi-square statistic&lt;/a&gt; that tests for independence – so in this case, that counts of crashes by vehicle type distribute independently of injury severity (&lt;em&gt;Slight&lt;/em&gt; or &lt;em&gt;KSI&lt;/em&gt;) – and can be derived from the contingency table:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E_i = \frac{C_i \times R_i}{GT}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So for an observed value (&lt;span class=&#34;math inline&#34;&gt;\(O_i\)&lt;/span&gt;), &lt;span class=&#34;math inline&#34;&gt;\(C_i\)&lt;/span&gt; is its column total in the contingency table; &lt;span class=&#34;math inline&#34;&gt;\(R_i\)&lt;/span&gt; is its row total; and &lt;span class=&#34;math inline&#34;&gt;\(GT\)&lt;/span&gt; is the grand total. Below is the contingency table updated with expected values and signed-chi-scores. A score &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;0\)&lt;/span&gt; means that casualty counts for that injury type and vehicle is greater than expected; a score &lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;0\)&lt;/span&gt; means casualty counts for that injury type and vehicle is less than expected. The signed chi-square residuals have mathematical properties and can be interpreted as any standard score. They are assumed to have a mean &lt;span class=&#34;math inline&#34;&gt;\(\sim1\)&lt;/span&gt; and standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(\sim0\)&lt;/span&gt;, with residuals of &lt;span class=&#34;math inline&#34;&gt;\(±2\)&lt;/span&gt; having statistical “significance”.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:tab-vehicle-severity-resids&#34;&gt;Table 3: &lt;/span&gt;Pedestrian casualties by vehicle involved and injury severity: contingency table with signed chi-scores.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Observed
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Expected
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Signed chi-scores
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Vehicle type
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
KSI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Slight
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Row Total
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
KSI Exp
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Slight Exp
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
KSI Resid
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Slight Resid
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Car
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42305
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
137924
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
180229
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
137034
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Van
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3786
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11422
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
15208
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3645
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11563
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.34
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.31
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Taxi
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2580
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9188
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
11768
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2820
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8948
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.53
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.54
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bus
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2951
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8425
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
11376
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2726
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8650
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.41
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Motorcycle
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2137
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7102
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
9239
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2214
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7025
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.92
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HGV
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2030
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
5225
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1252
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3973
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-12.34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1096
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3302
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
4398
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1054
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3344
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.73
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bicycle
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1033
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3184
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
4217
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1011
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3206
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.70
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.39
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Column Total
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
57918
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
183742
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;font-weight: bold;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The benefit for EDA is that the signed-scores are very quick and easy to compute, can be derived entirely from the contingency table without applying prior knowledge. They help to identify and locate anomalies, or deviations from expectation, in a way that is sensitive to both absolute and relative numbers. A common workflow in EDA is &lt;span class=&#34;citation&#34;&gt;(e.g. &lt;a href=&#34;#ref-correl_surprise_2017&#34; role=&#34;doc-biblioref&#34;&gt;Correll and Heer 2017b&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Expose pattern&lt;/li&gt;
&lt;li&gt;Model an expectation derived from pattern&lt;/li&gt;
&lt;li&gt;Show deviation from expectation.&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- Remembering Figure \@ref(fig:dots-heatmap), for example, the boroughs in which the largest crash frequencies occur has already been established in the Cleveland dot plot, but this signal dominates somewhat in the heatmap -- for boroughs that contain fewer crashes relative comparison across the columns challenging. --&gt;
&lt;p&gt;The heatmap in Figure &lt;a href=&#34;#fig:dots-heatmap&#34;&gt;5&lt;/a&gt; is effectively a contingency table, so we could demonstrate this by generating a modelled expectation – signed chi-score residuals for each cell. Figure &lt;a href=&#34;#fig:borough-freqs-resids&#34;&gt;7&lt;/a&gt; does this for days of the week and periods of the day separately. The signed residuals are mapped to a &lt;a href=&#34;https://web.natur.cuni.cz/~langhamr/lectures/vtfg1/mapinfo_2/barvy/colors.html#diverging&#34;&gt;diverging colour scheme&lt;/a&gt; – purple for cells with fewer crash counts than expected, green for cells with more crash counts than expected. To simplify things a little I generated heatmaps separately for day of week and period of day. In each case the assumption, the modelled expectation, is that crash counts by borough distribute independently of day of week or period of day.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:borough-freqs-resids&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/borough-freqs-resids.png&#34; alt=&#34;Heatmaps of crashes by day and time period for London Boroughs.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: Heatmaps of crashes by day and time period for London Boroughs.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Looking first by day of week, we see that inner London boroughs generally have fewer than expected crash counts during the weekends and that the reverse is true for outer London boroughs – the first column (Sunday) and last column (Saturday) are purple for the top half of the graphic and green for the bottom half. Unsurprisingly this pattern is most extreme for City of London, London’s financial centre. Looking a little more closely at the outer London boroughs we can also identify where this tendency is stronger: Enfield and Haringey, Newham, Redbridge and Waltham Forest. With some knowledge of London’s social and economic geography we could speculate around these patterns and investigate further as part of our EDA. Notice also that whilst we can interpret the direction of relative differences using colour hue (green:positive | purple:negative) for boroughs containing comparatively few crashes (bottom of graphic), these patterns are de-emphasised by the chi-score residuals which are mapped to colour value (lightness).&lt;/p&gt;
&lt;p&gt;For the period of day heatmap, the pattern is of higher than expected crash frequencies at night for particular inner London boroughs – Westminster, Lambeth, Tower Hamlets and especially Hackney – and lower than expected for particular outer London boroughs – Kingston upon Thames, Bromley, Richmond upon Thames. For the morning peak, City of London, Wandsworth, Southwark and Lambeth contain greater than expected crash frequencies. Again, with knowledge of London’s geography, one could speculate about why this might be the case and investigate further.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;After reading this section, you might have felt that the process of generating modelled expectations extends beyond an initial EDA. I’d argue that the sort of statistical computations described above are necessary as when visually analysing raw values the dominant “signal” is one that is often already obvious and subject to heavy confounding variables. We will return to this in the technical element of the session.&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;Additional aside: The discussion of Risk Ratios (RRs) was reasonably abbreviated. RRs are very interpretable measures of effect size, especially when considered alongside &lt;em&gt;absolute risk&lt;/em&gt; (observed KSI rates in this case). As they are a ratio of ratios, and therefore agnostic to sample size, RRs can nevertheless be unreliable. Two ratios might be compared that have very different sample sizes and no compensation is made for the one that contains more data. Although this was our justification for selecting signed chi-scores, this can be addressed by estimating confidence intervals or using robust Bayesian estimates for RRs (representing RRs as distributions) &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-greenland_bayesian_2006&#34; role=&#34;doc-biblioref&#34;&gt;Greenland 2006&lt;/a&gt;)&lt;/span&gt;.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;supporting-comparison-with-layout-and-colour&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Supporting comparison with layout and colour&lt;/h3&gt;
&lt;p&gt;Fundamental to EDA, and to the data graphics above, is the process of &lt;em&gt;comparison&lt;/em&gt;. Effective data graphics are arranged in such a way as to support and invite relevant comparison. There are three ways in which this can be achieved &lt;span class=&#34;citation&#34;&gt;(via &lt;a href=&#34;#ref-gleicher_visual_2011&#34; role=&#34;doc-biblioref&#34;&gt;Gleicher and Roberts 2011&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Juxtaposition&lt;/em&gt;: Laying the data items to be compared side-by-side, for example the &lt;a href=&#34;../class/03-class/#faceting-by-region-3&#34;&gt;regional comparison of Swing in the small multiple histograms&lt;/a&gt; that appeared in the previous session.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Superposition&lt;/em&gt;: Laying the data items to be compared on top of each other on the same coordinate space, for example the &lt;a href=&#34;../class/02-class/#manipulate-dates-with-lubridate-1&#34;&gt;gendered comparison of trip counts by hour of day and day of week in the line chart&lt;/a&gt; in session 2.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Direct encoding&lt;/em&gt;: Computing some comparison values and encoding those values explicitly, for example the signed chi-scores in Figure &lt;a href=&#34;#fig:borough-freqs-resids&#34;&gt;7&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Throughout this module you will generate data graphics that support comparison using each of these strategies. Table &lt;a href=&#34;#tab:comparison-strategies&#34;&gt;4&lt;/a&gt; is a useful guide for their implementation in &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:comparison-strategies&#34;&gt;Table 4: &lt;/span&gt;Implementing &lt;span class=&#34;citation&#34;&gt;Gleicher and Roberts (&lt;a href=&#34;#ref-gleicher_visual_2011&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;’s three &lt;em&gt;comparison&lt;/em&gt; strategies in &lt;code&gt;ggplot2&lt;/code&gt;.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Strategy
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Function
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Use
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;em&gt;Juxtaposition&lt;/em&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://ggplot2.tidyverse.org/reference/index.html#section-facetting&#34;&gt;&lt;code&gt;facetting&lt;/code&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Create separate plots in rows and/or columns by conditioning on a categorical variable. Each plot has same encoding and coordinate space.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;em&gt;Juxtaposition&lt;/em&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://patchwork.data-imaginist.com/&#34;&gt;&lt;code&gt;patchwork&lt;/code&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Flexibly arrange plots of different data types, encodings and coordinate space.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;em&gt;Superposition&lt;/em&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://ggplot2.tidyverse.org/reference/index.html#section-layers&#34;&gt;&lt;code&gt;geoms&lt;/code&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Layering marks on top of each other. Marks may be of different data types but must share the same coordinate space.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;em&gt;Direct encoding&lt;/em&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No strategy specialised to direct encoding. Often variables cross &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and so &lt;a href=&#34;https://web.natur.cuni.cz/~langhamr/lectures/vtfg1/mapinfo_2/barvy/colors.html#diverging&#34;&gt;diverging schemes&lt;/a&gt;, clearly annotated or symbolised thresholds are important.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As with most visualization design, these strategies require careful implementation. Particularly when juxtaposing views for comparison, it is important to use &lt;em&gt;layout&lt;/em&gt; or &lt;em&gt;arrangement&lt;/em&gt; in a way that best supports comparison. In the practical we will consider some more routine ways in which plots can be composed to support comparison as we analyse pedestrian casualties in the &lt;code&gt;Stats19&lt;/code&gt; dataset. To demonstrate how &lt;em&gt;layout&lt;/em&gt; and &lt;em&gt;colour&lt;/em&gt; can be used for effecting &lt;em&gt;spatial&lt;/em&gt; comparison, we will return to our analysis of pedestrian injuries by vehicle type, and explore variation by London Borough.&lt;/p&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:mosaic-harrow-westminster&#34;&gt;8&lt;/a&gt; Westminster and Harrow, two very different London Boroughs, are compared. Bar size heights vary according to absolute number of recorded injuries by vehicle type and widths according to relative number of injuries recorded on weekday versus weekends. Whilst the modal vehicle type involved in pedestrian injuries is the same for both boroughs (cars), it is far more dominant for crashes recorded in the outer London borough of Harrow. Where cars are involved in crashes in Westminster these occur more frequently (when compared with Harrow) on weekends. Notice that Taxis, Motorcycles, Vans and Bicycles take up a reasonable share of crashes involving pedestrian injuries and that for Bicycles, Vans and Motorcycles these occur more frequently on weekdays.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:mosaic-harrow-westminster&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/mosaic_harrow_westminster.png&#34; alt=&#34;Mosaic plots of vehicle type and injury severity for Westminster and Harrow.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: Mosaic plots of vehicle type and injury severity for Westminster and Harrow.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Given these differences, it may be useful to &lt;em&gt;compare&lt;/em&gt; across all 33 Boroughs in London, &lt;em&gt;juxtaposing&lt;/em&gt; mosaic plots for each borough side-by-side (Figure &lt;a href=&#34;#fig:mosaic-boroughs&#34;&gt;9&lt;/a&gt;). That the mosaic plots are annotated with labels for each vehicle type whose size varies according to frequency helps with interpretation. However, we can use &lt;em&gt;colour hue&lt;/em&gt; to enable selecting and associating of individual vehicle types. This allows very central (City, Westminster, to a lesser extent Camden and Islington) inner (Wandsworth, Southwark, Lambeth) and outer (Croydon, Sutton) London boroughs to be related.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:mosaic-boroughs&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/mosaic_boroughs.png&#34; alt=&#34;Mosaic plots of vehicle type and injury severity for London Boroughs.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: Mosaic plots of vehicle type and injury severity for London Boroughs.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The alphabetical &lt;em&gt;layout&lt;/em&gt; of boroughs helps with look-up-type tasks, but since we have already identified that patterns of pedestrian injuries by vehicle type varies by central-inner-outer London, we could further support this comparison by giving the mosaic plots a spatial arrangement, as in Figure &lt;a href=&#34;#fig:spatial-layout&#34;&gt;10&lt;/a&gt;. This ‘map’ may at first seem alien as you are most likely familiar seeing maps with a precise geographic arrangement. However, when studying spatial patterns in a dataset, such levels of precision are not always needed. Relaxing geography frees up space to introduce richer, more complex designs. In the layout in Figure &lt;a href=&#34;#fig:spatial-layout&#34;&gt;10&lt;/a&gt;, taken from &lt;a href=&#34;https://github.com/aftertheflood/londonsquared&#34;&gt;AfterTheFlood&lt;/a&gt;, each borough is represented as a square of regular size and arranged in its approximate geographic position, allowing the centrsl-inner-outer and London distinctions to be more efficiently made.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:spatial-layout&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/anim_real_grid.gif&#34; alt=&#34;Mosaic plots of vehicle type and injury severity for London Boroughs with spatial arrangement.&#34; width=&#34;90%&#34; /&gt;&lt;img src=&#34;../class/04-class_files/mosaic_boroughs_spatial.png&#34; alt=&#34;Mosaic plots of vehicle type and injury severity for London Boroughs with spatial arrangement.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: Mosaic plots of vehicle type and injury severity for London Boroughs with spatial arrangement.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;techniques&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Techniques&lt;/h2&gt;
&lt;p&gt;The technical element to this session continues in our analysis of STATS19 road crash data. After importing and describing the dataset, you will generate statistical summaries and data graphics for analysing pedestrian casualties. You will focus on visual design choices – colour and layout – that support &lt;em&gt;comparison&lt;/em&gt; of pedestrian casualties, conditioning on numerous categorical variables held in the STATS19 dataset.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the &lt;a href=&#34;../homework/04-homework_files/04-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 04-template.Rmd&lt;/a&gt; file for this session and save it to the &lt;code&gt;reports&lt;/code&gt; folder of your &lt;code&gt;vis-for-gds&lt;/code&gt; project.&lt;/li&gt;
&lt;li&gt;Open your &lt;code&gt;vis-for-gds&lt;/code&gt; project in RStudio and load the template file by clicking &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Open File ...&lt;/code&gt; &amp;gt; &lt;code&gt;reports/04-template.Rmd&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;import&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Import&lt;/h3&gt;
&lt;p&gt;The template file lists the required packages – &lt;code&gt;tidyverse&lt;/code&gt;, &lt;code&gt;sf&lt;/code&gt; and also the &lt;a href=&#34;https://docs.ropensci.org/stats19/&#34;&gt;&lt;code&gt;stats19&lt;/code&gt;&lt;/a&gt; package for downloading and formatting the road crash data. STATS19 is a form used by the police to record road crashes that result in injury. Raw data are released by the Department for Transport as a series of &lt;code&gt;.csv&lt;/code&gt; files spread across numerous &lt;code&gt;.zip&lt;/code&gt; folders. This makes working with the dataset somewhat tedious and behind the &lt;code&gt;stats19&lt;/code&gt; package is some laborious work combining, recoding and relabelling the raw data files.&lt;/p&gt;
&lt;p&gt;STATS19 data are organised into three tables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Accidents&lt;/strong&gt; (or &lt;strong&gt;Crashes&lt;/strong&gt;): Each observation is a recorded road crash with a unique identifier (&lt;code&gt;accident_index&lt;/code&gt;), date (&lt;code&gt;date&lt;/code&gt;) and time (&lt;code&gt;time&lt;/code&gt;), location (&lt;code&gt;longitude&lt;/code&gt;, &lt;code&gt;latitude&lt;/code&gt;). Many other characteristics associated with the crashes are also stored in this table.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Casualties&lt;/strong&gt;: Each observation is a recorded casualty that resulted from a road crash. The &lt;strong&gt;Crashes&lt;/strong&gt; and &lt;strong&gt;Casualties&lt;/strong&gt; data can be linked via the &lt;code&gt;accident_index&lt;/code&gt; variable. As well as the &lt;code&gt;casualty_severity&lt;/code&gt; (&lt;code&gt;Slight&lt;/code&gt;, &lt;code&gt;Serious&lt;/code&gt;, &lt;code&gt;Fatal&lt;/code&gt;), information on casualty demographics and other characteristics is stored in this table.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vehicles&lt;/strong&gt;: Each observation is a vehicle involved in a crash. Again &lt;strong&gt;Vehicles&lt;/strong&gt; can be linked with &lt;strong&gt;Crashes&lt;/strong&gt; and &lt;strong&gt;Casualties&lt;/strong&gt; via the &lt;code&gt;accident_index&lt;/code&gt; variable. As well as the vehicle type and manoeuvre being made, information on driver characteristics is recorded in this table.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In &lt;a href=&#34;../homework/04-homework_files/04-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 04-template.Rmd&lt;/a&gt; is code for downloading each of these tables using the &lt;code&gt;stats19&lt;/code&gt; package’s API. You will collect data on crashes, casualties and vehicles for 2010-2019, so these datasets take a little time to download. For this reason, I suggest that once downloaded you write the data out and read in as &lt;a href=&#34;https://www.fstpackage.org/&#34;&gt;&lt;code&gt;.fst&lt;/code&gt;&lt;/a&gt;. In fact I have made a version of this available for download on a separate repository in case you are having problems with the &lt;code&gt;stats19&lt;/code&gt; download.&lt;/p&gt;
&lt;p&gt;The data analysis that follows is concerned with pedestrian-vehicle crashes. Pedestrian casualties resulting from those crashes are filtered with the purpose of exploring how casualties vary by the socio-economic characteristics of the area in which the crashes took place.&lt;/p&gt;
&lt;p&gt;The first data processing task is to generate a subset of data describing these casualties and the crashes and vehicles to which they are linked. I provide code for generating this subset. The crashes (&lt;code&gt;crashes_all&lt;/code&gt;) and casualties (&lt;code&gt;casualties_all&lt;/code&gt;) tables are joined and pedestrian casualties filtered &lt;code&gt;filter(casualty_type==&#34;Pedestrian&#34;)&lt;/code&gt;. This new table is then joined on vehicles (&lt;code&gt;vehicles_all&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;There are some simplifications and assumptions made here. Some pedestrian crashes involve many vehicles, but ideally for the analysis we need a single vehicle (and vehicle type) to be attributed to each crash. For each casualty, the &lt;em&gt;largest&lt;/em&gt; vehicle involved in the crash is assigned. This is achieved by filtering all vehicle records in &lt;code&gt;vehicles_all&lt;/code&gt; involved in a pedestrian casualty, a &lt;a href=&#34;https://dplyr.tidyverse.org/reference/join.html#join-types&#34;&gt;&lt;code&gt;semi_join&lt;/code&gt;&lt;/a&gt; on the pedestrian casualty table. We then recode the &lt;code&gt;vehicle_type&lt;/code&gt; variable as an ordered factor, group the vehicles table by the crash identifier (&lt;code&gt;accident_index&lt;/code&gt;), and for each crash identify the largest vehicle involved, and then filter these largest vehicles. It is common to have several vehicles of the same type involved in a crash, so a final filter is on a &lt;code&gt;vehicle_reference&lt;/code&gt; variable. This is an integer variable starting from &lt;code&gt;1&lt;/code&gt; to &lt;code&gt;n&lt;/code&gt; assigned to each vehicle involved in the crash. I do not know whether there is an inherent order to this variable, but make the assumption that &lt;code&gt;vehicle_reference=1&lt;/code&gt; is the main vehicle involved in the crash. So &lt;em&gt;single&lt;/em&gt; vehicles are linked to &lt;em&gt;single&lt;/em&gt; casualties based first on the largest vehicle involved and then on &lt;code&gt;vehicle_reference&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A focus for our analysis is around the geo-demographic characteristics of the locations in which crashes occur, and of the pedestrians and vehicles involved in each crash. Collected in the STATS19 dataset is the &lt;a href=&#34;https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019&#34;&gt;Indices of Multiple Deprivation&lt;/a&gt; decile of the small area neighbourhood in which casualties and drivers live. Often IMD is reported at the quintile level (5 rather than 10 quantiles), and aggregating to this level may be useful as we condition on many categorical variables in our EDA. Code for performing this aggregation using an SQL-style &lt;a href=&#34;https://dplyr.tidyverse.org/reference/case_when.html&#34;&gt;&lt;code&gt;case_when&lt;/code&gt;&lt;/a&gt; is in the template.&lt;/p&gt;
&lt;p&gt;The crashes data does not automatically contain information on the IMD for the location of crashes. However, the neighbourhood area (&lt;a href=&#34;https://data.gov.uk/dataset/c481f2d3-91fc-4767-ae10-2efdf6d58996/lower-layer-super-output-areas-lsoas&#34;&gt;LSOA&lt;/a&gt;) at which IMD is measured &lt;em&gt;is&lt;/em&gt; recorded for each crash location. A final bit of data processing code in the template is for downloading the IMD dataset and assigning IMD ranks to quintiles (using an SQL-style &lt;a href=&#34;https://dplyr.tidyverse.org/reference/ranking.html&#34;&gt;&lt;code&gt;ntile()&lt;/code&gt;&lt;/a&gt; function). IMD measures deprivation for LSOAs in England and so when joining this on the crashes dataset we filter &lt;em&gt;only&lt;/em&gt; crashes taking place in England.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explore&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explore&lt;/h3&gt;
&lt;div id=&#34;crash-location-geodemographics&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Crash location geodemographics&lt;/h4&gt;
&lt;p&gt;To start we explore month-on-month frequencies of pedestrian road crashes by IMD of the location in which the crash took place (Figure &lt;a href=&#34;#fig:ped-month-imd&#34;&gt;11&lt;/a&gt;). From this we see that there is some seasonality to pedestrian casualties, that there are more crashes recorded in locations containing higher deprivation – unsurprising as cities tend to contain comparatively high/mid deprivation levels – and that overall recorded pedestrian crash frequencies are reasonably static over the 10-year period.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:ped-month-imd&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/plot-year-imd.png&#34; alt=&#34;Pedestrian casualties by month and IMD.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 11: Pedestrian casualties by month and IMD.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ped_veh %&amp;gt;%
 mutate(year_month=floor_date(date, &amp;quot;month&amp;quot;)) %&amp;gt;%
 group_by(year_month,  crash_quintile) %&amp;gt;%
 summarise(total=n()) %&amp;gt;%
 mutate(
   crash_quintile=factor(
     crash_quintile, levels=c(&amp;quot;5 least deprived&amp;quot;,&amp;quot;4 less deprived&amp;quot;,&amp;quot;3 mid deprived&amp;quot;, &amp;quot;2 more deprived&amp;quot;, &amp;quot;1 most deprived&amp;quot;)
   )
 ) %&amp;gt;%
 ggplot(aes(x=year_month, y=total)) +
 geom_col(aes(fill=crash_quintile),colour=&amp;quot;#8d8d8d&amp;quot;, size=.1 ) +
 scale_fill_brewer(palette = &amp;quot;Blues&amp;quot;, type=&amp;quot;seq&amp;quot;) +
 scale_x_date(date_breaks = &amp;quot;1 year&amp;quot;, date_labels = &amp;quot;%Y-%m&amp;quot;)+
 theme(axis.text.x = element_text(angle=90))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot specification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: Create a new variable of the year and month in which crash occurred (&lt;code&gt;year_month&lt;/code&gt;), using &lt;a href=&#34;https://lubridate.tidyverse.org/&#34;&gt;&lt;code&gt;lubridate&lt;/code&gt;&lt;/a&gt;’s &lt;code&gt;floor_date()&lt;/code&gt; function. Summarise over this and IMD &lt;code&gt;crash_quintile&lt;/code&gt; and make &lt;code&gt;crash_quintile&lt;/code&gt; an ordered factor for charting.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: Bar length varies according to crash counts, so map the date variable (&lt;code&gt;year_month&lt;/code&gt;) to the x-axis and crash counts (&lt;code&gt;total&lt;/code&gt;) to the y-axis. Fill the bars according to &lt;code&gt;crash_quintile&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_col()&lt;/code&gt; for bars.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt;: &lt;code&gt;scale_fill_brewer()&lt;/code&gt; for a perceptually valid sequential scheme for IMD – the darker the colour the higher the level of deprivation. &lt;code&gt;scale_x_date()&lt;/code&gt; for control over the intervals at which we wish to draw labels on the x-axis.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: A consequence of using very light colours for the low-deprivation quintile is that bar height becomes difficult to discern. We therefore draw thin grey outlines around the bars, specified with &lt;code&gt;colour=&#34;#8d8d8d&#34;&lt;/code&gt; and &lt;code&gt;size=.1&lt;/code&gt;. Additionally rotate the date labels 90 degrees to resolve overlaps: &lt;code&gt;axis.text.x=...&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With only five data categories (IMD quintiles) and counts that do not differ wildly per category, the stacked bars are reasonably effective. However, to better support trends within category, it is useful to have a common baseline and juxtapose separate charts for crashes in each IMD category (using &lt;code&gt;facet_wrap&lt;/code&gt;). This improves height comparison &lt;em&gt;between&lt;/em&gt; IMD categories, but also &lt;em&gt;within&lt;/em&gt; category temporal trends are easier to detect – for example, that of &lt;em&gt;slightly&lt;/em&gt; increasing seasonality across all quintiles, especially in the second most deprived quintile. By default &lt;code&gt;facet_wrap()&lt;/code&gt; lays out individual charts across rows and columns. We could instead &lt;em&gt;arrange&lt;/em&gt; the bars on top of each other, in rows, by parameterising the call to &lt;code&gt;facet_wrap()&lt;/code&gt;: &lt;code&gt;facet_wrap(~crash_quintile, ncol=1)&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:ped-year-imd-facet&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/plot-year-imd-facet.png&#34; alt=&#34;Pedestrian casualties by year and IMD.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 12: Pedestrian casualties by year and IMD.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Picking seasonality in these plots is still challenging; so too whether overall crashes are increasing or decreasing. To support seasonal comparison we could consider arranging in two dimensions such that the bars are grouped according to year along the columns and by IMD decile along the rows. This can be achieved with &lt;code&gt;facet_grid()&lt;/code&gt;. To support relative comparison within IMD decile we may choose to give each IMD decile its own (local) y-axis scaling. Rather than comparing absolute heights between IMD deciles we compare instead relative shapes in the month-on-month bars. To further explore whether crash counts are increasing by year, we include the monthly average by year using a horizontal line. This arrangement – Figure &lt;a href=&#34;#fig:ped-monthyear-imd-facet&#34;&gt;13&lt;/a&gt; – better exposes the fact that seasonal variation is greater for crashes that occur in the least deprived quintile. There are many speculative explanations for why this might be the case – many confounding factors, also that this may be a spurious observation due to differences in sample size, that would need to be explored/eliminated as part of an EDA.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:ped-monthyear-imd-facet&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/plot-monthyear-imd.png&#34; alt=&#34;Pedestrian casualties by year and IMD.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 13: Pedestrian casualties by year and IMD.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ped_veh %&amp;gt;%
  mutate(
    year=year(date),
    month=month(date, label=TRUE),
  ) %&amp;gt;%
  group_by(year, month,  crash_quintile) %&amp;gt;%
  summarise(total=n()) %&amp;gt;% ungroup() %&amp;gt;%
  group_by(year, crash_quintile) %&amp;gt;%
  mutate(month_avg=mean(total)) %&amp;gt;% ungroup %&amp;gt;%
  mutate(
    crash_quintile=factor(
      crash_quintile, levels=c(&amp;quot;5 least deprived&amp;quot;,&amp;quot;4 less deprived&amp;quot;,&amp;quot;3 mid deprived&amp;quot;, &amp;quot;2 more deprived&amp;quot;, &amp;quot;1 most deprived&amp;quot;)
    )
  ) %&amp;gt;%
  ggplot(aes(x=month, y=total)) +
  geom_col(width=1, alpha=.6)+
  geom_line(aes(y=month_avg, group=interaction(year, crash_quintile))) +
  scale_x_discrete(breaks=c(&amp;quot;Jan&amp;quot;, &amp;quot;Apr&amp;quot;, &amp;quot;Jul&amp;quot;, &amp;quot;Oct&amp;quot;)) +
  facet_grid(crash_quintile~year, scales=&amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot specification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: Create new variables for the year and month in which crash occurred using &lt;a href=&#34;https://lubridate.tidyverse.org/&#34;&gt;&lt;code&gt;lubridate&lt;/code&gt;&lt;/a&gt; functions. Summarise over this and IMD &lt;code&gt;crash_quintile&lt;/code&gt;. Also generate a &lt;code&gt;monthly_avg&lt;/code&gt; variable, grouping by year and IMD but not month.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: Bar length varies according to crash counts by month, so map the month variable (&lt;code&gt;month&lt;/code&gt;) to the x-axis and crash counts (&lt;code&gt;total&lt;/code&gt;) to the y-axis. The monthly average lines use the same coordinate space as the bars, so x-position remains unchanged, but the y-position is the monthly average variable rather than month count. We need to explicitly say how the lines are grouped – a concatenation of year and IMD, specified via &lt;code&gt;interaction()&lt;/code&gt;. This is within &lt;code&gt;aes()&lt;/code&gt; as it describes &lt;strong&gt;encoding&lt;/strong&gt; of the lines with data, not some additional &lt;strong&gt;setting&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_col()&lt;/code&gt; for bars, &lt;code&gt;geom_line()&lt;/code&gt; for lines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt;: &lt;code&gt;scale_x_discrete&lt;/code&gt; for control over the intervals at which we wish to draw labels on the x-axis. Our x-axis variable is an ordered factor and we supply a vector of the months that we wish to label.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Facets&lt;/strong&gt;: &lt;code&gt;facet_grid()&lt;/code&gt; for faceting on two variables. Parameterise with &lt;code&gt;scales=&#34;free_y&#34;&lt;/code&gt; in order to apply a local scaling and therefore support relative comparison by month within IMD group (at the expense of absolute number between IMD).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: De-emphasise the bars in order to see the monthly average reference lines by applying transparency (&lt;code&gt;alpha&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;crash-location-and-driver-purpose&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Crash location and driver purpose&lt;/h4&gt;
&lt;p&gt;Collected in the STATS19 vehicles table is the reported journey purpose of the driver. Prior to 2011 this field was not available and unfortunately is “not known” for a large proportion (67%) of pedestrian-vehicle crashes. Figure &lt;a href=&#34;#fig:ped-year-imd-facet-purpose&#34;&gt;14&lt;/a&gt; is an updated version of the previous plot, but with bars additionally coloured by journey purpose. Persisting with the analysis of monthly frequencies is useful here as it exposes the fact that an “Other” category has been used inconsistently over time. Further investigation of what this category constitutes is therefore necessary. We would also want to explore in detail the nature of non-reporting – whether there is systematic biases here. Any inferences drawn from Figure &lt;a href=&#34;#fig:ped-year-imd-facet-purpose&#34;&gt;14&lt;/a&gt; are therefore very speculative, but it does appear that the “school run” is reported more frequently in relative terms for crashes occurring in the least deprived quintile – relatively more blue (“school run”) and less green (“commute”). Again, there are speculative explanations for this that could be investigated as part of an EDA and an immediate task might be to update the plot to make the bars &lt;em&gt;standardised&lt;/em&gt; (equal height).&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:ped-year-imd-facet-purpose&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/plot-year-imd-facet-purpose.png&#34; alt=&#34;Pedestrian casualties by year, IMD and driver purpose.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 14: Pedestrian casualties by year, IMD and driver purpose.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;crash-location-and-driver-casualty-geodemographics&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Crash location and driver-casualty geodemographics&lt;/h4&gt;
&lt;p&gt;For the final part of our EDA, we consider the geodemographics of the individuals involved in crashes. In the casualties and vehicles data geodemographic characteristics are recorded – the IMD decile of the neighbourhood in which the driver and pedestrian lives. Geodemographic variables need to be treated cautiously (e.g. &lt;a href=&#34;https://en.wikipedia.org/wiki/Ecological_fallacy&#34;&gt;ecological fallacy&lt;/a&gt;), but it may be instructive to explore how the characteristics of drivers and pedestrians &lt;em&gt;co-vary&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To do this we can construct contingency tables of the joint frequency of each permutation of driver-pedestrian IMD group co-occurring. For consistency with the earlier analysis, IMD is aggregated to quintiles, so our contingency table contains 25 cells – 5x5 driver-pedestrian combinations. It may be useful to display these counts in a way that enables linearity to be inferred. Firstly this is because IMD is an ordered category variable, making it possible to explore linear association in ordered ranks. Secondly, we’d expect similarity in the IMD status of drivers-pedestrians – that is, drivers crashing with pedestrians within the neighbourhoods, or nearby neighbourhoods, in which they both live.&lt;/p&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:imd-driver-cas-overall&#34;&gt;15&lt;/a&gt; a heatmap matrix shows this association. Cells in the matrix are ordered along the x-axis according to the IMD quintile of the casualty (pedestrian) and along the y-axis according to the IMD quintile of the driver. The darker blues in the diagonals demonstrate that, as expected, it is more common for drivers-pedestrians involved in crashes to share the same geodemographic characteristics. That colour concentrates in the bottom left is therefore also to be expected. Previously we established that a greater number of pedestrian-vehicle crashes occur in high deprivation neighbourhoods and given there is an association between driver-casualty geodemographics, the largest cell frequencies will be concentrated in the bottom left of the matrix.&lt;/p&gt;
&lt;p&gt;This trend can be further investigated through &lt;em&gt;direct encoding&lt;/em&gt; - constructing a modelled expectation and representing deviation from expectation – the right heatmap in the top row of Figure &lt;a href=&#34;#fig:imd-driver-cas-overall&#34;&gt;15&lt;/a&gt;. Expected values are generated based on the assumption that crash frequencies distribute by driver IMD class independently of the IMD class of the pedestrian injured. I have also plotted raw expected values and the column and row marginals from which the residuals are derived. This demonstrates how expected values are spread across cells in the contingency table based on the overall size of each IMD class. Signed chi-scores are mapped to a diverging colour scale – green for residuals that are positive (cell counts are greater than expected), purple for residuals that are negative (cell counts are less than expected).&lt;/p&gt;
&lt;p&gt;The observed-versus-expected plot highlights that the largest positive residuals are in and around the diagonals and the largest negative residuals are those furthest from the diagonals: we see many more crash frequencies between drivers and pedestrians living in the same or similar IMD quintiles and fewer between those in different quintiles. That the bottom left cell – high-deprivation-driver + high-deprivation-pedestrian – is dark green can be understood when remembering that signed chi-scores emphasise effect sizes that are large in &lt;em&gt;absolute&lt;/em&gt; as well as relative terms. Not only is there an association between the geodemographics of drivers and casualties, but larger crash counts are recorded in locations containing the highest deprivation and so residuals here are large. The largest positive residuals (the darkest green) are nevertheless recorded in the top right of the matrix – and this is more surprising. Against an expectation of no association between the geodemographic characteristics of drivers and pedestrians involved in road crashes, there is a particularly high number of crashes between drivers and pedestrians living in neighbourhoods containing the lowest deprivation. An alternative phrasing: the association between the geodemographics of drivers and pedestrians is greater for those living in the lowest deprivation quintiles.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:imd-driver-cas-overall&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/imd-driver-cas-overall.png&#34; alt=&#34;IMD of driver-casualty.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 15: IMD of driver-casualty.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data staging
plot_data &amp;lt;- ped_veh %&amp;gt;%
  # Filter out crashes for which the IMD of the driver and pedestrian is not missing.
  filter(
    !is.na(casualty_imd_decile), !is.na(driver_imd_decile),
    casualty_imd_decile!=&amp;quot;Data missing or out of range&amp;quot;,
    driver_imd_decile!=&amp;quot;Data missing or out of range&amp;quot;, !is.na(crash_quintile)) %&amp;gt;%
  # Calculate the grand total -- used for deriving singed-chi-scores.
  mutate(grand_total=n()) %&amp;gt;%
  # Calculate row margins -- total crashes by IMD of driver.
  group_by(driver_imd_quintile) %&amp;gt;%
  mutate(row_total=n()) %&amp;gt;% ungroup %&amp;gt;%
  # Calculate col margins -- total crashes by IMD of pedestrian.
  group_by(casualty_imd_quintile) %&amp;gt;%
  mutate(col_total=n()) %&amp;gt;% ungroup %&amp;gt;%
  # Summarise over cells of contingency table -- observed, expected and residuals.
  group_by(casualty_imd_quintile, driver_imd_quintile) %&amp;gt;%
  summarise(
    observed=n(),
    row_total=first(row_total),
    col_total=first(col_total),
    grand_total=first(grand_total),
    expected=(row_total*col_total)/grand_total,
    resid=(observed-expected)/sqrt(expected),
    max_resid=max(abs(resid))
  ) %&amp;gt;% ungroup

# Plot observed
plot_data %&amp;gt;%
  ggplot(aes(x=casualty_imd_quintile, y=driver_imd_quintile)) +
  geom_tile(aes(fill=observed), colour=&amp;quot;#707070&amp;quot;, size=.2) +
  scale_fill_distiller(palette=&amp;quot;Blues&amp;quot;, direction=1)

# Plot signed chi-score
plot_data %&amp;gt;%
  ggplot(aes(x=casualty_imd_quintile, y=driver_imd_quintile)) +
  geom_tile(aes(fill=resid), colour=&amp;quot;#707070&amp;quot;, size=.2) +
  scale_fill_distiller(
    palette=&amp;quot;PRGn&amp;quot;, direction=1, limits=c(-max(plot_data$max_resid), max(plot_data$max_resid)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot specification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: As it is to be re-used (and reasonably lengthy) I have created a staged data frame for charting (&lt;code&gt;plot_data&lt;/code&gt;). Hopefully you can see how various &lt;code&gt;dplyr&lt;/code&gt; calls are used to generate row and column marginals of the contingency table and that the calculation for the signed chi-scores is &lt;a href=&#34;../#deriving-effect-sizes-from-contingency-tables-1&#34;&gt;as described earlier&lt;/a&gt;. You might have noticed that I also calculate a variable storing the absolute maximum value of these scores (&lt;code&gt;max_resid&lt;/code&gt;). This is to help centre the diverging colour scheme in the residuals plot.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: For both heatmaps, the IMD class of pedestrians is mapped to the x-axis and drivers to the y-axis. Tiles are coloured according to observed counts (&lt;code&gt;fill=observed&lt;/code&gt;) or signed chi-scores (&lt;code&gt;fill=resid&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_tile()&lt;/code&gt; for cells of the heatmap.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt;: &lt;code&gt;scale_fill_distiller()&lt;/code&gt; for continuous &lt;a href=&#34;https://colorbrewer2.org/#type=sequential&amp;amp;scheme=BuGn&amp;amp;n=3&#34;&gt;colorbrewer&lt;/a&gt; colour schemes. Note that for the diverging scheme (palette=“PRGn”) I set &lt;code&gt;limits&lt;/code&gt; based on the maximum absolute value of the residuals. Try deleting the limits call to see what happens to the plot without this.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Note that I apply a &lt;code&gt;colour&lt;/code&gt; to &lt;code&gt;geom_tile()&lt;/code&gt; – this draws a border around each cell.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;detailed-analysis-of-crash-location-and-driver-casualty-geodemographics&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Detailed analysis of crash location and driver-casualty geodemographics&lt;/h4&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    I had intended to finish the session here. However, there is an obvious confounding factor that the analysis above does not consider. I got a little carried away with investigating this and the pedestrian-driver demographics theme and, using &lt;em&gt;layout&lt;/em&gt; and &lt;em&gt;direct encoding&lt;/em&gt;, generated data-driven hypotheses/expectations to be explored in more detail. I do not provide code for the analysis below – it extends beyond what I anticipated for this session on EDA.
It is &lt;strong&gt;&lt;em&gt;optional&lt;/em&gt;&lt;/strong&gt; for the interested reader.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;!-- This theme can be further explored by *juxtaposing* separate plots based on the IMD quintile of the crash location. The plot in the top row shows observed values and the residuals are based on a restructured contingency table where the rows are IMD driver-pedestrian pairs -- 25 x rows -- and the columns are the IMD class of the crash location --  5 x columns. The assumption is that crashes by IMD casualty-driver pair distribute independently of crash location. This takes a little to get your head around, but is demonstrated in the plots of expected values and row and column marginals (Figure \@ref(fig:imd-driver-cas-overall)).

Against this expectation our signed chi-scores show obvious dependency/association between the demographics of drivers and pedestrians involved in crashes, and the locations in which those crashes took place: the darkest greens are in the diagonals.   There is also structure outside of these diagonals. Cells surrounding the diagonals are generally more similar in colour -- light green for the mid deprivation crash locations, again reinforcing the association between geodemographics of pedestrian-driver- and crash- location.  *Vertical* blocks of green can also be identified. Remembering that pedestrian IMD class is mapped to the x-axis, this indicates higher crash counts where pedestrians are involved in crashes that take place in the same IMD class in which they live, even where the driver lives in a different IMD class.  That  *horizontal* green blocks that are not so obvious suggest that the association between the IMD class of the crash location and the home location of the person involved is greater for pedestrians than drivers. This is again to be expected as, relative to pedestrians, it is more likely that drivers will be travelling further distances and involved in crashes in neighbourhoods geodemographically different to those in which they live. The strongest vertical block of green is in crashes occurring in the most deprived quintile (left-most column): larger than expected instances of pedestrians living in the most derived IMD class being hit by drivers of a different IMD class. Also the smooth and consistent variation between the bottom-left and top-right corner of the plot for crashes occurring in the least deprived quintile may suggest that the association between geodemographic characteristics (of driver-pedestrian-location) is strongest here.

A heavy confounding factor in the plots above is the *location* in which the crashes take place. To demonstrate this, Figure  \@ref(fig:imd-driver-cas-crash-imd) plots relative frequencies for each pedestrian-driver IMD combination separately depending on the IMD quintile of the crash location.   A local scaling is used -- cell counts are scaled using the maximum value within the containing matrix (so the maximum count within each IMD crash quintile). Eyeballing this graphic, we see the heavy association between geodemographics for crashes occurring in the least deprived quintile and elsewhere there is slightly more &#34;mixing&#34;. For crashes occurring in the most deprived quintile few pedestrians living outside of the  most deprived quintile are involved in crashes that occur in that quintile.


&lt;!-- Whilst these plots are successful at exposing the association between the geodemographics of the *drivers*, *pedestrians* and *locations* in which they took place, this very obvious association, and the fact that crash counts are concentrated in the most deprived quintile, is the dominant &#34;signal&#34;.  This can be seen in the expected values for the contingency table in Figure \@ref(fig:imd-driver-cas-crash). Updating our heatmaps of observed values with a local scaling is another way of demonstrating the strong association between the IMD of *drivers*, *pedestrians* and  crash *locations*.  In Figure \@ref(fig:imd-driver-cas-crash-rescaled) cell counts are scaled using the maximum value within the containing matrix (so the maximum count within each IMD crash quintile). Eyeballing this graphic, we see that comparatively few pedestrians living outside of the  most deprived quintile are involved in crashes that occur in that quintile, again the graphic exposes heavy association between geodemographics for crashes occurring in the least deprived quintile and elsewhere there is slightly more &#34;mixing&#34;. --&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:imd-driver-cas-crash-rescaled&#34;&gt;16&lt;/a&gt; shows frequencies by driver-pedestrian IMD group faceted on the IMD class of the &lt;em&gt;location&lt;/em&gt; in which the road crash occurred. A local scaling is applied by IMD class of location. It demonstrates an obvious confounding pattern – that pedestrians and drivers are far more likely to be involved in crashes in locations with same geodemographic characteristics as those in which they live. Ideally we want to update our expectations to model explicitly for some of this pattern. We have established that an association between the IMD class of &lt;em&gt;drivers&lt;/em&gt;, &lt;em&gt;pedestrians&lt;/em&gt; and crash &lt;em&gt;locations&lt;/em&gt; exists. The more interesting question is whether this is association is stronger for certain driver-pedestrian-location combinations than others: that is, net of the dominant pattern, in which cells are there greater or fewer crash counts.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:imd-driver-cas-crash-rescaled&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/imd-driver-cas-crash-local.png&#34; alt=&#34;IMD driver-casualty.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 16: IMD driver-casualty.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is not a straightforward task as the dependency is baked-in to our contingency table. Essentially we are interested in how crash counts vary depending on &lt;em&gt;geodemographic distance&lt;/em&gt; – of drivers, pedestrians, crash locations. I calculated a new variable measuring this distance directly – the euclidian distance between the geodemographics of the driver and pedestrian from the location of each crash, treating the &lt;code&gt;ordinal&lt;/code&gt; IMD class variable as a &lt;code&gt;continuous&lt;/code&gt; variable ranging from 1-5. To illustrate this, in Figure &lt;a href=&#34;#fig:imd-driver-cas-crash-distance&#34;&gt;17&lt;/a&gt; I’ve mapped this distance variable to each cell of the heatmap matrix.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:imd-driver-cas-crash-distance&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/imd-geodemog-dist.png&#34; alt=&#34;IMD driver-pedestrian-location geodemographic distance.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 17: IMD driver-pedestrian-location geodemographic distance.
&lt;/p&gt;
&lt;/div&gt;
&lt;!-- I then calculated row and column marginals from a new contingency table constructed on this variable -- the rows are different values of &#34;geodemographic distance&#34; and the columns are the different categories IMD road crash location -- and used this to spared expectation across cells in our original contingency table -- as demonstrated in Figure \@ref(fig:imd-driver-cas-crash-dist-full). The expectation is therefore that: road crash counts by &#34;geodemographic distance&#34; distribute independently of IMD crash location. A key pattern exposed in the residuals map here is that in the left-most plot for the most deprived IMD crash quintile: much higher than expected instances of pedestrians living in the most deprived quintile, being hit in neighbourhoods in the most deprived quintile and by drivers particularly living in the less deprived quintiles. --&gt;
&lt;p&gt;I then generated counts of pedestrian casualties by demographic distance, and from this, cell probabilities/likelihoods for each unique position in the matrix in Figure &lt;a href=&#34;#fig:imd-driver-cas-crash-distance&#34;&gt;17&lt;/a&gt;. The probabilities assume independence between crash location: that is, the relative likelihood of a crash occurring given a cell’s geodemographic distance does not vary by IMD class of the crash location.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:imd-driver-cas-crash-dist-full&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/04-class_files/imd-driver-cas-crash-dist-full.png&#34; alt=&#34;IMD driver-pedestrian-location with modelled expectations based on geodemographic distance.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 18: IMD driver-pedestrian-location with modelled expectations based on geodemographic distance.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Salient in Figure &lt;a href=&#34;#fig:imd-driver-cas-crash-dist-full&#34;&gt;18&lt;/a&gt; is the vertical block of green in the left column of the plot for injuries occurring in the most deprived quintile. Remembering that pedestrian IMD class is mapped to the x-axis, this indicates higher than expected injury counts where pedestrians are involved in crashes that take place in the same IMD class in which they live, even where the driver lives in a different IMD class. Pedestrian injuries occurring in the most deprived quintile appear to be experienced disproportionately by those living in that quintile. In the second-most deprived quintile are green blocks in the left corresponding with pedestrians living in that quintile, but especially the most deprived quintile (darkest green), being hit by drivers living in the &lt;em&gt;most&lt;/em&gt; deprived quintile. In the mid deprived quintile, drivers and pedestrians living in the most deprived quintile are again overrepresented. Glancing in each of the the matrices, cells towards the right and top are generally closer to purple: pedestrians living in the less deprived quintile groups are less likely to be injured in crashes, and drivers living in those less deprived quintiles are less likely to contribute to those pedestrian injuries, even after controlling geodemographic distance. And the corollary is that those pedestrians and drivers living in the more deprived quintiles are more likely to be both the injured pedestrians, and the drivers contributing to those injuries.&lt;/p&gt;
&lt;p&gt;There is much to unpick here, and I’m not sure about the validity of spreading estimated counts from a model based on the derived “geodemographic distance” variable. The upshot from this exploratory analysis is, like many health issues, pedestrian road injuries have a heavy socio-economic element and so is worthy of formal investigation. Hopefully from this you get a sense of the common workflow for EDA:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Expose pattern&lt;/li&gt;
&lt;li&gt;Model an expectation derived from pattern&lt;/li&gt;
&lt;li&gt;Show deviation from expectation&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- Generating modelled expectations is necessary as the dominant “signal” when visually analysing raw values is one that is often already obvious and subject to heavy confounding variables

* larger than expected instances of pedestrians living in the most derived IMD class being hit by drivers of a different IMD class (salient vertical block in left column);
* larger than expected instances of pedestrians living in other IMD classes being hit by drivers who live in the most deprived IMD class (*very* light green horizontal block in bottom row)
* fewer than expected instances of pedestrians living in less-deprived IMD classes being hit by drivers who live in other less deprived IMD classes (purple block of the matrix).



, suggesting that crashes occurring between drivers and pedestrians, and in locations, of *similar* (not only identical) IMD class. Again this is to be expected as neighbourhoods of similar demographic characteristics tend to concentrate geographically.

 emphasise the darkest blue again in the diagonals; crashes are concentrated between drivers and pedestrians of the same IMD class and occur in crash locations of the same IMD class. the same IMD position in x- and y- of the IMD of the crash location. The structure of &#34;leakage&#34; away from these diagonals is noteworthy.



 this patters is most obvious for crashes that take place in the most deprived IMD quintile:


 pedestrians and the location in which the crash took place


 Cells surrounding the diagonals are generally darker, suggesting that crashes occurring between drivers and pedestrians, and in locations, of *similar* (not only identical) IMD class. Again this is to be expected as neighbourhoods of similar demographic characteristics tend to concentrate geographically. *Vertical* blocks of green can also be identified. Remembering that pedestrian IMD class is mapped to the x-axis, this indicates higher crash counts where pedestrians are involved crashes that take place in the same IMD class in which they live, even where the driver involved lives in a different IMD class.

 However, notice that the green vertical and horizontal blocks  distribute differently between the IMD classes of crash location.  The vertical green block is most salient for crashes occurring in the most deprived quintile: larger than expected instances of pedestrians living in the most derived IMD class being hit by drivers of a different IMD class (salient vertical block in left column). We would probably expect this to be quite a dominant pattern given the largest crash frequencies occur in the most deprived quintile (the col marginals graphic). Although we see the same pattern for crashes in the lest deprive quintile


 * fewer than expected instances of pedestrians living in less-deprived IMD class, being hit in the most deprived IMD class and by drivers who live in other less deprived IMD classes (purple block of the matrix).


  The location of the green *vertical* blocks varies consistently on crash quintile. These are pedestrians injured in crashes that occurs in neighbourhoods of the same IMD class to which they live. The darkest green is in the cell diagonal -- as already established and by drivers that live in same (cell diagonal) or different (cells above or below) neighbourhood classes.





In the top row of Figure \@ref(fig:imd-driver-cas-crash), a local scaling is applied to support relative comparison across these plots -- cell counts are scaled using the maximum value within the containing matrix (so the maximum count within each IMD crash quintile).

This additional structure can be made more discriminating through *direct encoding*. Here signed chi-square scores a computed under a slightly different assumption: that cell frequencies distribute amongst the driver-casualty pairings independently of the IMD quintile of the location in which they occurred. This formulation is instructive as it de-emphasises the dominant pattern that has already been established (the association between IMD of driver-casualty) and  emphasises variation that we are interested in -- differences in driver-casualty crash count pairings due to crash location.

This graphic exposes interesting *vertical* and *horizontal* blocks. The location of the green *vertical* blocks varies consistently on crash quintile. These are pedestrians injured in crashes that occurs in neighbourhoods of the same IMD class to which they live. The darkest green is in the cell diagonal -- as already established and by drivers that live in same (cell diagonal) or different (cells above or below) neighbourhood classes.
* The green *horizontal* blocks are less salient is instructive. These correspond to drivers involved in a crash that occurs in the neighbourhood with same IMD class to which they live and injuring pedestrians that live in the same (cell diagonal) or different neighbourhood classes (cells left or right). Relative to pedestrians, it is more common for drivers to be involved in crashes that occur in neighbourhoods that are *different* to those in which they live. This tendency is greatest for crashes that occur

* Single graphic, annotated. --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Exploratory data analysis (EDA) is an &lt;em&gt;approach&lt;/em&gt; to analysis that aims to expand knowledge and understanding of a dataset. The idea is to explore structure, and data-driven hypotheses, by quickly generating many often throwaway statistical and graphical summaries. In this session we discussed chart &lt;em&gt;idioms&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;Munzner 2014&lt;/a&gt;)&lt;/span&gt; for exposing distributions and relationships in a dataset, depending on data type. We also showed that EDA is not model-free. Data graphics help us to see dominant patterns and from here formulate expectations that are to be modelled. Different from so-called &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_analysis#Exploratory_and_confirmatory_approaches&#34;&gt;&lt;em&gt;confirmatory data analysis&lt;/em&gt;&lt;/a&gt;, however, in an EDA the goal of model-building is not “to identify &lt;em&gt;whether&lt;/em&gt; the model fits or not […] but rather to &lt;em&gt;understand&lt;/em&gt; in what ways the fitted model departs from the data” &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-gelman_exploratory_2004&#34; role=&#34;doc-biblioref&#34;&gt;Gelman 2004&lt;/a&gt;)&lt;/span&gt;. We covered visualization approaches to supporting &lt;em&gt;comparison&lt;/em&gt; between data and expectation using &lt;em&gt;juxtaposition&lt;/em&gt;, &lt;em&gt;superimposition&lt;/em&gt; and/or &lt;em&gt;direct encoding&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gleicher_visual_2011&#34; role=&#34;doc-biblioref&#34;&gt;Gleicher and Roberts 2011&lt;/a&gt;)&lt;/span&gt;. The session did not provide an exhaustive survey of EDA approaches, and certainly not an exhaustive set of chart idioms for exposing distributions and relationships. By locating the session closely in the STATS19 dataset, we learnt a workflow for EDA that is common to most effective data analysis (and communication) activity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-correll_regression_2017&#34; class=&#34;csl-entry&#34;&gt;
Correll, M., and J. Heer. 2017a. &lt;span&gt;“Regression by Eye: Estimating Trends in Bivariate Visualizations.”&lt;/span&gt; In &lt;em&gt;ACM Human Factors in Computing Systems (CHI)&lt;/em&gt;. &lt;a href=&#34;http://idl.cs.washington.edu/papers/regression-by-eye&#34;&gt;http://idl.cs.washington.edu/papers/regression-by-eye&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-correl_surprise_2017&#34; class=&#34;csl-entry&#34;&gt;
———. 2017b. &lt;span&gt;“Surprise! Bayesian Weighting for de-Biasing Thematic Maps.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Visualization &amp;amp; Computer Graphics&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-friendly_mosaic_1992&#34; class=&#34;csl-entry&#34;&gt;
Friendly, M. 1992. &lt;span&gt;“Mosaic Displays for Loglinear Models.”&lt;/span&gt; In &lt;em&gt;ASA, Proceedings of the Statistical Graphics Section&lt;/em&gt;, 61–68.
&lt;/div&gt;
&lt;div id=&#34;ref-gelman_exploratory_2004&#34; class=&#34;csl-entry&#34;&gt;
Gelman, A. 2004. &lt;span&gt;“Exploratory Data Analysis for Complex Models.”&lt;/span&gt; &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 13 (4). Taylor &amp;amp; Francis: 755–79. doi:&lt;a href=&#34;https://doi.org/10.1198/106186004X11435&#34;&gt;10.1198/106186004X11435&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-gleicher_visual_2011&#34; class=&#34;csl-entry&#34;&gt;
Gleicher, Albers, M., and J. Roberts. 2011. &lt;span&gt;“Visual Comparison for Information Visualization. Information Visualization.”&lt;/span&gt; &lt;em&gt;Information Visualization&lt;/em&gt; 10 (4): 289–309.
&lt;/div&gt;
&lt;div id=&#34;ref-greenland_bayesian_2006&#34; class=&#34;csl-entry&#34;&gt;
Greenland, Sander. 2006. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Bayesian perspectives for epidemiological research: I. Foundations and basic methods&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;International Journal of Epidemiology&lt;/em&gt; 35 (3): 765–75. doi:&lt;a href=&#34;https://doi.org/10.1093/ije/dyi312&#34;&gt;10.1093/ije/dyi312&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-harrison_ranking_2014&#34; class=&#34;csl-entry&#34;&gt;
Harrison, L., F. Yang, S. Franconeri, and R. Chang. 2014. &lt;span&gt;“Ranking Visualizations of Correlation Using &lt;span&gt;W&lt;/span&gt;eber’s &lt;span&gt;L&lt;/span&gt;aw.”&lt;/span&gt; &lt;em&gt;IEEE Conference on Information Visualization (InfoVis)&lt;/em&gt; 20: 1943–52.
&lt;/div&gt;
&lt;div id=&#34;ref-kay_beyond_2016&#34; class=&#34;csl-entry&#34;&gt;
Kay, M., and J. Heer. 2016. &lt;span&gt;“Beyond &lt;span&gt;W&lt;/span&gt;eber’s &lt;span&gt;L&lt;/span&gt;aw: A Second Look at Ranking Visualizations of Correlation.”&lt;/span&gt; &lt;em&gt;IEEE Trans. Visualization &amp;amp; Comp. Graphics (InfoVis)&lt;/em&gt; 22: 469–78.
&lt;/div&gt;
&lt;div id=&#34;ref-mcgill_variations_1978&#34; class=&#34;csl-entry&#34;&gt;
McGill, Tukey, R., and W. A. Larsen. 1978. &lt;span&gt;“Variations of Box Plots.”&lt;/span&gt; &lt;em&gt;The American Statistician&lt;/em&gt; 32: 12–16.
&lt;/div&gt;
&lt;div id=&#34;ref-munzner_visualization_2014&#34; class=&#34;csl-entry&#34;&gt;
Munzner, T. 2014. &lt;em&gt;Visualization Analysis and Design&lt;/em&gt;. AK Peters Visualization Series. Boca Raton, FL: CRC Press.
&lt;/div&gt;
&lt;div id=&#34;ref-rensink_perception_2010&#34; class=&#34;csl-entry&#34;&gt;
Rensink, R., and G. Baldridge. 2010. &lt;span&gt;“The Perception of Correlation in Scatterplots.”&lt;/span&gt; &lt;em&gt;Computer Graphics Forum&lt;/em&gt; 29: 1203–10.
&lt;/div&gt;
&lt;div id=&#34;ref-tukey_exploratory_1977&#34; class=&#34;csl-entry&#34;&gt;
Tukey, J. W. 1977. &lt;em&gt;Exploratory Data Analysis&lt;/em&gt;. Reading, MA, USA: Addison-Wesley.
&lt;/div&gt;
&lt;div id=&#34;ref-visalingam_signed_1981&#34; class=&#34;csl-entry&#34;&gt;
Visalingam, M. 1981. &lt;span&gt;“The Signed Chi-Score Measure for the Classification and Mapping of Plychotomous Data.”&lt;/span&gt; &lt;em&gt;The Cartographic Journal&lt;/em&gt; 18 (1): 32–43.
&lt;/div&gt;
&lt;div id=&#34;ref-wickham_r_2017&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., and G. Grolemund. 2017. &lt;em&gt;&lt;span&gt;R&lt;/span&gt; for Data Science: Import, Tidy, Transform, Visualize, and Model Data&lt;/em&gt;. Sebastopol, California: O’Reilly Media.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualization fundamentals: Codify, map, evaluate</title>
      <link>/class/03-class/</link>
      <pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate>
      <guid>/class/03-class/</guid>
      <description>
&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;script src=&#34;../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;../rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;../rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;h2&gt;Contents&lt;/h2&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concepts&#34;&gt;Concepts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#characteristics-of-effective-data-graphics&#34;&gt;Characteristics of effective data graphics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grammar-of-graphics&#34;&gt;Grammar of Graphics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#marks-and-visual-channels&#34;&gt;Marks and visual channels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluating-designs&#34;&gt;Evaluating designs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#symbolisation&#34;&gt;Symbolisation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#checking-perceptual-rankings&#34;&gt;Checking perceptual rankings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#colour&#34;&gt;Colour&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#techniques&#34;&gt;Techniques&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#import&#34;&gt;Import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summarise&#34;&gt;Summarise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-distributions&#34;&gt;Plot distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-ranksmagnitudes&#34;&gt;Plot ranks/magnitudes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-relationships&#34;&gt;Plot relationships&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-geography&#34;&gt;Plot geography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;knowledge&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Recognise&lt;/strong&gt; the characteristics of effective data graphics.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Understand&lt;/strong&gt; that there is a &lt;strong&gt;grammar&lt;/strong&gt; of graphics, and that this grammar underpins modern visualization toolkits (&lt;code&gt;ggplot&lt;/code&gt;, vega-lite and Tableau).&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Be &lt;strong&gt;aware&lt;/strong&gt; of the vocabulary used by these toolkits – that of encoding data through visual channels.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Be able to &lt;strong&gt;select&lt;/strong&gt; appropriate &lt;strong&gt;visual channels&lt;/strong&gt; given a data item’s measurement type.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Appreciate how &lt;strong&gt;visual channels&lt;/strong&gt; and evidence of their &lt;strong&gt;encoding effectiveness&lt;/strong&gt; can be used to evaluate data graphics.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;practical skills&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Write&lt;/strong&gt; &lt;code&gt;ggplot2&lt;/code&gt; specifications that represent data using marks (&lt;code&gt;geoms&lt;/code&gt;) and encoding channels (&lt;code&gt;aesthetics&lt;/code&gt; – &lt;strong&gt;colour&lt;/strong&gt; and &lt;strong&gt;position&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This session outlines the fundamentals of visualization design. It offers a position on what effective data graphics should do, before discussing in detail the processes that take place when creating data graphics. You will learn that there is a framework – a vocabulary and grammar – for supporting this process which, combined with established knowledge around visual perception, can be used to &lt;em&gt;describe&lt;/em&gt;, &lt;em&gt;evaluate&lt;/em&gt; and &lt;em&gt;create&lt;/em&gt; effective data graphics. Talking about a vocabulary and grammar of data and graphics may sound alien and abstract, the preserve of Computer Scientists. However, through an analysis of 2019 General Election results data we will demonstrate that these concepts underpin most visual data analysis.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    Watch &lt;a href=&#34;https://www.cs.utah.edu/~miriah/&#34;&gt;Miriah Meyer’s&lt;/a&gt; TEDx talk, &lt;em&gt;Information Visualization for Scientific Discovery&lt;/em&gt;, which provides a nice introduction to many of the concepts covered in the session.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;https://www.youtube.com/embed/Sua0xDCf8MA&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concepts&lt;/h2&gt;
&lt;div id=&#34;characteristics-of-effective-data-graphics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Characteristics of effective data graphics&lt;/h3&gt;
&lt;p&gt;Data graphics take numerous forms and are used in many different ways by scientists, journalists, designers and many more. Whilst the intentions of those producing data graphics varies, those that are &lt;em&gt;effective&lt;/em&gt; generally have the following characteristics:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Represent complex datasets graphically to expose structure, connections and comparisons that could not be achieved easily via other means.&lt;/li&gt;
&lt;li&gt;Are data rich: present many numbers in a small space.&lt;/li&gt;
&lt;li&gt;Reveal patterns at several levels of detail: from broad overview to fine structure.&lt;/li&gt;
&lt;li&gt;Have elegance – emphasise dimensions of a dataset without extraneous details.&lt;/li&gt;
&lt;li&gt;Generate an aesthetic response that encourages people to engage with the data or question.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    Considering these characteristics, take a look at the data graphics below, which present an analysis of the 2016 US Presidential Election. Use the links to read the full stories and accompanying data analyses.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:wp-map&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/trump_maps.png&#34; alt=&#34;Maps of 2016 US presidential election results. Left - two-colour choropleth in [Medium](https://pensivepost.com/understanding-rural-america-d9695a6b3516). Right - information-rich data graphic in [The Washington Post](https://www.washingtonpost.com/graphics/politics/2016-election/election-results-from-coast-to-coast/).&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Maps of 2016 US presidential election results. Left - two-colour choropleth in &lt;a href=&#34;https://pensivepost.com/understanding-rural-america-d9695a6b3516&#34;&gt;Medium&lt;/a&gt;. Right - information-rich data graphic in &lt;a href=&#34;https://www.washingtonpost.com/graphics/politics/2016-election/election-results-from-coast-to-coast/&#34;&gt;The Washington Post&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Both maps use 2016 county-level results data, but the &lt;a href=&#34;https://www.washingtonpost.com/graphics/politics/2016-election/election-results-from-coast-to-coast/&#34;&gt;The Washington Post&lt;/a&gt; graphic encodes many more data items than the Medium post (see Table &lt;a href=&#34;#tab:wp-data&#34;&gt;1&lt;/a&gt; below).&lt;/p&gt;
&lt;p&gt;It is not simply the data density that makes the Washington Post graphic successful. The authors usefully incorporate annotations and transformations in order to support comparison and emphasise structure. By varying the height of triangles according to the number of votes cast, the thickness according to whether or not the result for Trump/Clinton was a landslide and rotating the scrollable map 90 degrees, the very obvious differences between metropolitan, densely populated coastal counties that voted emphatically for Clinton and the vast number of suburban, provincial town and rural counties (everywhere else) that voted Trump, are exposed.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: replacing previous import &amp;#39;lifecycle::last_warnings&amp;#39; by
## &amp;#39;rlang::last_warnings&amp;#39; when loading &amp;#39;pillar&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:wp-data&#34;&gt;Table 1: &lt;/span&gt;Data items encoded in the Washington Post and Medium articles.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Data item
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Data measurement level
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Medium
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Washington Post
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
county location
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;Ratio&lt;/code&gt; (&lt;code&gt;cyclic&lt;/code&gt;)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;www.google.com&#34;&gt;&lt;svg aria-hidden=&#34;true&#34; role=&#34;img&#34; viewBox=&#34;0 0 512 512&#34; style=&#34;height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;&#34;&gt;&lt;path d=&#34;M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206 0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204 0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204 0l36.203 36.204c9.997 9.997 9.997 26.206 0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z&#34;/&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;www.google.com&#34;&gt;&lt;svg aria-hidden=&#34;true&#34; role=&#34;img&#34; viewBox=&#34;0 0 512 512&#34; style=&#34;height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;&#34;&gt;&lt;path d=&#34;M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206 0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204 0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204 0l36.203 36.204c9.997 9.997 9.997 26.206 0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z&#34;/&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
county result
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;Nominal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;www.google.com&#34;&gt;&lt;svg aria-hidden=&#34;true&#34; role=&#34;img&#34; viewBox=&#34;0 0 512 512&#34; style=&#34;height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;&#34;&gt;&lt;path d=&#34;M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206 0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204 0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204 0l36.203 36.204c9.997 9.997 9.997 26.206 0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z&#34;/&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;www.google.com&#34;&gt;&lt;svg aria-hidden=&#34;true&#34; role=&#34;img&#34; viewBox=&#34;0 0 512 512&#34; style=&#34;height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;&#34;&gt;&lt;path d=&#34;M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206 0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204 0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204 0l36.203 36.204c9.997 9.997 9.997 26.206 0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z&#34;/&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
state result
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;Nominal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;www.google.com&#34;&gt;&lt;svg aria-hidden=&#34;true&#34; role=&#34;img&#34; viewBox=&#34;0 0 512 512&#34; style=&#34;height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;&#34;&gt;&lt;path d=&#34;M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206 0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204 0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204 0l36.203 36.204c9.997 9.997 9.997 26.206 0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z&#34;/&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
county votes cast (~pop size)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;Ratio&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;www.google.com&#34;&gt;&lt;svg aria-hidden=&#34;true&#34; role=&#34;img&#34; viewBox=&#34;0 0 512 512&#34; style=&#34;height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;&#34;&gt;&lt;path d=&#34;M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206 0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204 0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204 0l36.203 36.204c9.997 9.997 9.997 26.206 0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z&#34;/&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
county result margin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;Ratio&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;www.google.com&#34;&gt;&lt;svg aria-hidden=&#34;true&#34; role=&#34;img&#34; viewBox=&#34;0 0 512 512&#34; style=&#34;height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;&#34;&gt;&lt;path d=&#34;M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206 0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204 0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204 0l36.203 36.204c9.997 9.997 9.997 26.206 0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z&#34;/&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
county result landslide
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;Nominal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;www.google.com&#34;&gt;&lt;svg aria-hidden=&#34;true&#34; role=&#34;img&#34; viewBox=&#34;0 0 512 512&#34; style=&#34;height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;&#34;&gt;&lt;path d=&#34;M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206 0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204 0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204 0l36.203 36.204c9.997 9.997 9.997 26.206 0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z&#34;/&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;grammar-of-graphics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Grammar of Graphics&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Data graphics visually display measured quantities by means of the combined use of points, lines, a coordinate system, numbers, symbols, words, shading, and color.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Tufte (&lt;a href=&#34;#ref-tufte_visual_1983&#34; role=&#34;doc-biblioref&#34;&gt;1983&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In evidence in the Washington Post graphic is a judicious &lt;strong&gt;mapping&lt;/strong&gt; of data to visuals, underpinned by a secure understanding of analysis context. This act of carefully considering how best to leverage visual systems given the available data and analysis priorities is key to designing effective data graphics.&lt;/p&gt;
&lt;p&gt;In the late 1990s Leland Wilkinson, a Computer Scientist and Statistician, introduced the &lt;a href=&#34;https://www.springer.com/gp/book/9780387245447&#34;&gt;Grammar of Graphics&lt;/a&gt; as an approach that captures this process of turning data into visuals. &lt;span class=&#34;citation&#34;&gt;Wilkinson (&lt;a href=&#34;#ref-wilkinson_grammar_1999&#34; role=&#34;doc-biblioref&#34;&gt;1999&lt;/a&gt;)&lt;/span&gt;’s thesis is that if graphics can be described in a consistent way according to their structure and composition, then the process of &lt;strong&gt;generating&lt;/strong&gt; graphics of different types can be systematised. This has obvious benefits for building visualization toolkits: it makes it easy to specify chart types and combinations and helps formalise the process of designing data visualizations. &lt;a href=&#34;https://vega.github.io/vega-lite/&#34;&gt;vega-lite&lt;/a&gt;, &lt;a href=&#34;https://www.tableau.com/en-gb&#34;&gt;Tableau&lt;/a&gt; and &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;&lt;code&gt;ggplot2&lt;/code&gt;&lt;/a&gt; are all underpinned by &lt;a href=&#34;https://www.springer.com/gp/book/9780387245447&#34;&gt;Grammar of Graphics&lt;/a&gt; thinking.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Wilkinson (&lt;a href=&#34;#ref-wilkinson_grammar_1999&#34; role=&#34;doc-biblioref&#34;&gt;1999&lt;/a&gt;)&lt;/span&gt;’s grammar separates the construction of a data graphic into a series of components. Below are the components of the &lt;em&gt;Layered Grammar of Graphics&lt;/em&gt; on which &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;&lt;code&gt;ggplot2&lt;/code&gt;&lt;/a&gt; is based &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wickham_layered_2010&#34; role=&#34;doc-biblioref&#34;&gt;Wickham 2010&lt;/a&gt;)&lt;/span&gt;, a slight edit on &lt;span class=&#34;citation&#34;&gt;Wilkinson (&lt;a href=&#34;#ref-wilkinson_grammar_1999&#34; role=&#34;doc-biblioref&#34;&gt;1999&lt;/a&gt;)&lt;/span&gt;’s original work.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:gog&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/gog.png&#34; alt=&#34;Components of @wickham_layered_2010&#39;s Layered Grammar of Graphics.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Components of &lt;span class=&#34;citation&#34;&gt;Wickham (&lt;a href=&#34;#ref-wickham_layered_2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;’s Layered Grammar of Graphics.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The seven components in Figure &lt;a href=&#34;#fig:gog&#34;&gt;2&lt;/a&gt; are together used to create &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;&lt;code&gt;ggplot2&lt;/code&gt;&lt;/a&gt; specifications. The aspects to emphasise at this stage are those in &lt;strong&gt;emphasis&lt;/strong&gt;, which are required in any &lt;code&gt;ggplot2&lt;/code&gt; specification: the &lt;span style=&#34;color:#5A91CA;font-weight:bold&#34;&gt;data&lt;/span&gt; containing the variables of interest, the &lt;code&gt;geom&lt;/code&gt; or &lt;span style=&#34;color:#E17637;font-weight:bold&#34;&gt;marks&lt;/span&gt; to be used to represent data and the &lt;strong&gt;aesthetic&lt;/strong&gt; (&lt;code&gt;mapping=aes(...)&lt;/code&gt;) attributes, or visual &lt;span style=&#34;color:#62B743;font-weight:bold&#34;&gt;channels&lt;/span&gt; through which variables are to be encoded.&lt;/p&gt;
&lt;p&gt;To demonstrate this, let’s generate some scatterplots based on the 2019 General Election data we will be analysing later in the session. Two variables worth exploring for association here are: &lt;code&gt;con_1719&lt;/code&gt;, the change in Conservative vote share by constituency between 2017-2019, and &lt;code&gt;leave_hanretty&lt;/code&gt;, the size of the Leave vote in the 2016 EU referendum, estimated at Parliamentary Constituency level &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-hanretty_areal_2017&#34; role=&#34;doc-biblioref&#34;&gt;Hanretty 2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:gog-demo&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/gog-demo.png&#34; alt=&#34;Plots, grammars and associated `ggplot2` specifications for the scatterplot.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Plots, grammars and associated &lt;code&gt;ggplot2&lt;/code&gt; specifications for the scatterplot.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:gog-demo&#34;&gt;3&lt;/a&gt; are three plots and associated &lt;code&gt;ggplot2&lt;/code&gt; specifications. Reading-off the graphics and the associated code, you should get a feel for how &lt;code&gt;ggplot2&lt;/code&gt; specifications are constructed:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We start with a data frame, in this case each observation is an electoral result for a Parliamentary Constituency. In the &lt;code&gt;ggplot2&lt;/code&gt; spec this is passed using the pipe operator (&lt;code&gt;data_ge %&amp;gt;%&lt;/code&gt;). We also identify the variables we wish to encode and their measurement &lt;span style=&#34;color:#5A91CA;font-weight:bold&#34;&gt;type&lt;/span&gt;. Remembering last session’s materials, both &lt;code&gt;con_1719&lt;/code&gt; and &lt;code&gt;leave_hanretty&lt;/code&gt; are &lt;code&gt;ratio&lt;/code&gt; scale variables.&lt;/li&gt;
&lt;li&gt;Next is the encoding (&lt;code&gt;mapping=aes()&lt;/code&gt;), which determines how the data are to be mapped to visual &lt;span style=&#34;color:#62B743;font-weight:bold&#34;&gt;channels&lt;/span&gt;. A scatterplot is a 2d representation in which horizontal and vertical position varies in a meaningful way, in response to the values of a data set. Here the values of &lt;code&gt;leave_hanretty&lt;/code&gt; are mapped along the x-axis and the values of &lt;code&gt;con_1719&lt;/code&gt; are mapped along the y-axis.&lt;/li&gt;
&lt;li&gt;Finally, we represent individual data items with &lt;span style=&#34;color:#E17637;font-weight:bold&#34;&gt;marks&lt;/span&gt; using the &lt;code&gt;geom_point&lt;/code&gt; geometry.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the middle plot, the grammar is updated such that the points are coloured according to &lt;code&gt;winning_party&lt;/code&gt;, a variable of type categorical &lt;code&gt;nominal&lt;/code&gt;. In the bottom plot constituencies that flipped from Labour-to-Conservative between 2017-19 are emphasised by varying the transparency (&lt;code&gt;alpha&lt;/code&gt;) of points. I have described &lt;code&gt;flipped&lt;/code&gt; as an &lt;code&gt;ordinal&lt;/code&gt; variable, but strictly it is a &lt;code&gt;nominal&lt;/code&gt; (binary) variable. Due to the way it is encoded in the plot – constituencies that flipped (&lt;code&gt;flipped=TRUE&lt;/code&gt;) are given greater visual emphasis – I think it is more appropriate to call it an ordinal variable.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    It is understandable if at this stage the specifications in Figure &lt;a href=&#34;#fig:gog-demo&#34;&gt;3&lt;/a&gt; still seem alien to you. We will be updating, expanding and refining &lt;code&gt;ggplot2&lt;/code&gt; specifications throughout this module to support all aspects of modern data analysis – from data cleaning and exploratory analysis through to model evaluation and communication.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;marks-and-visual-channels&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Marks and visual channels&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Effective data visualization design is concerned with representing data through marks and visual channels in a way that best conveys the properties of the data that are to be depicted.&lt;/p&gt;
&lt;p&gt;via &lt;a href=&#34;https://www.gicentre.net/jwo/index&#34;&gt;Jo Wood&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You might have noticed that in my descriptions of &lt;code&gt;ggplot2&lt;/code&gt; specifications I introduced &lt;span style=&#34;color:#E17637;font-weight:bold&#34;&gt;marks&lt;/span&gt; as another term for &lt;strong&gt;geometry&lt;/strong&gt; and visual encoding &lt;span style=&#34;color:#62B743;font-weight:bold&#34;&gt;channels&lt;/span&gt; as another term for &lt;strong&gt;aesthetics&lt;/strong&gt;. I also paid special attention to the &lt;span style=&#34;color:#5A91CA;font-weight:bold&#34;&gt;data types&lt;/span&gt; that are being encoded.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Marks&lt;/strong&gt; are graphical elements such as &lt;em&gt;bars&lt;/em&gt;, &lt;em&gt;lines&lt;/em&gt;, &lt;em&gt;points&lt;/em&gt;, &lt;em&gt;ellipses&lt;/em&gt; that can be used to represent data items – in &lt;code&gt;ggplot2&lt;/code&gt;, these are accessed through the functions prefaced with &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/#section-geoms&#34;&gt;&lt;code&gt;geom_*&lt;/code&gt;&lt;/a&gt;. Visual &lt;strong&gt;channels&lt;/strong&gt; are attributes such as &lt;em&gt;colour&lt;/em&gt;, &lt;em&gt;size&lt;/em&gt;, &lt;em&gt;position&lt;/em&gt; that, when mapped to data, control the appearance of marks in response to the values of a dataset. Not all &lt;strong&gt;channels&lt;/strong&gt; are equally effective. In fact we can say confidently that for particular &lt;strong&gt;data types&lt;/strong&gt; and &lt;strong&gt;tasks&lt;/strong&gt;, some &lt;strong&gt;channels&lt;/strong&gt; perform better than others.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Marks&lt;/strong&gt; and &lt;strong&gt;channels&lt;/strong&gt; are terms used in the interface of &lt;a href=&#34;https://www.tableau.com/en-gb&#34;&gt;Tableau&lt;/a&gt; and in &lt;a href=&#34;https://vega.github.io/vega-lite/&#34;&gt;vega-lite&lt;/a&gt; specifications. They are also used widely in Information Visualization, an academic discipline devoted to the study of data graphics, and most notably by Tamara &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; in her textbook &lt;a href=&#34;https://www.routledge.com/Visualization-Analysis-and-Design/Munzner/p/book/9781466508910&#34;&gt;&lt;em&gt;Visualization Analysis and Design&lt;/em&gt;&lt;/a&gt;. &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;’s work is important and widely adopted as it synthesises over foundational research in Information Visualization and Cognitive Science testing how effective different visual channels are at supporting different tasks.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:munzner&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/munzner.png&#34; alt=&#34;Visual channels to which data items can be encoded, as they appear in @munzner_visualization_2014.&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Visual channels to which data items can be encoded, as they appear in &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:munzner&#34;&gt;4&lt;/a&gt; is taken from Chapter 5 of &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; and lists the main visual channels with which data might be encoded. The grouping and order of the figure is meaningful. Channels are grouped according to the tasks to which they are best suited and then ordered according to their effectiveness at supporting those tasks. To the left are &lt;strong&gt;magnitude:order&lt;/strong&gt; channels – those that are best suited to tasks aimed at quantifying data items. To the right are &lt;strong&gt;identity:category&lt;/strong&gt; channels – those that are most suited to supporting tasks that involve isolating, grouping and associating data items.&lt;/p&gt;
&lt;p&gt;We can use this organisation of visual channels to make decisions about appropriate encodings given a variable’s measurement level. If we wished to convey the magnitude of something, for example a quantitative (&lt;code&gt;ratio&lt;/code&gt;) variable like the size of the Conservative vote share in a constituency, we might select a channel that has good quantitative effectiveness – &lt;em&gt;position on a common scale&lt;/em&gt; or &lt;em&gt;length&lt;/em&gt;. If we wished to also effectively identify and associate constituencies according to the political party that was elected, a categorical &lt;code&gt;nominal&lt;/code&gt; variable, we might select a channel that has good associative properties such as &lt;em&gt;colour hue&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluating-designs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluating designs&lt;/h3&gt;
&lt;p&gt;The effectiveness rankings of visual channels in Figure &lt;a href=&#34;#fig:munzner&#34;&gt;4&lt;/a&gt; are not simply based on Munzner’s preference. They are informed by detailed experimental work – &lt;span class=&#34;citation&#34;&gt;W. Cleveland and McGill (&lt;a href=&#34;#ref-cleveland_graphical_1984&#34; role=&#34;doc-biblioref&#34;&gt;1984&lt;/a&gt;)&lt;/span&gt;, later replicated by &lt;span class=&#34;citation&#34;&gt;Heer and Bostock (&lt;a href=&#34;#ref-heer_crowdsourcing_2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; – which involved conducting controlled experiments testing people’s ability to make judgements from graphical elements. We can use Figure &lt;a href=&#34;#fig:munzner&#34;&gt;4&lt;/a&gt; to help make decisions around which data item to encode with which
visual channel. This is particularly useful when designing data-rich graphics, where several data items are to be encoded
simultaneously. The figure also offers a low cost way of &lt;strong&gt;evaluating&lt;/strong&gt; different designs against their encoding effectiveness. To illustrate this, we can use Munzner’s ranking of channels to evaluate the Washington Post graphic discussed in Figure &lt;a href=&#34;#fig:wp-map&#34;&gt;1&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:wp-eval-size&#34;&gt;Table 2: &lt;/span&gt;Encoding effectiveness for Washington Post graphic that emphasises &lt;em&gt;vote margin and size&lt;/em&gt; of counties using triangle marks.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Mark
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Data item
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Channel
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Rank
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;4&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 0px solid;&#34;&gt;
&lt;strong&gt;Magnitude:Order&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 7em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;html&gt;
&lt;body&gt;
&lt;img src=&#34;../class/03-class_files/location.png&#34;&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Location
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;ratio&lt;/code&gt; (&lt;code&gt;cyclic&lt;/code&gt;)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
position in x,y
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;quant
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 7em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;html&gt;
&lt;body&gt;
&lt;img src=&#34;../class/03-class_files/height.png&#34;&gt;
&lt;/body&gt;
&lt;/html&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Votes cast (~pop size)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;ratio&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
length
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;quant
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 7em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;html&gt;
&lt;body&gt;
&lt;img src=&#34;../class/03-class_files/width.png&#34;&gt;
&lt;/body&gt;
&lt;/html&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Margin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;ratio&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
length
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;quant
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 7em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;html&gt;
&lt;body&gt;
&lt;img src=&#34;../class/03-class_files/landslide.png&#34;&gt;
&lt;/body&gt;
&lt;/html&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Landslide
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;ordinal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
area
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;quant
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;1&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 0px solid;&#34;&gt;
&lt;strong&gt;Identify:Category&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;width: 7em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;html&gt;
&lt;body&gt;
&lt;img src=&#34;../class/03-class_files/winner.png&#34;&gt;
&lt;/body&gt;
&lt;/html&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Winner
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;nominal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
colour hue
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;cat
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Table &lt;a href=&#34;#tab:wp-eval-size&#34;&gt;2&lt;/a&gt; provides a summary of the encodings used in the version of the graphic emphasising &lt;em&gt;vote margin and size&lt;/em&gt;. US counties are represented using a peak-shaped &lt;strong&gt;mark&lt;/strong&gt; (&lt;i class=&#34;fas fa-angle-up&#34;&gt;&lt;/i&gt;). The key purpose of the graphic is to depict the geography of voting outcomes, and so the most effective quantitative channel – position on an aligned scale – is used to order the county marks (&lt;i class=&#34;fas fa-angle-up&#34;&gt;&lt;/i&gt;) with a 2D geographic arrangement. With the positional channels taken, the two quantitive measures, &lt;em&gt;votes cast&lt;/em&gt; and &lt;em&gt;result margin&lt;/em&gt;, are encoded with the next highest ranked channel, 1D length: height varies according to number of &lt;em&gt;votes cast&lt;/em&gt; and width according to &lt;em&gt;result margin&lt;/em&gt;. The marks are additionally encoded with two categorical variables: whether the county-level result was a &lt;em&gt;landslide&lt;/em&gt; and also the ultimate &lt;em&gt;winner&lt;/em&gt;. Since the intention is to give greater visual saliency to counties that resulted in a &lt;em&gt;landslide&lt;/em&gt;, this as an &lt;code&gt;ordinal&lt;/code&gt; variable, encoded with a quantitative channel: 2D area. The &lt;em&gt;winning party&lt;/em&gt;, a categorical &lt;code&gt;nominal&lt;/code&gt; variable, is encoded using colour hue.&lt;/p&gt;
&lt;p&gt;Each of the encoding choices used in the graphic follow conventional wisdom in that data items are encoded using visual channels that are appropriate to their measurement level. Glancing down the “rank” column we can also argue that the graphic has high effectiveness. Whilst technically &lt;em&gt;spatial region&lt;/em&gt; is the most effective channel for encoding &lt;code&gt;nominal&lt;/code&gt; data, it is already in use in our graphic as the &lt;i class=&#34;fas fa-angle-up&#34;&gt;&lt;/i&gt; marks are arranged by geographic position. Additionally, it makes sense to distinguish &lt;span style=&#34;color:#DB534D;font-weight:bold&#34;&gt;Republican&lt;/span&gt; and &lt;span style=&#34;color:#3879A1;font-weight:bold&#34;&gt;Democrat&lt;/span&gt; wins using the colours with which they are always represented. Given the fact that the positional channels are in use to represent geographic &lt;em&gt;location&lt;/em&gt;, length to represent &lt;em&gt;votes cast&lt;/em&gt; and &lt;em&gt;vote margin&lt;/em&gt;, the only superior visual channel to 2D area that could be used to encode the &lt;em&gt;landslide&lt;/em&gt; variable is &lt;em&gt;orientation&lt;/em&gt;. There are very good reasons for not varying the orientation of the &lt;i class=&#34;fas fa-angle-up&#34;&gt;&lt;/i&gt; marks. Most obvious is that this would clearly undermine perception of length encodings used to represent the vote margin (width) and absolute vote size (height).&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Data visualization design almost always involves trade-offs. When deciding on a design configuration, it is necessary to prioritise analysis tasks and data and match representations and encodings that are most effective to the tasks that are most important. This then constrains the encoding options for less important data items and tasks. Good visualization design is sensitive to this interplay between tasks, data and encoding.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;symbolisation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Symbolisation&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Symbolization is the process of encoding something with meaning in order to represent something else. Effective symbol design requires that the relationship between a symbol and the information that symbol represents (the referent) be clear and easily interpreted.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;White (&lt;a href=&#34;#ref-white_symbolization_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Implicit in the discussion above, and when making design decisions, is the importance of &lt;a href=&#34;https://gistbok.ucgis.org/bok-topics/symbolization-and-visual-variables&#34;&gt;&lt;strong&gt;symbolisation&lt;/strong&gt;&lt;/a&gt;. Scrolling through the original Washington Post article, the overall pattern that can be discerned is of population-dense coastal and metropolitan counties voting Democrat – densely-packed, tall, wide and blue &lt;i style=&#34;color:#3879A1;font-weight:bold&#34; class=&#34;fas fa-lg fa-angle-up&#34;&gt;&lt;/i&gt; marks – contrasted against population-sparse rural and small town areas voting Republican – short, wide and red &lt;i style=&#34;color:#DB534D;font-weight:bold&#34; class=&#34;fas fa-angle-up&#34;&gt;&lt;/i&gt; marks. The graphic evokes a distinctive landscape of voting behaviour, emphasised by its caption: “&lt;em&gt;The peaks and valleys of Trump and Clinton’s support&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Symbolisation&lt;/strong&gt; is used equally well in the variant of the graphic emphasising two-party &lt;em&gt;Swing&lt;/em&gt; between the 2012 and 2016 elections. Each county is represented as a &lt;span style=&#34;font-weight:bolder&#34;&gt;|&lt;/span&gt; mark. The &lt;em&gt;Swing&lt;/em&gt; variable is then encoded by continuously varying mark angles: counties swinging Republican are angled to the right &lt;span style=&#34;color:#DB534D;font-weight:bold&#34;&gt;/&lt;/span&gt;; counties swinging Democrat are angled to the left &lt;span style=&#34;color:#3879A1;font-weight:bolder&#34;&gt;\&lt;/span&gt;. Although &lt;em&gt;angle&lt;/em&gt; is a less effective channel at encoding quantities than is &lt;em&gt;length&lt;/em&gt;, there are obvious links to the political phenomena in the symbolisation – angled right for counties that moved to the right politically. Additionally, the variable itself might be regarded as cyclic – or at least it has a ceiling with an important mid-point that requires emphasis. It is worth taking a second look at the full graphic here. Since there is spatial autocorrelation in case trajectories, we quickly assemble from the graphic dominant patterns of Swing to the Republicans (Great Lakes, rural East Coast), predictable Republican stasis (the mid west) and to detect more isolated, locally exceptional Swings to the Democrats (rapidly urbanising counties in the deep south).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-perceptual-rankings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Checking perceptual rankings&lt;/h3&gt;
&lt;p&gt;I mentioned that Munzner’s effectiveness ordering of visual channels is informed by empirical evidence – controlled experiments that examine perceptual abilities at making judgements from graphical primitives. It is worth elaborating a little on this experimental work, and on how established knowledge in Cognitive Science can be used to inform design choices.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;W. S. Cleveland (&lt;a href=&#34;#ref-cleveland_elements_1993&#34; role=&#34;doc-biblioref&#34;&gt;1993&lt;/a&gt;)&lt;/span&gt; emphasises three perceptual activities that take place when we make sense of data graphics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt; : the element of the graphic must be easily discernible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assembly&lt;/strong&gt; : the process of identifying patterns and structure within the graphical elements of the
visualization.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Estimation&lt;/strong&gt; : the process of making comparisons of the magnitudes of data items from the visual elements used.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These activities can be related to the categories of task outlined earlier. &lt;strong&gt;Detection&lt;/strong&gt; is especially important for &lt;strong&gt;selective&lt;/strong&gt; and &lt;strong&gt;associative&lt;/strong&gt; tasks that involve isolating and grouping data items, whilst &lt;strong&gt;estimation&lt;/strong&gt; is necessary for tasks that are &lt;strong&gt;orderable&lt;/strong&gt; and &lt;strong&gt;quantitative&lt;/strong&gt;, involving the ranking and reading-off of quantities.&lt;/p&gt;
&lt;div id=&#34;detection-and-preattentive-processing&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Detection and preattentive processing&lt;/h4&gt;
&lt;p&gt;A useful distinction when considering graphical cognition is between processes that are attentive and pre-attentive &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ware_visual_2008&#34; role=&#34;doc-biblioref&#34;&gt;Ware 2008&lt;/a&gt;)&lt;/span&gt;. Attentive processing describes the conscious processing that happens when we attempt to make sense of a visual field. Preattentive processing happens unconsciously and is the type of cognitive processing that allows something to be understood ‘at a glance’. Visual items that immediately pop-out to us induce preattentive processing.&lt;/p&gt;
&lt;p&gt;The ability to provoke pop-out – making some things on a data graphic more easily detectible than others – relates to &lt;strong&gt;detection&lt;/strong&gt;. It can be useful for supporting &lt;strong&gt;selective&lt;/strong&gt; and &lt;strong&gt;associative&lt;/strong&gt; tasks, and so is often used in a data graphic to encode categorical variables. For example, in the Washington Post graphic the use of colour hue to differentiate and group together counties that voted Republican or Democrat. Preattentive processes can also apply to &lt;strong&gt;assembly&lt;/strong&gt;. We naturally construct and assemble patterns that are smooth and continuous when perceiving a graphic and so deviations from this continuity are often attended to unconsciously. An example here would be those urbanising counties in the deep South, which were locally exceptional in swinging to Democrat (to the left).&lt;/p&gt;
&lt;p&gt;We can test this preattentive processing by using visual encoding channels to assist a task that requires us to &lt;strong&gt;select&lt;/strong&gt; and &lt;strong&gt;associate&lt;/strong&gt; visual items. Below are a set of data graphics containing 200 numbers. The graphics are currently hidden, but can be revealed by clicking the &lt;i class=&#34;fa fa-xs fa-play&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt; icon. For each graphic I want you to scan across the number, isolate or select the number &lt;code&gt;3&lt;/code&gt;, then group or associate the &lt;code&gt;3&lt;/code&gt;s together and count the number of instances that they occur. Speed is important here – so work as quickly as you can.&lt;/p&gt;
&lt;p&gt;First, a set of numbers without applying any special encoding to the number &lt;code&gt;3&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Encoding: &lt;strong&gt;none&lt;/strong&gt;&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:no-encoding&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/no-encoding.png&#34; alt=&#34;Encoding: *none*.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Encoding: &lt;em&gt;none&lt;/em&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/details&gt;
Isolate/select &lt;code&gt;3&lt;/code&gt; from the list of numbers and count its number of occurrences.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;If you were racing to complete the task, I imagine you found it moderately stressful. Let’s explore using visual encoding to off-load some of this cognitive effort. We’ll start with a visual channel that does not have particularly strong preattentive properties: &lt;em&gt;area&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Encoding: &lt;strong&gt;area&lt;/strong&gt;&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:area-encoding&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/area-encoding.png&#34; alt=&#34;Encoding: *area*.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Encoding: &lt;em&gt;area&lt;/em&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/details&gt;
Isolate/select &lt;code&gt;3&lt;/code&gt; from the list of numbers and count its number of occurrences.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Using visualization to support the task makes it an order of magnitude easier. But let’s explore some visual channels that have even more powerful properties. I mentioned that tilt/angle has preattentive properties where the data items to be emphasised deviate from some regular pattern. In the graphic below, the number &lt;code&gt;3&lt;/code&gt; is encoded with &lt;em&gt;tilt&lt;/em&gt; or &lt;em&gt;angle&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Encoding: &lt;strong&gt;angle&lt;/strong&gt;&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:angle-encoding&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/angle-encoding.png&#34; alt=&#34;Encoding: *tilt/angle*.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: Encoding: &lt;em&gt;tilt/angle&lt;/em&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/details&gt;
Isolate/select &lt;code&gt;3&lt;/code&gt; from the list of numbers and count its number of occurrences.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;This is in fact more challenging than the size encoding. I think this is most likely because the geometric patterns of the marks used (numbers) is being varied and so this limits the extent to which we unconsciously perceive smoothness and continuity (e.g. limits &lt;strong&gt;assembly&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;Next we’ll use a visual channel with known effectiveness at assisting select and associate tasks. &lt;em&gt;Colour hue&lt;/em&gt; appears as the second-ranked most effective in &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;’s ordering.&lt;/p&gt;
&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Encoding: &lt;strong&gt;colour hue&lt;/strong&gt;&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:hue-encoding&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/hue-encoding.png&#34; alt=&#34;Encoding: *colour hue*.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: Encoding: &lt;em&gt;colour hue&lt;/em&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/details&gt;
Isolate/select &lt;code&gt;3&lt;/code&gt; from the list of numbers and count its number of occurrences.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Finally, though a slightly contrived example, we can use the top-ranked channel according to &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;: &lt;em&gt;spatial region&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Encoding: &lt;strong&gt;spatial region&lt;/strong&gt;&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:spatial-encoding&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/spatial-encoding.png&#34; alt=&#34;Encoding: *spatial region*.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: Encoding: &lt;em&gt;spatial region&lt;/em&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/details&gt;
Isolate/select &lt;code&gt;3&lt;/code&gt; from the list of numbers and count its number of occurrences.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Estimation&lt;/h4&gt;
&lt;p&gt;The informal tests above hopefully persuade you of &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;’s ordering of &lt;em&gt;identity:category&lt;/em&gt; channels in the right side of Figure &lt;a href=&#34;#fig:munzner&#34;&gt;4&lt;/a&gt;. The ranking of &lt;em&gt;magnitude:order&lt;/em&gt; channels is also informed by established theory and evidence.&lt;/p&gt;
&lt;p&gt;When using data graphics to communicate quantities, certain visual channels are known to induce biases. &lt;a href=&#34;https://en.wikipedia.org/wiki/Psychophysics&#34;&gt;Psychophysics&lt;/a&gt; is a branch of psychology that develops methods aimed at capturing the often non-linear relationship between the properties of a &lt;em&gt;stimuli&lt;/em&gt; such as symbol length, area or colour value, and their &lt;em&gt;perceived response&lt;/em&gt;. Stevens’ power law is an empirically-derived relationship that models this effect. The power function takes the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R=kS_n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is the magnitude of the stimulus, for example, the absolute length of a line or area of a circle, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is the response, the perceived length and area, and &lt;span class=&#34;math inline&#34;&gt;\(_n\)&lt;/span&gt; is the power law exponent that varies with the type of stimulus. If there is a perfect linear mapping between the stimulus and response, &lt;span class=&#34;math inline&#34;&gt;\(_n\)&lt;/span&gt; is 1.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Stevens and Guirao (&lt;a href=&#34;#ref-stevens_subjective_1963&#34; role=&#34;doc-biblioref&#34;&gt;1963&lt;/a&gt;)&lt;/span&gt;’ experimental work involved varying the length of lines and areas of squares and deriving power functions for their perception. For length, an exponent of ~1.0 was estimated; for area an exponent of 0.7. So whilst variation in length is accurately perceived, we &lt;em&gt;underestimate&lt;/em&gt; the size of areas as they increase. &lt;span class=&#34;citation&#34;&gt;Flannery (&lt;a href=&#34;#ref-flannery_subjective_1963&#34; role=&#34;doc-biblioref&#34;&gt;1971&lt;/a&gt;)&lt;/span&gt;’s work, which was concerned with the perception of quantities in graduated point maps, estimated an exponent of 0.87 for the perception of circle size.&lt;/p&gt;
&lt;p&gt;Experimental findings vary and so these models of human perception are also subject to variation. Nevertheless, corrections can be applied. In cartography a &lt;a href=&#34;https://makingmaps.net/2007/08/28/perceptual-scaling-of-map-symbols/&#34;&gt;Flannery compensation&lt;/a&gt; is used when representing quantities with area.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:perception&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/perception.png&#34; alt=&#34;Differences in power law exponents for the perception of variation in length and area.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: Differences in power law exponents for the perception of variation in length and area.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This early experimental work that tries to understand how encoded quantities are perceived is clearly important. But we use data graphics to do much more than estimate single quantities. If data graphics are to serve as tools for analysis, we also need some confidence that the &lt;strong&gt;inferences&lt;/strong&gt; made when studying data using graphics are accurate and reliable. In the Information Visualization domain, experimental work has recently been published exploring the perception of statistical quantities – location and dispersion &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-correll_error_2014&#34; role=&#34;doc-biblioref&#34;&gt;Correll and Gleicher 2014&lt;/a&gt;)&lt;/span&gt;, correlation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rensink_perception_2010&#34; role=&#34;doc-biblioref&#34;&gt;Rensink and Baldridge 2010&lt;/a&gt;; &lt;a href=&#34;#ref-harrison_ranking_2014&#34; role=&#34;doc-biblioref&#34;&gt;Harrison et al. 2014&lt;/a&gt;; &lt;a href=&#34;#ref-kay_beyond_2016&#34; role=&#34;doc-biblioref&#34;&gt;Kay and Heer 2016&lt;/a&gt;)&lt;/span&gt; and spatial autocorrelation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-klippel_interpreting_2011&#34; role=&#34;doc-biblioref&#34;&gt;Klippel, Hardisty, and Li 2011&lt;/a&gt;; &lt;a href=&#34;#ref-beecham_maplineups_2017&#34; role=&#34;doc-biblioref&#34;&gt;Beecham et al. 2017&lt;/a&gt;)&lt;/span&gt; – in commonly used chart types. More on this later in the module.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;colour&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Colour&lt;/h3&gt;
&lt;p&gt;As demonstrated in the section on &lt;a href=&#34;#detection-and-preattentive-processing-1&#34;&gt;preattentive processing&lt;/a&gt;, colour is a very powerful visual channel. When considering how to encode data with colour, it is helpful to consider three properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hue&lt;/strong&gt; : what we generally refer to as “colour” in everyday life – red, blue green, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Saturation&lt;/strong&gt; : how much of a colour there is.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Luminance/Brightness&lt;/strong&gt; : how dark or light a colour is.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The underlying rule when using colour in data graphics is to use properties of colour that match the properties of the data. Categorical &lt;code&gt;nominal&lt;/code&gt; data – data that cannot be easily ordered – should be encoded using discrete colours with no obvious order: colour hue. Categorical &lt;code&gt;ordinal&lt;/code&gt; data – data whose categories can be ordered – should be encoded with colours that contain an intrinsic order: saturation or brightness, usually allocated into gradients. &lt;code&gt;Quantitative&lt;/code&gt; data – data that can be ordered and contain values on a continuous scale – should also be encoded with colours that contain an intrinsic order: saturation or brightness, expressed on a continuous scale.&lt;/p&gt;
&lt;p&gt;As we will discover shortly, these principles are applied by default in &lt;code&gt;ggplot2&lt;/code&gt;, along with access to perceptually uniform schemes. Its &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/scale_brewer.html&#34;&gt;&lt;code&gt;brewer&lt;/code&gt;&lt;/a&gt; scales, for example.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    There are many considerations when using colour to support visual data analysis and communication – and we will return to these at various points in the module. Read Lisa Charotte-Rost’s &lt;a href=&#34;https://blog.datawrapper.de/colorguide/&#34;&gt;Guide to Colours in Data Visualization&lt;/a&gt; before proceeding to the &lt;a href=&#34;#techniques-1&#34;&gt;Techniques&lt;/a&gt; section.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;techniques&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Techniques&lt;/h2&gt;
&lt;p&gt;The technical element to this session involves analysing data from the 2019 UK General Election, reported by Parliamentary Constituency. After importing and describing the dataset, you will generate data graphics that expose patterns in voting behaviour. You will do so by writing &lt;code&gt;ggplot2&lt;/code&gt; specifications.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the &lt;a href=&#34;../homework/03-homework_files/03-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 03-template.Rmd&lt;/a&gt; file for this session and save it to the &lt;code&gt;reports&lt;/code&gt; folder of your &lt;code&gt;vis-for-gds&lt;/code&gt; project.&lt;/li&gt;
&lt;li&gt;Open your &lt;code&gt;vis-for-gds&lt;/code&gt; project in RStudio and load the template file by clicking &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Open File ...&lt;/code&gt; &amp;gt; &lt;code&gt;reports/03-template.Rmd&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;import&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Import&lt;/h3&gt;
&lt;p&gt;The template file lists the required packages – &lt;code&gt;tidyverse&lt;/code&gt;, &lt;code&gt;sf&lt;/code&gt; and also &lt;a href=&#34;https://cran.r-project.org/web/packages/parlitools/index.html&#34;&gt;&lt;code&gt;parlitools&lt;/code&gt;&lt;/a&gt;. Installing &lt;code&gt;parlitools&lt;/code&gt; brings down the 2019 UK General Election dataset, along with other constituency-level datasets. Loading it with &lt;code&gt;library(parlitools)&lt;/code&gt; makes these data available to your R session.&lt;/p&gt;
&lt;p&gt;The dataset containing 2019 UK General Election data is called &lt;code&gt;bes_2019&lt;/code&gt;. This contains results data released by &lt;a href=&#34;https://commonslibrary.parliament.uk/research-briefings/cbp-8749/&#34;&gt;House of Commons Library&lt;/a&gt;. We can get a quick overview in the usual way – with a call to &lt;code&gt;glimpse(&amp;lt;dataset-name&amp;gt;)&lt;/code&gt;. The dataset’s variables are also described on the &lt;code&gt;parlitools&lt;/code&gt; &lt;a href=&#34;https://docs.evanodell.com/parlitools/articles/bes-2019.html&#34;&gt;web pages&lt;/a&gt;. You will notice that &lt;code&gt;bes_2019&lt;/code&gt; contains 650 rows, one for each Parliamentary Constituency, and 118 columns. Contained in the columns are variables reporting vote numbers and shares for the main political parties for 2019 and 2017 General Elections, as well as names and codes (&lt;code&gt;ID&lt;/code&gt;s) for each Parliamentary Constituency and the county, region and country in which they are contained. You might want to count the number of counties and regions in the UK, and the number of constituencies contained by counties and regions, using some of the &lt;code&gt;dplyr&lt;/code&gt; functions introduced in the last session – for example with calls to &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;count()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The aim of this analysis session is to get you familiar with &lt;code&gt;ggplot2&lt;/code&gt; specifications. We will be replicating some of the visual data analysis of the 2019 UK General Election in &lt;span class=&#34;citation&#34;&gt;Beecham (&lt;a href=&#34;#ref-beecham_using_2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, inspired by the &lt;a href=&#34;https://www.washingtonpost.com/graphics/politics/2016-election/election-results-from-coast-to-coast/&#34;&gt;Washington Post&lt;/a&gt; graphic. For this we need to calculate an additional variable – Butler Swing &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-butler_why_1990&#34; role=&#34;doc-biblioref&#34;&gt;Butler and Van Beek 1990&lt;/a&gt;)&lt;/span&gt; – which represents the average change in share of the vote won by two parties contesting successive elections. Code for calculating this variable (named &lt;code&gt;swing_con_lab&lt;/code&gt;) is in the &lt;a href=&#34;../homework/03-homework_files/03-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 03-template.Rmd&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although initially intuitive, the measure takes a little interpretation. A Swing to the Conservatives, which we observe most often in this dataset, could happen in three ways:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;An increase in Conservative vote share and a decrease in Labour vote share.&lt;/li&gt;
&lt;li&gt;An increase in both Conservative and Labour vote share, but with the Conservative increase outstripping that of Labour’s.&lt;/li&gt;
&lt;li&gt;A decrease in both Conservative and Labour vote share, but with the Conservative decline being less severe than that of Labour’s.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Different from the US where “third parties” play a negligible role, scenarios 2 and 3 do occur in the UK. You will notice that &lt;code&gt;swing_con_lab&lt;/code&gt; is a signed value: positive indicates a Swing to Conservative, negative a Swing to Labour.&lt;/p&gt;
&lt;p&gt;The only other dataset to load is a &lt;code&gt;.geojson&lt;/code&gt; file containing the geometries of constituencies, collected originally from &lt;a href=&#34;https://geoportal.statistics.gov.uk/&#34;&gt;ONS Open Geography Portal&lt;/a&gt; and simplified using &lt;a href=&#34;https://github.com/mbloch/mapshaper&#34;&gt;mapshaper&lt;/a&gt;. This is a special class of data frame containing a &lt;a href=&#34;https://r-spatial.github.io/sf/articles/sf1.html&#34;&gt;Simple Features&lt;/a&gt; &lt;code&gt;geometry&lt;/code&gt; column – more on this later in the module.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summarise&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summarise&lt;/h3&gt;
&lt;p&gt;You will no doubt be familiar with the result of the 2019 General Election – a landslide Conservative victory that confounded expectations. To start, we can quickly compute some summary statistics around the vote. In the code block below, we count the number of seats won by party and overall vote share by party. For the latter, my code is a little more elaborate than I intended it to be. I needed to reshape the data frame using &lt;code&gt;pivot_wider()&lt;/code&gt; such that each row represents a vote for a party in a constituency. From here I computed in a single function the vote share for each party.&lt;/p&gt;
&lt;p&gt;Whilst the Conservative party hold 56% of constituencies, they won only 44% of the vote share. The equivalent stats for Labour are 31% and 32% respectively. Incidentally, whilst the Conservatives increased their share of constituencies from 2017 (where they had just 317, 49% of constituencies) their vote share increase was reasonably small – in 2017 they gained 42.5% of the vote.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of constituencies won by party.
bes_2019 %&amp;gt;%
  group_by(winner_19) %&amp;gt;%
  summarise(count=n()) %&amp;gt;%
  arrange(desc(count))
## # A tibble: 11 x 2
##    winner_19                        count
##    &amp;lt;chr&amp;gt;                            &amp;lt;int&amp;gt;
##  1 Conservative                       365
##  2 Labour                             202
##  3 Scottish National Party             48
##  4 Liberal Democrat                    11
##  5 Democratic Unionist Party            8
##  6 Sinn Fein                            7
##  7 Plaid Cymru                          4
##  8 Social Democratic &amp;amp; Labour Party     2
##  9 Alliance                             1
## 10 Green                                1
## 11 Speaker                              1

# Share of vote by party.
bes_2019 %&amp;gt;%
  select(constituency_name, total_vote_19, con_vote_19:alliance_vote_19, region) %&amp;gt;% # Select cols containing vote counts by party.
  pivot_longer(cols=con_vote_19:alliance_vote_19, names_to=&amp;quot;party&amp;quot;, values_to=&amp;quot;votes&amp;quot;) %&amp;gt;% # Pivot to make each row a vote for a party in a constituency.
  mutate(party=str_extract(party, &amp;quot;[^_]+&amp;quot;)) %&amp;gt;% # Use some regex to pull out party name.
  group_by(party) %&amp;gt;%
  summarise(vote_share=sum(votes, na.rm=TRUE)/sum(total_vote_19)) %&amp;gt;%
  arrange(desc(vote_share))

## # A tibble: 12 x 2
##    party    vote_share
##    &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;
##  1 con         0.436
##  2 lab         0.321
##  3 ld          0.115
##  4 snp         0.0388
##  5 green       0.0270
##  6 brexit      0.0201
##  7 dup         0.00763
##  8 sf          0.00568
##  9 pc          0.00479
## 10 alliance    0.00419
## 11 sdlp        0.00371
## 12 uup         0.00291&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below are some summary statistics computed over the newly created &lt;code&gt;swing_con_lab&lt;/code&gt; variable. As the Conservative and Labour votes are negligible in Northern Ireland, it makes sense to focus on Great Britain for our analysis of Con-Lab Swing and so the first step in the code is to create a new data frame filtering out Northern Ireland. We will work with this for the rest of the session.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_gb &amp;lt;- bes_2019 %&amp;gt;%
  filter(region != &amp;quot;Northern Ireland&amp;quot;) %&amp;gt;%
  mutate(
    swing_con_lab=0.5*((con_19-con_17)-(lab_19-lab_17)),
    # Recode to 0 Chorley incoming speaker,Buckingham outgoing speaker --  uncontested seat.
    swing_con_lab=if_else(constituency_name %in% c(&amp;quot;Chorley&amp;quot;, &amp;quot;Buckingham&amp;quot;),0,swing_con_lab)
  )

data_gb %&amp;gt;%
  summarise(
    min_swing=min(swing_con_lab),
    max_swing=max(swing_con_lab),
    median_swing=median(swing_con_lab),
    num_swing=sum(swing_con_lab&amp;gt;0),
    num_landslide_con=sum(con_19&amp;gt;50, na.rm=TRUE),
    num_landslide_lab=sum(lab_19&amp;gt;50, na.rm=TRUE)
    )

## # A tibble: 1 x 6
##   min_swing max_swing median_swing num_swing num_landslide_con num_landslide_lab
##       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;             &amp;lt;int&amp;gt;             &amp;lt;int&amp;gt;
## 1     -6.47      18.4         4.44       599               280               120&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot distributions&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:histogram&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/hist.png&#34; alt=&#34;Histograms of Swing variable.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 11: Histograms of Swing variable.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let’s get going with some &lt;code&gt;ggplot2&lt;/code&gt; specifications by plotting some of these variables. Below is the code for plotting a histogram of the Swing variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_gb %&amp;gt;%
  ggplot(mapping=aes(swing_con_lab)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A reminder of the general form of the &lt;code&gt;ggplot2&lt;/code&gt; specification (first covered in &lt;a href=&#34;#grammar-of-graphics-1&#34;&gt;Grammar of Graphics&lt;/a&gt; section):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start with some &lt;strong&gt;data&lt;/strong&gt;: &lt;code&gt;data_gb&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Define the &lt;strong&gt;encoding&lt;/strong&gt;: &lt;code&gt;mapping=aes()&lt;/code&gt;. In this case, we want to summarise over the &lt;code&gt;swing_con_lab&lt;/code&gt; variable.&lt;/li&gt;
&lt;li&gt;Specify the &lt;strong&gt;marks&lt;/strong&gt; to be used: &lt;code&gt;geom_histogram()&lt;/code&gt; in this case.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Different from the scatterplot example, there is more happening in the internals of &lt;code&gt;ggplot2&lt;/code&gt; when creating a histogram. Technically &lt;code&gt;geom_histogram()&lt;/code&gt; is what &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; would describe as a &lt;em&gt;chart idiom&lt;/em&gt; rather than a &lt;em&gt;mark&lt;/em&gt; (geometric primitive). The Swing variable is partitioned into bins and observations in each bin are counted. The x-axis (bins) and y-axis (counts by bin) is therefore derived from the supplied variable (&lt;code&gt;swing_con_lab&lt;/code&gt;). Should you wish, you could enter &lt;code&gt;?geom_histogram&lt;/code&gt; for fuller detail and documentation around controlling bin sizes amongst other things.&lt;/p&gt;
&lt;p&gt;You will notice that by default the histogram’s bars are given a grey colour. To &lt;em&gt;set&lt;/em&gt; them to a different colour, add a &lt;code&gt;fill=&lt;/code&gt; argument to &lt;code&gt;geom_histogram()&lt;/code&gt;. In the code block below, colour is set using &lt;a href=&#34;https://en.wikipedia.org/wiki/Web_colors&#34;&gt;hex codes&lt;/a&gt; – &lt;code&gt;&#34;#003c8f&#34;&lt;/code&gt;, based on the theme for this course website. I use the term &lt;em&gt;set&lt;/em&gt; here and not &lt;em&gt;map&lt;/em&gt; or &lt;em&gt;encode&lt;/em&gt; and there is a principled explanation for this. Any part of a &lt;code&gt;ggplot2&lt;/code&gt; specification that involves encoding data – mapping data to a visual channel – should be specified through the &lt;code&gt;mapping=aes()&lt;/code&gt; argument. Anything else, for example changing the default colour of marks, their thickness and transparency, needs to be &lt;em&gt;set&lt;/em&gt; outside of this argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_gb %&amp;gt;%
  ggplot(mapping=aes(swing_con_lab)) +
  geom_histogram(fill=&amp;quot;#003c8f&amp;quot;) +
  labs(
    title=&amp;quot;Butler two-party Labour-Conservative Swing for Constituencies in GB&amp;quot;,
    subtitle=&amp;quot;-- 2019 versus 2017 election&amp;quot;,
    caption=&amp;quot;Data published by House of Commons Library, accessed via `parlitools`&amp;quot;,
    x=&amp;quot;Swing&amp;quot;, y=&amp;quot;count&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might have noticed that different elements of a &lt;code&gt;ggplot2&lt;/code&gt; specification are added (&lt;code&gt;+&lt;/code&gt;) as layers. In the example above, the additional layer of labels (&lt;code&gt;labs()&lt;/code&gt;) is not intrinsic to the graphic. However, often you will add layers that do affect the graphic itself: for example the scaling of encoded values (e.g. &lt;code&gt;scale_*_continuous()&lt;/code&gt;) or whether the graphic is to be conditioned on another variable to generate &lt;a href=&#34;https://flowingdata.com/tag/small-multiples/&#34;&gt;small multiples&lt;/a&gt; for comparison (e.g. &lt;code&gt;facet_*()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Read this design exposition by &lt;a href=&#34;http://tinlizzie.org/histograms/&#34;&gt;Lunzner and McNamara, 2020&lt;/a&gt; for an excellent discussion of the analysis and design considerations when working with histograms. There are of course other &lt;em&gt;geoms&lt;/em&gt; for summarising over 1D distributions: &lt;code&gt;geom_boxplot()&lt;/code&gt;, &lt;code&gt;geom_dotplot()&lt;/code&gt;, &lt;code&gt;geom_violin()&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div id=&#34;faceting-by-region&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Faceting by region&lt;/h4&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:hist-region&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/hist-region.png&#34; alt=&#34;Histograms of Swing variable, grouped by region.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 12: Histograms of Swing variable, grouped by region.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Adding a call to &lt;code&gt;facet_*()&lt;/code&gt;, we can quickly compare how Swing varies by region (as in Figure &lt;a href=&#34;#fig:hist-region&#34;&gt;12&lt;/a&gt;). The plot is annotated with the &lt;em&gt;median&lt;/em&gt; value for Swing (4.4) by adding a vertical line layer (&lt;code&gt;geom_vline()&lt;/code&gt;) and setting its x-intercept at this value. From this, there is some evidence of a regional geography to the 2019 vote: London and Scotland are particularly distinctive in containing relatively few constituencies swinging greater than the expected midpoint; North East, Yorkshire &amp;amp; The Humber, and to a lesser extent West and East Midlands, appear to show the largest relative number of constituencies swinging greater than the midpoint. It was this graphic, especially the fact that London and Scotland look different from the rest of the country, that prompted the scatterplots in Figure &lt;a href=&#34;#fig:gog-demo&#34;&gt;3&lt;/a&gt; comparing gain in Conservative vote shares against the Brexit vote.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-ranksmagnitudes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot ranks/magnitudes&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:bars&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/bars.png&#34; alt=&#34;Plots of vote shares by party.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 13: Plots of vote shares by party.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Previously we calculated overall vote share by Political Party. We could continue the exploration of votes by region by re-using this code to generate plots displaying quantities but also comparing by region, using marks and encoding channels that are suitable for magnitudes.&lt;/p&gt;
&lt;p&gt;To generate a bar chart similar to the left of Figure &lt;a href=&#34;#fig:bars&#34;&gt;13&lt;/a&gt; the &lt;code&gt;ggplot2&lt;/code&gt; specification would be&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_gb %&amp;gt;%
  &amp;lt;some dplyr code&amp;gt; %&amp;gt;% # The code block summarising vote by party.
  ...  %&amp;gt;% #
  &amp;lt;summarised data frame&amp;gt;  %&amp;gt;% # The summarised data frame of vote share by party, piped to ggplot2.
  ggplot(aes(x=reorder(party, -vote_share), y=vote_share)) + # Categorical-ordinal x-axis (party, reordered), Ratio y-axis (vote_share).
  geom_col(fill=&amp;quot;#003c8f&amp;quot;) # Set colour by website theme.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quick breakdown of the specification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: This is the summarised data frame in which each row is a political party and the column describes the vote share recorded for that party.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: I have dropped the &lt;code&gt;mapping=&lt;/code&gt;. &lt;code&gt;ggplot2&lt;/code&gt; always looks for &lt;code&gt;aes()&lt;/code&gt; and so can save some code clutter. In this case we are mapping &lt;code&gt;party&lt;/code&gt; to the x-axis, a categorical variable made ordinal by the fact that we reorder the axis left-to-right descending according to &lt;code&gt;vote_share&lt;/code&gt;. &lt;code&gt;vote_share&lt;/code&gt; is mapped to the y-axis – so encoded using bar length, on an aligned scale, an effective channel for conveying magnitudes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_col()&lt;/code&gt; for generating the bars.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Again, I’ve set bar colour according to the website theme and included titles and captions. Optionally we add a &lt;code&gt;coord_flip()&lt;/code&gt; layer in order to display the bars horizontally. This makes the category axis labels easier to read and also seems more appropriate for the visual “ranking” of bars.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;ggplot2&lt;/code&gt; themes control the appearance of all non-data items – font sizes and types, gridlines, axes labels. Checkout the complete list of &lt;code&gt;ggplot2&lt;/code&gt;’s &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/ggtheme.html&#34;&gt;default themes&lt;/a&gt;. If you like the look of the BBC’s in-house data graphics – I do (or at least I like many of them) – explore their &lt;a href=&#34;https://bbc.github.io/rcookbook/&#34;&gt;Data Journalism cookbook&lt;/a&gt;. In fact I’d recommend working through the cookbook as it is a great resource for distilling many of the non-data-related decisions that are made when communicating graphically.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div id=&#34;faceting-by-region-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Faceting by region&lt;/h4&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:bars-region&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/bars-region.png&#34; alt=&#34;Plots of vote shares by party and region.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 14: Plots of vote shares by party and region.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:bars-region&#34;&gt;14&lt;/a&gt; the graphic is faceted by region. This requires an updated derived dataset grouping by &lt;code&gt;vote_share&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; &lt;code&gt;region&lt;/code&gt; and of course adding a faceting layer (&lt;code&gt;geom_facet(~region)&lt;/code&gt;) to the &lt;code&gt;ggplot2&lt;/code&gt; specification&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. The graphic is more data-rich, but additional cognitive effort is required in relating the bars representing political parties between different graphical subsets. We can assist this &lt;em&gt;identify&lt;/em&gt; and &lt;em&gt;associate&lt;/em&gt; task by encoding the bars with an appropriate visual channel: &lt;em&gt;colour hue&lt;/em&gt;. The &lt;code&gt;ggplot2&lt;/code&gt; specification for this is as you would expect – we add a mapping to &lt;code&gt;geom_col()&lt;/code&gt; and pass the variable name &lt;code&gt;party&lt;/code&gt; to the fill argument (&lt;code&gt;aes(fill=party)&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;lt;derived_data&amp;gt; %&amp;gt;%
  ggplot(aes(x=reorder(party, vote_share), y=vote_share)) +
  geom_col(aes(fill=party)) +
  coord_flip() +
  facet_wrap(~region)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Trying this for yourself, you will observe that the &lt;code&gt;ggplot2&lt;/code&gt; internals are clever here. Since &lt;code&gt;party&lt;/code&gt; is a categorical variable, a categorical (hue-based) colour scheme is automatically applied. Try passing a quantitative variable (&lt;code&gt;fill=vote_share&lt;/code&gt;) and see what happens.&lt;/p&gt;
&lt;p&gt;Clever as this is, when encoding political parties with colour &lt;strong&gt;symbolisation&lt;/strong&gt; is important. More control over the encoding is necessary in order to specify the colours with which parties are most commonly represented. We can override &lt;code&gt;ggplot2&lt;/code&gt;’s default colour by adding a &lt;code&gt;scale_fill_manual()&lt;/code&gt; layer into which a vector of hex codes describing the colour of each political party is passed (&lt;code&gt;party_colours&lt;/code&gt;). We also need to tell &lt;code&gt;ggplot2&lt;/code&gt; which element of &lt;code&gt;party_colours&lt;/code&gt; to apply to which value of &lt;code&gt;party&lt;/code&gt;. In the code below, a derived table is generated summarising &lt;code&gt;vote_share&lt;/code&gt; by political party and region. In the final line the &lt;code&gt;party&lt;/code&gt; variable is recoded as a &lt;a href=&#34;https://r4ds.had.co.nz/factors.html&#34;&gt;&lt;code&gt;factor&lt;/code&gt;&lt;/a&gt;. You might recall from the last session that factors are categorical variables of fixed and orderable values, called &lt;code&gt;levels&lt;/code&gt;. The call to &lt;code&gt;mutate()&lt;/code&gt; recodes &lt;code&gt;party&lt;/code&gt; as a factor variable and orders the levels according to overall vote share.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Generate derived data.
temp_party_shares_region &amp;lt;- data_gb %&amp;gt;%
  select(constituency_name, region, total_vote_19, con_vote_19:alliance_vote_19) %&amp;gt;%
  pivot_longer(cols=con_vote_19:alliance_vote_19, names_to=&amp;quot;party&amp;quot;, values_to=&amp;quot;votes&amp;quot;) %&amp;gt;%
  mutate(party=str_extract(party, &amp;quot;[^_]+&amp;quot;)) %&amp;gt;%
  group_by(party, region) %&amp;gt;%
  summarise(vote_share=sum(votes, na.rm=TRUE)/sum(total_vote_19)) %&amp;gt;%
  filter(party %in% c(&amp;quot;con&amp;quot;, &amp;quot;lab&amp;quot;, &amp;quot;ld&amp;quot;, &amp;quot;snp&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;brexit&amp;quot;, &amp;quot;pc&amp;quot;)) %&amp;gt;%
  mutate(party=factor(party, levels=c(&amp;quot;con&amp;quot;, &amp;quot;lab&amp;quot;, &amp;quot;ld&amp;quot;, &amp;quot;snp&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;brexit&amp;quot;, &amp;quot;pc&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, a vector of objects is created containing the hex codes for the colours of political parties (&lt;code&gt;party_colours&lt;/code&gt;). This is a named vector, with names assigned from the &lt;code&gt;levels&lt;/code&gt; of the &lt;code&gt;party&lt;/code&gt; variable that was just created.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define colours.
con &amp;lt;- &amp;quot;#0575c9&amp;quot;
lab &amp;lt;- &amp;quot;#ed1e0e&amp;quot;
ld &amp;lt;- &amp;quot;#fe8300&amp;quot;
snp &amp;lt;- &amp;quot;#ebc31c&amp;quot;
green &amp;lt;- &amp;quot;#78c31e&amp;quot;
pc &amp;lt;- &amp;quot;#4e9f2f&amp;quot;
brexit &amp;lt;- &amp;quot;#25b6ce&amp;quot;
other &amp;lt;- &amp;quot;#bdbdbd&amp;quot;

party_colours &amp;lt;- c(con, lab, ld, snp, green, brexit, pc)
names(party_colours) &amp;lt;- levels(temp_party_shares_region %&amp;gt;% pull(party))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;ggplot2&lt;/code&gt; specification is then updated with the &lt;code&gt;scale_fill_manual()&lt;/code&gt; layer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot2 spec.
temp_party_shares_region %&amp;gt;%
  ggplot(aes(x=reorder(party, vote_share), y=vote_share)) +
  geom_col(aes(fill=party)) +
  scale_fill_manual(values=party_colours) +
  coord_flip() +
  facet_wrap(~region)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The idea behind visualization toolkits such as &lt;a href=&#34;https://vega.github.io/vega-lite/&#34;&gt;vega-lite&lt;/a&gt;, &lt;a href=&#34;https://www.tableau.com/en-gb&#34;&gt;Tableau&lt;/a&gt; and &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;&lt;code&gt;ggplot2&lt;/code&gt;&lt;/a&gt; is to insert visual data analysis approaches into the Data Scientist’s workflow. Rather than being overly concerned with low-level aspects of drawing, mapping to screen coordinates and scaling factors, the analyst instead focuses on aspects crucial to analysis – exposing patterns in the data by carefully specifying an encoding of data to visuals. Hadley Wickham talks about the type of workflow you will see used throughout this module – bits of &lt;code&gt;dplyr&lt;/code&gt; to prepare data for charting before being piped (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) to a &lt;code&gt;ggplot2&lt;/code&gt; specification – as equivalent to a &lt;a href=&#34;https://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Towards-a-grammar-of-interactive-graphics&#34;&gt;grammar of interactive graphics&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;The process of searching for, defining and inserting manual colour schemes for creating Figure &lt;a href=&#34;#fig:bars-region&#34;&gt;14&lt;/a&gt; might seem inimical to this. Indeed I was reluctant to include this code so early in the module – there is some reasonably advanced &lt;code&gt;dplyr&lt;/code&gt; and a little &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html&#34;&gt;regular expression&lt;/a&gt; in the data preparation code that I don’t want you to be overly concerned with. However, having control of these slightly more low-level properties is sometimes necessary even for supporting exploratory analysis, in this case for enabling a &lt;strong&gt;symbolisation&lt;/strong&gt; that is clear and easily interpretable. Try relating the bars without our manual setting of colours by political party – it certainly requires some mental gymnastics.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;faceting-by-region-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Faceting by region&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-relationships&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot relationships&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:scatters-con&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/scatters-con.png&#34; alt=&#34;Plots of 2019 versus 2017 vote shares.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 15: Plots of 2019 versus 2017 vote shares.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In the &lt;a href=&#34;../#grammar-of-graphics-1/&#34;&gt;Grammar of Graphics&lt;/a&gt; section we demonstrated how scatterplots are specified in &lt;code&gt;ggplot2&lt;/code&gt;. Scatterplots are useful examples for introducing &lt;code&gt;ggplot2&lt;/code&gt; specifications as they involve working with genuine mark primitives (&lt;code&gt;geom_point()&lt;/code&gt;) and can be built up using a wide range of encoding channels.&lt;/p&gt;
&lt;p&gt;To continue the investigation of change in vote shares for the major parties between 2017 and 2019, Figure &lt;a href=&#34;#fig:scatters-con&#34;&gt;15&lt;/a&gt; contains scatterplots of vote share in 2019 (y-axis) against vote share in 2017 (x-axis) for Conservative and Labour. The graphics are annotated with a diagonal line. If constituencies voted in 2019 in exactly the same way as 2017, the points would all converge on the diagonal, points above the diagonal indicate a larger vote share than 2017, those below the diagonal represent a smaller vote share than 2017. Points are coloured according to the winning party in 2019 and constituencies that flipped from Labour to Conservative are emphasised using transparency and shape.&lt;/p&gt;
&lt;p&gt;The code for generating most of the scatterplot comparing Conservative vote shares is below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_gb %&amp;gt;%
  mutate(winner_19=case_when(
           winner_19 == &amp;quot;Conservative&amp;quot; ~ &amp;quot;Conservative&amp;quot;,
           winner_19 == &amp;quot;Labour&amp;quot; ~ &amp;quot;Labour&amp;quot;,
           TRUE ~ &amp;quot;Other&amp;quot;
         )) %&amp;gt;%
  ggplot(aes(x=con_17, y=con_19)) +
  geom_point(aes(colour=winner_19), alpha=.8) +
  geom_abline(intercept = 0, slope = 1) +
  scale_colour_manual(values=c(con,lab,other)) +
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hopefully there is little surprising here:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: The &lt;code&gt;data_gb&lt;/code&gt; data frame. Values of &lt;code&gt;winner_19&lt;/code&gt; that are not &lt;em&gt;Conservative&lt;/em&gt; or &lt;em&gt;Labour&lt;/em&gt; are recoded to &lt;em&gt;Other&lt;/em&gt; using a &lt;strong&gt;conditional statement&lt;/strong&gt;. This is because points are eventually coloured according to winning party, but the occlusion of points adds visual complexity and so I’ve chosen to prioritise the two main parties and recode remaining parties to other.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: Conservative vote share in 2017 and 2019 are mapped to the x- and y- axes respectively and &lt;code&gt;winner_19&lt;/code&gt; to colour. &lt;code&gt;scale_colour_manual()&lt;/code&gt; is used for customising the colours.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_point()&lt;/code&gt; for generating the points of the scatterplot; geom_abline() for drawing the reference diagonal.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;You will have encountered conditionals in the reading from last session. &lt;code&gt;case_when&lt;/code&gt; allows you to avoid writing multiple &lt;code&gt;if_else()&lt;/code&gt; statements. It wasn’t really necessary here – I could have used a single if_else with something like:&lt;/p&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data_gb %&amp;gt;%
  mutate(
    winner_19=if_else(!winner_19 %in% c(&amp;quot;Conservative&amp;quot;, &amp;quot;Labour&amp;quot;), &amp;quot;Other&amp;quot;, winner_19)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A general point from the code blocks in this session is of the importance of proficiency in &lt;code&gt;dplyr&lt;/code&gt;. Throughout the module you will find yourself needing to calculate new variables, recode variables, and reorganise data frames before passing through to &lt;code&gt;ggplot2&lt;/code&gt;.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-geography&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot geography&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:map-winners&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/map-winners.png&#34; alt=&#34;Choropleth of elected parties in 2019 General Election.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 16: Choropleth of elected parties in 2019 General Election.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In the graphics that facet by region, our analysis suggests at a geography to voting and certainly to observed changes in voting comparing the 2017 and 2019 elections (e.g. Figure &lt;a href=&#34;#fig:hist-region&#34;&gt;12&lt;/a&gt;). We end the session by encoding the results data with a spatial arrangement – we’ll generate some maps.&lt;/p&gt;
&lt;p&gt;To do this we need to define a join on the boundary data (&lt;code&gt;cons_outline&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Join constituency boundaries.
data_gb &amp;lt;- cons_outline %&amp;gt;%
  inner_join(data_gb, by=c(&amp;quot;pcon19cd&amp;quot;=&amp;quot;ons_const_id&amp;quot;))
# Check class.
## [1] &amp;quot;sf&amp;quot;         &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code for generating the &lt;a href=&#34;https://en.wikipedia.org/wiki/Choropleth_map&#34;&gt;Choropleth maps&lt;/a&gt; of winning party by constituency in Figure &lt;a href=&#34;#fig:map-winners&#34;&gt;16&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Recode winner_19 as a factor variable for assigning colours.
data_gb &amp;lt;- data_gb %&amp;gt;%
  mutate(
    winner_19=if_else(winner_19==&amp;quot;Speaker&amp;quot;, &amp;quot;Other&amp;quot;, winner_19),
    winner_19=as_factor(winner_19))

# Create a named vector of colours
party_colours &amp;lt;- c(con, lab, ld, green, other, snp, pc)
names(party_colours) &amp;lt;- levels(data_gb %&amp;gt;% pull(winner_19))

# Plot map.
data_gb %&amp;gt;%
  ggplot(aes(fill=winner_19)) +
  geom_sf(colour=&amp;quot;#eeeeee&amp;quot;, size=0.01)+
  # Optionally add a layer for regional boundaries.
  # geom_sf(data=. %&amp;gt;% group_by(region) %&amp;gt;% summarise(), colour=&amp;quot;#eeeeee&amp;quot;, fill=&amp;quot;transparent&amp;quot;, size=0.08)+
  coord_sf(crs=27700, datum=NA) +
  scale_fill_manual(values=party_colours)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A breakdown of the &lt;code&gt;ggplot2&lt;/code&gt; spec:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: The &lt;code&gt;dplyr&lt;/code&gt; code updates &lt;code&gt;data_gb&lt;/code&gt; by recoding &lt;code&gt;winner_19&lt;/code&gt; as a factor and defining a named vector of colours to supply to &lt;code&gt;scale_fill_manual()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: No surprises here – &lt;code&gt;fill&lt;/code&gt; according to &lt;code&gt;winner_19&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_sf()&lt;/code&gt; is a special class of geometry. It draws objects depending on the contents of the &lt;code&gt;geometry&lt;/code&gt; column. In this case &lt;code&gt;MULTIPOLYGON&lt;/code&gt;, so read this as a &lt;em&gt;polygon&lt;/em&gt; geometric primitive.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coordinates&lt;/strong&gt;: &lt;code&gt;coord_sf&lt;/code&gt; – we set the coordinate system (CRS) explicitly. In this case &lt;a href=&#34;https://spatialreference.org/ref/epsg/osgb-1936-british-national-grid/&#34;&gt;OS British National Grid&lt;/a&gt;. More on this later in the module.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: I’ve subtly introduced light grey (&lt;code&gt;colour=&#34;#eeeeee&#34;&lt;/code&gt;) and thin (&lt;code&gt;size=0.01&lt;/code&gt;) constituency boundaries to the &lt;code&gt;geom_sf&lt;/code&gt; mark. On the map to the right outlines for regions are added as another &lt;code&gt;geom_sf&lt;/code&gt; layer.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:spoke-map&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../class/03-class_files/spoke-map.png&#34; alt=&#34;Map of Butler Con-Lab Swing in 2019 General Election.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 17: Map of Butler Con-Lab Swing in 2019 General Election.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This has been a packed session. I’m providing a very abbreviated introduction to map design with &lt;code&gt;ggplot2&lt;/code&gt; and want to reserve the details of how &lt;code&gt;ggplot2&lt;/code&gt; can be used in more involved visualization design for later in the module. Since the graphic has been discussed at length, it would be strange not to demonstrate how the encoding in the Washington Post piece can be applied here to analyse our Butler two-party swing variable &lt;span class=&#34;citation&#34;&gt;(e.g. &lt;a href=&#34;#ref-beecham_using_2020&#34; role=&#34;doc-biblioref&#34;&gt;Beecham 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;First, some helper functions – converting degrees to radians and centring &lt;code&gt;geom_spoke()&lt;/code&gt; geometries. Don’t bother yourself with these details, just run the code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Convert degrees to radians.
get_radians &amp;lt;- function(degrees) {
  (degrees * pi) / (180)
}
# Rescaling function.
map_scale &amp;lt;- function(value, min1, max1, min2, max2) {
  return  (min2+(max2-min2)*((value-min1)/(max1-min1)))
}
# Position subclass for centred geom_spoke as per --
# https://stackoverflow.com/questions/55474143/how-to-center-geom-spoke-around-their-origin
position_center_spoke &amp;lt;- function() PositionCenterSpoke
PositionCenterSpoke &amp;lt;- ggplot2::ggproto(&amp;#39;PositionCenterSpoke&amp;#39;, ggplot2::Position,
                                        compute_panel = function(self, data, params, scales) {
                                          data$x &amp;lt;- 2*data$x - data$xend
                                          data$y &amp;lt;- 2*data$y - data$yend
                                          data$radius &amp;lt;- 2*data$radius
                                          data
                                        }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next re-define &lt;code&gt;party_colours&lt;/code&gt;, the object we use for manually setting colours, to contain just three values: hex codes for Conservative, Labour and Other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;party_colours &amp;lt;- c(con, lab, other)
names(party_colours) &amp;lt;- c(&amp;quot;Conservative&amp;quot;, &amp;quot;Labour&amp;quot;, &amp;quot;Other&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the &lt;code&gt;ggplot2&lt;/code&gt; specification:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max_shift &amp;lt;- max(abs(data_gb %&amp;gt;% pull(swing_con_lab)))
min_shift &amp;lt;- -max_shift

gb &amp;lt;- data_gb %&amp;gt;%
  mutate(
    is_flipped=seat_change_1719 %in% c(&amp;quot;Conservative gain from Labour&amp;quot;,&amp;quot;Labour gain from Conservative&amp;quot;),
    elected=if_else(!winner_19 %in% c(&amp;quot;Conservative&amp;quot;, &amp;quot;Labour&amp;quot;), &amp;quot;Other&amp;quot;, as.character(winner_19))
    ) %&amp;gt;%
  ggplot()+
  geom_sf(aes(fill=elected), colour=&amp;quot;#636363&amp;quot;, alpha=.2, size=.01)+
  geom_spoke(
             aes(x=bng_e, y=bng_n, angle=get_radians(map_scale(swing_con_lab,min_shift,max_shift,135,45)), colour=elected, size=is_flipped),
             radius=7000, position=&amp;quot;center_spoke&amp;quot;
             )+
  coord_sf(crs=27700, datum=NA)+
  scale_size_ordinal(range=c(.3,.9))+
  scale_colour_manual(values=party_colours)+
  scale_fill_manual(values=party_colours)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A breakdown:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: &lt;code&gt;data_gb&lt;/code&gt; is updated with a boolean identifying whether or not the Constituency flipped Con-Lab/Lab-Con between successive elections (&lt;code&gt;is_flipped&lt;/code&gt;), and a variable simplifying the party elected to either Conservative, Labour or Other.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;/strong&gt;: &lt;code&gt;geom_sf&lt;/code&gt; is again filled by elected party. This encoding is made more subtle by adding transparency (&lt;code&gt;alpha=.2&lt;/code&gt;). &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/geom_spoke.html&#34;&gt;&lt;code&gt;geom_spoke()&lt;/code&gt;&lt;/a&gt; is a line primitive that can be encoded with a location and direction. It is mapped to the geographic centroid of each Constituency (&lt;code&gt;bng_e&lt;/code&gt; - easting, &lt;code&gt;bng_n&lt;/code&gt; - northing), coloured according to elected party, sized according to whether the Constituency flipped its vote and tilted according to the Swing variable. Here I’ve created a function (map_scale) which pegs the maximum Swing values in either direction to 45 degrees (max Swing to the right, Conservative) and 135 degrees (max Swing to the left, Labour).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Marks&lt;/strong&gt;: &lt;code&gt;geom_sf()&lt;/code&gt; for the Constituency boundaries, &lt;code&gt;geom_spoke()&lt;/code&gt; for the angled line primitives.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt;: &lt;code&gt;geom_spoke()&lt;/code&gt; primitives are sized to emphasise whether constituencies have flipped. The size encoding is censored to two values with &lt;code&gt;scale_size_ordinal()&lt;/code&gt;. Passed to &lt;code&gt;scale_colour_manual()&lt;/code&gt; and &lt;code&gt;scale_fill_manual()&lt;/code&gt; is the vector of &lt;code&gt;party_colours&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coordinates&lt;/strong&gt;: &lt;code&gt;coord_sf&lt;/code&gt; – the CRS is &lt;a href=&#34;https://spatialreference.org/ref/epsg/osgb-1936-british-national-grid/&#34;&gt;OS British National Grid&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;/strong&gt;: The &lt;code&gt;radius&lt;/code&gt;, the of &lt;code&gt;geom_spoke()&lt;/code&gt; primitives is a sensible default arrived at through trial and error, its &lt;code&gt;position&lt;/code&gt; set using our &lt;code&gt;center_spoke&lt;/code&gt; class.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Visualization design is ultimately a process of decision-making. Data must be filtered and prioritised before being encoded with marks, visual channels and symbolisation. The most successful data graphics are those that expose structure, connections and comparisons that could not be achieved easily via other, non-visual means. This session has introduced concepts – a vocabulary, framework and empirically-informed guidelines – that helps support this decision-making process and that underpins modern visualization toolkits (&lt;code&gt;ggplot2&lt;/code&gt; included). Through an analysis of UK 2019 General Election data, we have demonstrated how these concepts can be applied in a real data analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-beecham_using_2020&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R. 2020. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Using position, angle and thickness to expose the shifting geographies of the 2019 UK General Election&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;Environment and Planning A: Economy and Space&lt;/span&gt;&lt;/em&gt; 52 (5): 833–36.
&lt;/div&gt;
&lt;div id=&#34;ref-beecham_maplineups_2017&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R., J. Dykes, W. Meulemans, A. Slingsby, C. Turkay, and J. Wood. 2017. &lt;span&gt;“Map Line-Ups: Effects of Spatial Structure on Graphical Inference.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;IEEE&lt;/span&gt; Transactions on Visualization &amp;amp; Computer Graphics&lt;/em&gt; 23 (1): 391–400.
&lt;/div&gt;
&lt;div id=&#34;ref-butler_why_1990&#34; class=&#34;csl-entry&#34;&gt;
Butler, D., and S. Van Beek. 1990. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Why not swing? Measuring electoral change&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;Political Science &amp;amp; Politics&lt;/span&gt;&lt;/em&gt; 23 (2): 178–84.
&lt;/div&gt;
&lt;div id=&#34;ref-cleveland_elements_1993&#34; class=&#34;csl-entry&#34;&gt;
Cleveland, William S. 1993. &lt;em&gt;The Elements of Graphing Data&lt;/em&gt;. Hobart Press.
&lt;/div&gt;
&lt;div id=&#34;ref-cleveland_graphical_1984&#34; class=&#34;csl-entry&#34;&gt;
Cleveland, W., and R. McGill. 1984. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Graphical &lt;span&gt;Perception&lt;/span&gt;: &lt;span&gt;Theory&lt;/span&gt;, &lt;span&gt;Experimentation&lt;/span&gt;, and &lt;span&gt;Application&lt;/span&gt; to the &lt;span&gt;Development&lt;/span&gt; of &lt;span&gt;Graphical&lt;/span&gt; &lt;span&gt;Methods&lt;/span&gt;&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 79 (387): 531–54.
&lt;/div&gt;
&lt;div id=&#34;ref-correll_error_2014&#34; class=&#34;csl-entry&#34;&gt;
Correll, M., and M. Gleicher. 2014. &lt;span&gt;“Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error.”&lt;/span&gt; &lt;em&gt;&lt;span&gt;IEEE&lt;/span&gt; Transactions on Visualization &amp;amp; Computer Graphics&lt;/em&gt; 20 (12): 2141–51.
&lt;/div&gt;
&lt;div id=&#34;ref-flannery_subjective_1963&#34; class=&#34;csl-entry&#34;&gt;
Flannery, J. J. 1971. &lt;span&gt;“The Relative Effectiveness of Some Common Graduated Point Symbols in the Presentation of Quantitative Data.”&lt;/span&gt; &lt;em&gt;Cartographica&lt;/em&gt; 8 (2): 96–109.
&lt;/div&gt;
&lt;div id=&#34;ref-hanretty_areal_2017&#34; class=&#34;csl-entry&#34;&gt;
Hanretty, C. 2017. &lt;span&gt;“Areal Interpolation and the UK’s Referendum on EU Membership.”&lt;/span&gt; &lt;em&gt;Journal of Elections, Public Opinion and Parties&lt;/em&gt; 37 (4): 466–83.
&lt;/div&gt;
&lt;div id=&#34;ref-harrison_ranking_2014&#34; class=&#34;csl-entry&#34;&gt;
Harrison, L., F. Yang, S. Franconeri, and R. Chang. 2014. &lt;span&gt;“Ranking Visualizations of Correlation Using &lt;span&gt;W&lt;/span&gt;eber’s &lt;span&gt;L&lt;/span&gt;aw.”&lt;/span&gt; &lt;em&gt;IEEE Conference on Information Visualization (InfoVis)&lt;/em&gt; 20: 1943–52.
&lt;/div&gt;
&lt;div id=&#34;ref-heer_crowdsourcing_2010&#34; class=&#34;csl-entry&#34;&gt;
Heer, J., and M. Bostock. 2010. &lt;span&gt;“Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.”&lt;/span&gt; In &lt;em&gt;ACM Human Factors in Computing Systems&lt;/em&gt;, 203–12. doi:&lt;a href=&#34;https://doi.org/10.1145/1753326.1753357&#34;&gt;10.1145/1753326.1753357&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kay_beyond_2016&#34; class=&#34;csl-entry&#34;&gt;
Kay, M., and J. Heer. 2016. &lt;span&gt;“Beyond &lt;span&gt;W&lt;/span&gt;eber’s &lt;span&gt;L&lt;/span&gt;aw: A Second Look at Ranking Visualizations of Correlation.”&lt;/span&gt; &lt;em&gt;IEEE Trans. Visualization &amp;amp; Comp. Graphics (InfoVis)&lt;/em&gt; 22: 469–78.
&lt;/div&gt;
&lt;div id=&#34;ref-klippel_interpreting_2011&#34; class=&#34;csl-entry&#34;&gt;
Klippel, A., F. Hardisty, and Rui. Li. 2011. &lt;span&gt;“Interpreting Spatial Patterns: &lt;span&gt;A&lt;/span&gt;n Inquiry into Formal and Cognitive Aspects of &lt;span&gt;T&lt;/span&gt;obler’s &lt;span&gt;F&lt;/span&gt;irst &lt;span&gt;L&lt;/span&gt;aw of &lt;span&gt;G&lt;/span&gt;eography.”&lt;/span&gt; &lt;em&gt;Annals of the Association of American Geographers&lt;/em&gt; 101 (5): 1011–31.
&lt;/div&gt;
&lt;div id=&#34;ref-munzner_visualization_2014&#34; class=&#34;csl-entry&#34;&gt;
Munzner, T. 2014. &lt;em&gt;Visualization Analysis and Design&lt;/em&gt;. AK Peters Visualization Series. Boca Raton, FL: CRC Press.
&lt;/div&gt;
&lt;div id=&#34;ref-rensink_perception_2010&#34; class=&#34;csl-entry&#34;&gt;
Rensink, R., and G. Baldridge. 2010. &lt;span&gt;“The Perception of Correlation in Scatterplots.”&lt;/span&gt; &lt;em&gt;Computer Graphics Forum&lt;/em&gt; 29: 1203–10.
&lt;/div&gt;
&lt;div id=&#34;ref-stevens_subjective_1963&#34; class=&#34;csl-entry&#34;&gt;
Stevens, S, and M. Guirao. 1963. &lt;span&gt;“Subjective Scaling of Length and Area and the Matching of Length to Loudness and Brightness.”&lt;/span&gt; &lt;em&gt;Journal of Experimental Psychology&lt;/em&gt; 66 (2): 177–86.
&lt;/div&gt;
&lt;div id=&#34;ref-tufte_visual_1983&#34; class=&#34;csl-entry&#34;&gt;
Tufte, E. 1983. &lt;em&gt;The Visual Display of Quantitative Information&lt;/em&gt;. Cheshire, CT: Graphics Press.
&lt;/div&gt;
&lt;div id=&#34;ref-ware_visual_2008&#34; class=&#34;csl-entry&#34;&gt;
Ware, C. 2008. &lt;em&gt;Visual Thinking for Design&lt;/em&gt;. Waltham, MA: Morgan Kaufman.
&lt;/div&gt;
&lt;div id=&#34;ref-white_symbolization_2017&#34; class=&#34;csl-entry&#34;&gt;
White, T. 2017. &lt;span&gt;“Symbolization and the Visual Variables.”&lt;/span&gt; In &lt;em&gt;He Geographic Information Science &amp;amp; Technology Body of Knowledge&lt;/em&gt;, edited by John P. Wilson.
&lt;/div&gt;
&lt;div id=&#34;ref-wickham_layered_2010&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. 2010. &lt;span&gt;“A Layered Grammar of Graphics.”&lt;/span&gt; &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 19 (1): 3–28.
&lt;/div&gt;
&lt;div id=&#34;ref-wilkinson_grammar_1999&#34; class=&#34;csl-entry&#34;&gt;
Wilkinson, L. 1999. &lt;em&gt;The Grammar of Graphics&lt;/em&gt;. New York: Springer.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I’m suggesting that you reuse the code to calculate party vote share in the &lt;a href=&#34;../class/03-class/#summarise-1&#34;&gt;summarise&lt;/a&gt; code block before piping (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) to the &lt;code&gt;ggplot2&lt;/code&gt; specification.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Again you will reuse and &lt;strong&gt;update&lt;/strong&gt; the code to calculate party vote share in the &lt;a href=&#34;../class/03-class/#summarise-1&#34;&gt;summarise&lt;/a&gt; code block before piping (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) to the &lt;code&gt;ggplot2&lt;/code&gt; specification.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data fundamentals: Describe, wrangle, tidy</title>
      <link>/class/02-class/</link>
      <pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate>
      <guid>/class/02-class/</guid>
      <description>
&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;script src=&#34;../rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;../rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;h2&gt;Contents&lt;/h2&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#session-outcomes&#34;&gt;Session outcomes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concepts&#34;&gt;Concepts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-structure&#34;&gt;Data structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-variable&#34;&gt;Types of variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-observation&#34;&gt;Types of observation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidy-data&#34;&gt;Tidy data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#techniques&#34;&gt;Techniques&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#import&#34;&gt;Import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#describe&#34;&gt;Describe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transform&#34;&gt;Transform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidy&#34;&gt;Tidy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;session-outcomes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session outcomes&lt;/h2&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;knowledge&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Learn&lt;/strong&gt; the vocabulary and concepts used to &lt;strong&gt;describe&lt;/strong&gt; data.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Appreciate&lt;/strong&gt; the characteristics and importance of &lt;strong&gt;tidy data&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wickham_tidy_2014&#34; role=&#34;doc-biblioref&#34;&gt;Wickham 2014&lt;/a&gt;)&lt;/span&gt; for data processing and analysis.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;practical skills&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Load&lt;/strong&gt; flat file datasets in RStudio via querying an API.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Calculate&lt;/strong&gt; descriptive summaries over datasets.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Apply&lt;/strong&gt; high-level functions in &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; for working with data.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Create&lt;/strong&gt; statistical graphics that expose high-level structure in data for cleaning purposes.&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This session covers some of the basics around how to describe and organise data. Whilst this might sound prosaic, there are several reasons why being able to consistently describe a dataset is important. First: it is the initial step in any analysis and helps delimit the research themes and technical procedures that can be deployed. This is especially relevant to modern Data Science-type workflows (like those supported by &lt;code&gt;tidyverse&lt;/code&gt;), where it is common to apply the same analysis templates for working over data. Describing your dataset with a consistent vocabulary enables you to identify which analysis templates to reuse. Second relates to the point in &lt;a href=&#34;../class/01-class/#what-vis-for-gds-1&#34;&gt;session 1&lt;/a&gt; that Geographic Data Science projects usually involve repurposing datasets for social science research for the first time. It is often not obvious whether the data contain sufficient detail and structure to characterise the target behaviours to be researched and the target populations they are assumed to represent. This leads to additional levels of uncertainty and places greater importance on the initial step of data processing, description and exploration.&lt;/p&gt;
&lt;p&gt;&lt;!-- Data will be &#34;messy&#34;, with missing observations, potentially inconsistent structure and levels of precision.
Paying attention to   --&gt;&lt;/p&gt;
&lt;p&gt;Through the session we will learn both language and concepts for describing and thinking about data, but also how to deploy some of the most important data processing and organisation techniques in &lt;code&gt;R&lt;/code&gt; to wrangle over a real dataset. We will be working throughout with data from New York’s &lt;a href=&#34;https://www.citibikenyc.com/&#34;&gt;Citibike&lt;/a&gt; scheme, accessed through the &lt;code&gt;bikedata&lt;/code&gt; package, an API to Citibike’s &lt;a href=&#34;https://www.citibikenyc.com/system-data&#34;&gt;publicly available origin-destination trip data&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The idea of applying a consistent vocabulary to describing your data applies especially to working with modern visualization toolkits (&lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;ggplot2&lt;/a&gt;, &lt;a href=&#34;https://www.tableau.com/en-gb&#34;&gt;Tableau&lt;/a&gt;, &lt;a href=&#34;https://vega.github.io/vega-lite/&#34;&gt;vega-lite&lt;/a&gt;), and will be covered in some detail during the next session as we introduce Visualization Fundamentals and the &lt;a href=&#34;https://www.springer.com/gp/book/9780387245447&#34;&gt;Grammar of Graphics&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concepts&lt;/h2&gt;
&lt;div id=&#34;data-structure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data structure&lt;/h3&gt;
&lt;p&gt;In the module we will work with &lt;a href=&#34;http://adv-r.had.co.nz/Data-structures.html#data-frames&#34;&gt;data frames&lt;/a&gt; in R. These are spreadsheet like representations where rows are &lt;strong&gt;observations&lt;/strong&gt; (case/record) and columns are &lt;strong&gt;variables&lt;/strong&gt;. Each variable (column) in a data frame is a &lt;a href=&#34;http://adv-r.had.co.nz/Data-structures.html#vectors&#34;&gt;vector&lt;/a&gt; that must be of equal length. Where observations have missing values for certain variables – that is, where they may violate this equal-length requirement – the missing values must be substituted with something, usually with &lt;code&gt;NA&lt;/code&gt; or similar. This constraint occasionally causes difficulties, for example when working with variables that contain values of different length for an observation. In these cases we create a special class of column, a &lt;a href=&#34;https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html&#34;&gt;&lt;code&gt;list-column&lt;/code&gt;&lt;/a&gt;, something we’ll return to later in the module.&lt;/p&gt;
&lt;p&gt;Organising data according this simple structure – rows as observations, columns as variables – makes working with data more straightforward. A specific set of tools, made available via the &lt;code&gt;tidyverse&lt;/code&gt;, can be deployed for doing most data &lt;strong&gt;tidy&lt;/strong&gt;ing tasks &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wickham_tidy_2014&#34; role=&#34;doc-biblioref&#34;&gt;Wickham 2014&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;types-of-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Types of variable&lt;/h3&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:variable-types&#34;&gt;Table 1: &lt;/span&gt;A breakdown of &lt;span class=&#34;citation&#34;&gt;Stevens (&lt;a href=&#34;#ref-stevens_on_1946&#34; role=&#34;doc-biblioref&#34;&gt;1946&lt;/a&gt;)&lt;/span&gt; variable types, operators and measures of central tendency that can be applied to each.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Measurement
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Description
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Example
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Operators
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Midpoint
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Dispersion
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 0px solid;&#34;&gt;
Categories
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left; padding-left:  2em;width: 8em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;code&gt;Nominal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Non-orderable categories
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Political parties; street names
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
= ≠
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
mode
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
entropy
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left; padding-left:  2em;width: 8em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;code&gt;Ordinal&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Orderable categories
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://www.gov.uk/terrorism-national-emergency&#34;&gt;Terrorism threat levels&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
… | &amp;lt;&amp;gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
… | median
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
… | percentile
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 0px solid;&#34;&gt;
Measures
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left; padding-left:  2em;width: 8em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;code&gt;Interval&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Numeric measurements
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Temperatures; years
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
… | + -
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
… | mean
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
… | variance
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left; padding-left:  2em;width: 8em; &#34; indentlevel=&#34;1&#34;&gt;
&lt;code&gt;Ratio&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
… | Counts
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Distances; prices
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
… | × ÷
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
… | mean
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
… | variance
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A classification you may have encountered for describing variables is that developed by &lt;span class=&#34;citation&#34;&gt;Stevens (&lt;a href=&#34;#ref-stevens_on_1946&#34; role=&#34;doc-biblioref&#34;&gt;1946&lt;/a&gt;)&lt;/span&gt;, which considers the &lt;em&gt;level of measurement&lt;/em&gt; of a variable. &lt;span class=&#34;citation&#34;&gt;Stevens (&lt;a href=&#34;#ref-stevens_on_1946&#34; role=&#34;doc-biblioref&#34;&gt;1946&lt;/a&gt;)&lt;/span&gt; classed variables into two groups: variables that describe &lt;em&gt;categories&lt;/em&gt; of things and variables that describe &lt;em&gt;measurements&lt;/em&gt; of things. Categories include attributes like gender, titles, &lt;code&gt;Subscribers&lt;/code&gt; or &lt;code&gt;Casual&lt;/code&gt; users of a bikeshare scheme and ranked orders (1st, 2nd, 3rd largest etc.). Measurements include quantities like distance, age, travel time, number of journeys made on a bikeshare scheme.&lt;/p&gt;
&lt;p&gt;Categories can be further subdivided into those that are unordered (&lt;strong&gt;nominal&lt;/strong&gt;) from those that are ordered (&lt;strong&gt;ordinal&lt;/strong&gt;). Measurements can also be subdivided. &lt;strong&gt;Interval&lt;/strong&gt; measurements are quantities that can be ordered and where the difference between two values is meaningful. &lt;strong&gt;Ratio&lt;/strong&gt; measurements have both these properties, but also have a meaningful &lt;code&gt;0&lt;/code&gt; – where &lt;code&gt;0&lt;/code&gt; means the absence of something – and where the ratio of two values can be computed. The most common cited example of an interval measurement is temperature (in degrees C). Temperatures can be ordered and compared additively, but &lt;code&gt;0&lt;/code&gt; degrees C does not mean the absence of temperature and 20 degrees C is not twice as “hot” as 10 degrees C.&lt;/p&gt;
&lt;p&gt;Why is this important? The measurement level of a variable determines the types of data analysis procedures that can be performed and therefore allows us to efficiently make decisions when working with a dataset for the first time (Table &lt;a href=&#34;#tab:variable-types&#34;&gt;1&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;types-of-observation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Types of observation&lt;/h3&gt;
&lt;p&gt;Observations either together form an entire &lt;strong&gt;population&lt;/strong&gt; or a subset, or &lt;strong&gt;sample&lt;/strong&gt; that we expect represents a &lt;strong&gt;target population&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You no doubt will be familiar with these concepts, but we have to think a little more about this in Geographic Data Science applications as we may often be working with datasets that are so-called population-level. The &lt;a href=&#34;(https://www.citibikenyc.com/system-data)&#34;&gt;Citibike dataset&lt;/a&gt; is a complete, population-level dataset in that every journey made through the scheme is recorded. Whether or not this is truly a population-level dataset, however, depends on the analysis purpose. When analysing the bikeshare dataset are we interested &lt;strong&gt;only&lt;/strong&gt; in describing use within the Citibike scheme? Or are we taking the patterns observed through our analysis to make claims and inferences about cycling more generally?&lt;/p&gt;
&lt;p&gt;If the latter, then there are problems as the level of detail we have on our sample is pretty trivial compared to traditional datasets, where we deliberately design data collection activities with a specified target population in mind. It may therefore be difficult to gauge how representative Citibike users and Citibike cycling is of New York’s general cycling population. The flipside is that passively collected data do not suffer from the same problems such as non-response bias and social-desirability bias as traditionally collected datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidy-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tidy data&lt;/h3&gt;
&lt;p&gt;I mentioned that we would be working with data frames organised such that columns always and only refer to variables and rows always and only refer to observations. This arrangement, called &lt;strong&gt;tidy&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wickham_tidy_2014&#34; role=&#34;doc-biblioref&#34;&gt;Wickham 2014&lt;/a&gt;)&lt;/span&gt;, has two key advantages. First, if data are arranged in a consistent way, then it is easier to apply and re-use tools for wrangling them due to data having the same underlying structure. Second, placing variables into columns, with each column containing a vector of values, means that we can take advantage of &lt;code&gt;R&lt;/code&gt;’s vectorised functions for transforming data – we will demonstrate this in the technical element of this session.&lt;/p&gt;
&lt;p&gt;The three rules for tidy data:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Each variable forms a column.&lt;/li&gt;
&lt;li&gt;Each observation forms a row.&lt;/li&gt;
&lt;li&gt;Each type of observational unit forms a table.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;drug-treatment-dataset&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Drug treatment dataset&lt;/h4&gt;
&lt;p&gt;To elaborate further, we can use the example given in &lt;span class=&#34;citation&#34;&gt;Wickham (&lt;a href=&#34;#ref-wickham_tidy_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;, a drug treatment dataset in which two different treatments were administered to participants.&lt;/p&gt;
&lt;p&gt;The data could be represented as:&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:drugs-one&#34;&gt;Table 2: &lt;/span&gt;Table 1 of &lt;span class=&#34;citation&#34;&gt;Wickham (&lt;a href=&#34;#ref-wickham_tidy_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
person
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
treatment_a
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
treatment_b
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
John Smith
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jane Doe
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mary Johnson
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;An alternative organisation could be:&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:drugs-two&#34;&gt;Table 3: &lt;/span&gt;Alternative organisation of Table 1 of &lt;span class=&#34;citation&#34;&gt;Wickham (&lt;a href=&#34;#ref-wickham_tidy_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
treatment
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
John Smith
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Jane Doe
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Mary Johnson
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
treatment_a
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
treatment_b
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Both present the same information unambiguously – Table &lt;a href=&#34;#tab:drugs-two&#34;&gt;3&lt;/a&gt; is simply Table &lt;a href=&#34;#tab:drugs-one&#34;&gt;2&lt;/a&gt; transposed. However, neither is &lt;strong&gt;tidy&lt;/strong&gt; as the observations are spread across both the rows and columns. This means that we need to apply different procedures to extract, perform computations on, and visually represent, these data.&lt;/p&gt;
&lt;p&gt;Much better would be to organise the table into a &lt;strong&gt;tidy&lt;/strong&gt; form. To do this we need to identify the &lt;strong&gt;variables&lt;/strong&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;person&lt;/code&gt;: a categorical nominal variable which takes three values: John Smith, Jane Doe, Mary Johnson.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;treatment&lt;/code&gt;: a categorical nominal variable which takes values: a and b.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;result&lt;/code&gt;: a measurement ratio (I think) variable which six recorded values (including the missing value): -, 16, 3, 2, 11,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each &lt;strong&gt;observation&lt;/strong&gt; is then a test result returned for each combination of &lt;code&gt;person&lt;/code&gt; and &lt;code&gt;treatment&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, a &lt;strong&gt;tidy&lt;/strong&gt; organisation for this dataset would be:&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:drugs-tidy&#34;&gt;Table 4: &lt;/span&gt;Tidy version of Table 1 of &lt;span class=&#34;citation&#34;&gt;Wickham (&lt;a href=&#34;#ref-wickham_tidy_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
person
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
treatment
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
result
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
John Smith
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
John Smith
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
b
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jane Doe
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
16
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jane Doe
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
b
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mary Johnson
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mary Johnson
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
b
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;gapminder-population-dataset&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Gapminder population dataset&lt;/h4&gt;
&lt;p&gt;In &lt;a href=&#34;https://r4ds.had.co.nz/tidy-data.html#tidy-data-1&#34;&gt;chapter 12&lt;/a&gt; of &lt;span class=&#34;citation&#34;&gt;Wickham and Grolemund (&lt;a href=&#34;#ref-wickham_r_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, the benefits of this layout, particularly for working with R, are demonstrated with the &lt;a href=&#34;https://www.gapminder.org/data/&#34;&gt;&lt;code&gt;gapminder&lt;/code&gt;&lt;/a&gt; dataset. I recommend reading this short chapter in full. We will be applying similar approaches in the technique part of this class (which follows shortly) and also the &lt;a href=&#34;&#34;&gt;homework&lt;/a&gt;. To consolidate our conceptual understanding of &lt;strong&gt;tidy&lt;/strong&gt; data let’s quickly look at the &lt;code&gt;gapminder&lt;/code&gt; data, as it is a dataset we’re probably more likely to encounter.&lt;/p&gt;
&lt;p&gt;First, a &lt;strong&gt;tidy&lt;/strong&gt; version of the data:&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:gapminder-tidy&#34;&gt;Table 5: &lt;/span&gt;Tidy excerpt from &lt;code&gt;gapminder&lt;/code&gt; dataset.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
country
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
cases
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
population
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Afghanistan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
745
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
19987071
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Afghanistan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2666
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
20595360
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Brazil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
37737
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
172006362
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Brazil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
80488
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
174504898
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
China
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
212258
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1272915272
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
China
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
213766
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1280428583
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So the &lt;strong&gt;variables&lt;/strong&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;country&lt;/code&gt;: a categorical nominal variable.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;year&lt;/code&gt;: a date (cyclic ratio) variable.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cases&lt;/code&gt;: a ratio (count) variable.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;population&lt;/code&gt;: a ratio (count) variable.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each &lt;strong&gt;observation&lt;/strong&gt; is therefore a recorded count of cases and population for a country in a year.&lt;/p&gt;
&lt;p&gt;An alternative organisation of this dataset that appears in &lt;span class=&#34;citation&#34;&gt;Wickham and Grolemund (&lt;a href=&#34;#ref-wickham_r_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; is below. This is &lt;strong&gt;untidy&lt;/strong&gt; as the observations are spread across two rows. This makes operations that we might want to perform on the &lt;code&gt;cases&lt;/code&gt; and &lt;code&gt;population&lt;/code&gt; variables – for example computing exposure rates – somewhat tedious.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:gapminder-untidy1&#34;&gt;Table 6: &lt;/span&gt;Untidy excerpt of &lt;a href=&#34;https://www.gapminder.org/data/&#34;&gt;&lt;code&gt;gapminder&lt;/code&gt;&lt;/a&gt; dataset: observations spread across rows
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
country
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
count
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Afghanistan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
cases
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
745
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Afghanistan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
population
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
19987071
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Afghanistan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
cases
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2666
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Afghanistan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
population
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
20595360
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Brazil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
cases
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
37737
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Brazil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
population
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
174504898
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
…
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This actually doesn’t appear in &lt;span class=&#34;citation&#34;&gt;Wickham and Grolemund (&lt;a href=&#34;#ref-wickham_r_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, but imagine that the &lt;code&gt;gapminder&lt;/code&gt; dataset instead reported values of &lt;code&gt;cases&lt;/code&gt; separately by gender. A type of representation I’ve often seen in social science domains, probably as it is helpful for data entry, is where observations are spread across the columns. This too creates problems for performing aggregate functions, but also for specifying visualization designs (in &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;&lt;code&gt;ggplot2&lt;/code&gt;&lt;/a&gt;) as we will discover in the next session.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:gapminder-untidy2&#34;&gt;Table 7: &lt;/span&gt;Untidy possible excerpt of &lt;a href=&#34;https://www.gapminder.org/data/&#34;&gt;&lt;code&gt;gapminder&lt;/code&gt;&lt;/a&gt; dataset: observations spread across columns
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
country
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
f_cases
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
m_cases
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
f_population
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
m_population
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Afghanistan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
447
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
298
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
9993400
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
9993671
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Afghanistan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1599
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1067
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
10296280
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
10299080
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Brazil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
16982
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
20755
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
86001181
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
86005181
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Brazil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
39440
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
41048
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
87251329
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
87253569
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
China
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
104007
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
108252
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
636451250
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
636464022
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
China
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
104746
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
109759
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
640212600
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
640215983
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;techniques&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Techniques&lt;/h2&gt;
&lt;p&gt;The technical element to this session involves importing, describing, transforming and tidying data from a large bikeshare scheme – New York’s &lt;a href=&#34;https://www.citibikenyc.com/&#34;&gt;Citibike&lt;/a&gt; scheme.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the &lt;a href=&#34;../homework/02-homework_files/02-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 02-template.Rmd&lt;/a&gt; file for this session and save it to the &lt;code&gt;reports&lt;/code&gt; folder of your &lt;code&gt;vis-for-gds&lt;/code&gt; project that you created in session 1.&lt;/li&gt;
&lt;li&gt;Open your &lt;code&gt;vis-for-gds&lt;/code&gt; project in RStudio and load the template file by clicking &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Open File ...&lt;/code&gt; &amp;gt; &lt;code&gt;reports/02-template.Rmd&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;import&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Import&lt;/h3&gt;
&lt;p&gt;In the template file there is a discussion of how to &lt;strong&gt;setup&lt;/strong&gt; your R session with key packages – &lt;code&gt;tidyverse&lt;/code&gt; , &lt;code&gt;fst&lt;/code&gt;, &lt;code&gt;lubridate&lt;/code&gt;, &lt;code&gt;sf&lt;/code&gt; – and also the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package for accessing bikeshare data.&lt;/p&gt;
&lt;p&gt;Available via the &lt;code&gt;bikedata&lt;/code&gt; package are trip and occupancy data for a number of bikeshare schemes (as below). We will work with data from New York’s &lt;a href=&#34;https://www.citibikenyc.com/&#34;&gt;Citibike&lt;/a&gt; scheme for June 2020. A list of all cities covered by the &lt;code&gt;bikedata&lt;/code&gt; package is below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_cities()
##    city     city_name      bike_system
## 1    bo        Boston           Hubway
## 2    ch       Chicago            Divvy
## 3    dc Washington DC CapitalBikeShare
## 4    gu   Guadalajara           mibici
## 5    la   Los Angeles            Metro
## 6    lo        London        Santander
## 7    mo      Montreal             Bixi
## 8    mn   Minneapolis         NiceRide
## 9    ny      New York         Citibike
## 10   ph  Philadelphia           Indego
## 11   sf      Bay Area       FordGoBike&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the template there are code chunks demonstrating how to download and process these data using &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;bikedata&lt;/a&gt;’s API. This is mainly for illustrative purposes and the code chunks take some time to execute. We ultimately use the &lt;a href=&#34;https://www.fstpackage.org/&#34;&gt;&lt;code&gt;fst&lt;/code&gt;&lt;/a&gt; package for serializing and reading in the these data. So I suggest you ignore the import code and calls to the &lt;code&gt;bikedata&lt;/code&gt; API and instead follow the instructions for downloading and reading in the &lt;code&gt;.fst&lt;/code&gt; file with the trips data and also the &lt;code&gt;.csv&lt;/code&gt; &lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; file containing stations data, with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create subdirectory in data folder for storing bike data.
if(!dir.exists(here(&amp;quot;data&amp;quot;, &amp;quot;bikedata&amp;quot;))) dir.create(here(&amp;quot;data&amp;quot;, &amp;quot;bikedata&amp;quot;))

# Read in .csv file of stations data from url.
tmp_file &amp;lt;- tempfile()
url &amp;lt;- &amp;quot;https://www.roger-beecham.com/datasets/ny_stations.csv&amp;quot;
curl::curl_download(url, tmp_file, mode=&amp;quot;wb&amp;quot;)
ny_stations &amp;lt;- read_csv(tmp_file)

# Read in .fst file of trips data from url.
tmp_file &amp;lt;- tempfile()
cs_url &amp;lt;- &amp;quot;https://www.roger-beecham.com/datasets/ny_trips.fst&amp;quot;
curl::curl_download(url, tmp_file, mode=&amp;quot;wb&amp;quot;)
ny_trips &amp;lt;- read_fst(tmp_file)

# Write out to subdirectory for future use.
write_fst(trips, here(&amp;quot;data&amp;quot;, &amp;quot;ny_trips.fst&amp;quot;))
write_csv(stations, here(&amp;quot;data&amp;quot;, &amp;quot;ny_stations.csv&amp;quot;))

# Clean workspace.
rm(url, tmp_file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;fst&lt;/code&gt; implements in the background various operations such as multi-threading to reduce load on disk space. It therefore makes it possible to work with large datasets in-memory in &lt;code&gt;R&lt;/code&gt; rather than connecting to a database and serving up summaries/subsets to be loaded into R. We will be working with just 2 million records, but with &lt;code&gt;fst&lt;/code&gt; it is possible to work in-memory with much larger datasets – in &lt;span class=&#34;citation&#34;&gt;Lovelace et al. (&lt;a href=&#34;#ref-lovelace_is_2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; we ended up working with 80 million + trip records.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;If you completed the reading and research from the &lt;a href=&#34;../homework/01-homework/&#34;&gt;Session 1 Homework&lt;/a&gt;, some of the above should be familiar to you. The key arguments to look at are &lt;code&gt;read_csv()&lt;/code&gt; and &lt;code&gt;read_fst()&lt;/code&gt;, into which we pass the path to the file. In this case we created a &lt;code&gt;tmpfile()&lt;/code&gt; within the R session. We then write these data out and save locally to the project’s &lt;code&gt;data&lt;/code&gt; folder. This is useful as we only want to download the data once. In the &lt;code&gt;write_*&amp;lt;&amp;gt;&lt;/code&gt; functions we reference this location using the &lt;a href=&#34;https://here.r-lib.org/&#34;&gt;&lt;code&gt;here&lt;/code&gt;&lt;/a&gt; package’s &lt;code&gt;here()&lt;/code&gt; function. &lt;a href=&#34;https://here.r-lib.org/&#34;&gt;&lt;code&gt;here&lt;/code&gt;&lt;/a&gt; is really useful for reliably creating paths relative to your project’s root. To read in these data for future sessions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read in these local copies of the trips and stations data.
ny_trips &amp;lt;- read_fst(here(&amp;quot;data&amp;quot;, &amp;quot;ny_trips.fst&amp;quot;))
ny_stations &amp;lt;- read_csv(here(&amp;quot;data&amp;quot;, &amp;quot;ny_stations.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we use &lt;strong&gt;assignment&lt;/strong&gt; here (&lt;code&gt;&amp;lt;-&lt;/code&gt;) so that these data are loaded as objects and appear in the Environment pane of your RStudio window. An efficient description of data import with &lt;code&gt;read_csv()&lt;/code&gt; is also in &lt;a href=&#34;https://r4ds.had.co.nz/data-import.html&#34;&gt;Chapter 11&lt;/a&gt; of &lt;span class=&#34;citation&#34;&gt;Wickham and Grolemund (&lt;a href=&#34;#ref-wickham_r_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;code&gt;ny_stations&lt;/code&gt; and &lt;code&gt;ny_trips&lt;/code&gt; are data frames, spreadsheet type representations containing observations in rows and variables in columns. Inspecting the layout of the stations data with &lt;code&gt;View(ny_stations)&lt;/code&gt; you will notice that the top line is the header and contains column (variable) names.
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:view-annotate&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../../../../../../../../../class/02-class_files/view.png&#34; alt=&#34;`ny_trips` and `ny_stations` as they appear when calling `View()`.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: &lt;code&gt;ny_trips&lt;/code&gt; and &lt;code&gt;ny_stations&lt;/code&gt; as they appear when calling &lt;code&gt;View()&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;describe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Describe&lt;/h3&gt;
&lt;p&gt;There are several functions for generating a quick overview of a data frame’s contents. &lt;code&gt;glimpse&amp;lt;dataset-name&amp;gt;&lt;/code&gt; is particularly useful. It provides a summary of the data frame dimensions – we have c. 1.9 million trip observations in &lt;code&gt;ny_trips&lt;/code&gt; and 11 variables&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. The function also prints out the object type for each of these variables, with the variables either of type &lt;code&gt;int&lt;/code&gt; or &lt;code&gt;chr&lt;/code&gt; in this case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(ny_trips)
## Rows: 1,882,273
## Columns: 11
## $ id               &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21…
## $ city             &amp;lt;chr&amp;gt; &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;…
## $ trip_duration    &amp;lt;dbl&amp;gt; 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 69…
## $ start_time       &amp;lt;chr&amp;gt; &amp;quot;2020-06-01 00:00:03&amp;quot;, &amp;quot;2020-06-01 00:00:03&amp;quot;, &amp;quot;2020-06-01 00:00:09&amp;quot;, &amp;quot;202…
## $ stop_time        &amp;lt;chr&amp;gt; &amp;quot;2020-06-01 00:17:46&amp;quot;, &amp;quot;2020-06-01 01:03:33&amp;quot;, &amp;quot;2020-06-01 00:17:06&amp;quot;, &amp;quot;202…
## $ start_station_id &amp;lt;chr&amp;gt; &amp;quot;ny3419&amp;quot;, &amp;quot;ny366&amp;quot;, &amp;quot;ny389&amp;quot;, &amp;quot;ny3255&amp;quot;, &amp;quot;ny367&amp;quot;, &amp;quot;ny248&amp;quot;, &amp;quot;ny3232&amp;quot;, &amp;quot;ny3263…
## $ end_station_id   &amp;lt;chr&amp;gt; &amp;quot;ny3419&amp;quot;, &amp;quot;ny336&amp;quot;, &amp;quot;ny3562&amp;quot;, &amp;quot;ny505&amp;quot;, &amp;quot;ny497&amp;quot;, &amp;quot;ny247&amp;quot;, &amp;quot;ny390&amp;quot;, &amp;quot;ny496&amp;quot;,…
## $ bike_id          &amp;lt;chr&amp;gt; &amp;quot;39852&amp;quot;, &amp;quot;37558&amp;quot;, &amp;quot;37512&amp;quot;, &amp;quot;39674&amp;quot;, &amp;quot;21093&amp;quot;, &amp;quot;39594&amp;quot;, &amp;quot;43315&amp;quot;, &amp;quot;16571&amp;quot;, &amp;quot;…
## $ user_type        &amp;lt;chr&amp;gt; &amp;quot;Customer&amp;quot;, &amp;quot;Subscriber&amp;quot;, &amp;quot;Customer&amp;quot;, &amp;quot;Customer&amp;quot;, &amp;quot;Customer&amp;quot;, &amp;quot;Subscriber…
## $ birth_year       &amp;lt;chr&amp;gt; &amp;quot;1997&amp;quot;, &amp;quot;1969&amp;quot;, &amp;quot;1988&amp;quot;, &amp;quot;1969&amp;quot;, &amp;quot;1997&amp;quot;, &amp;quot;1990&amp;quot;, &amp;quot;1938&amp;quot;, &amp;quot;1995&amp;quot;, &amp;quot;1971&amp;quot;, &amp;quot;…
## $ gender           &amp;lt;dbl&amp;gt; 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(ny_stations)
## Rows: 1,010
## Columns: 6
## $ id        &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2…
## $ city      &amp;lt;chr&amp;gt; &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;ny&amp;quot;, &amp;quot;n…
## $ stn_id    &amp;lt;chr&amp;gt; &amp;quot;ny116&amp;quot;, &amp;quot;ny119&amp;quot;, &amp;quot;ny120&amp;quot;, &amp;quot;ny127&amp;quot;, &amp;quot;ny128&amp;quot;, &amp;quot;ny143&amp;quot;, &amp;quot;ny144&amp;quot;, &amp;quot;ny146&amp;quot;, &amp;quot;ny150&amp;quot;,…
## $ name      &amp;lt;chr&amp;gt; &amp;quot;W 17 St &amp;amp; 8 Ave&amp;quot;, &amp;quot;Park Ave &amp;amp; St Edwards St&amp;quot;, &amp;quot;Lexington Ave &amp;amp; Classon Ave&amp;quot;, &amp;quot;B…
## $ longitude &amp;lt;chr&amp;gt; &amp;quot;-74.00149746&amp;quot;, &amp;quot;-73.97803415&amp;quot;, &amp;quot;-73.95928168&amp;quot;, &amp;quot;-74.00674436&amp;quot;, &amp;quot;-74.00297088&amp;quot;, …
## $ latitude  &amp;lt;chr&amp;gt; &amp;quot;40.74177603&amp;quot;, &amp;quot;40.69608941&amp;quot;, &amp;quot;40.68676793&amp;quot;, &amp;quot;40.73172428&amp;quot;, &amp;quot;40.72710258&amp;quot;, &amp;quot;40.6…&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:data-types&#34;&gt;Table 8: &lt;/span&gt;A breakdown of data types in R.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Description
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;lgl&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Logical – vectors that can contain only &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt; values
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;int&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Integers – whole numbers
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;dbl&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Double – real numbers with decimals
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;chr&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Character – text strings
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;dttm&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Date-times – a date + a time
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;fctr&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Factors – represent categorical variables of fixed and potentially orderable values
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The object type of a variable in a data frame relates to that variable’s &lt;em&gt;measurement level&lt;/em&gt;. It is often useful to convert to types with greater specificity. For example, we may which to convert the &lt;code&gt;start_time&lt;/code&gt; and &lt;code&gt;stop_time&lt;/code&gt; variables to a date-time format so that various time-related functions could be used. For efficient storage, we may wish to convert the &lt;em&gt;station identifier&lt;/em&gt; variables as &lt;code&gt;int&lt;/code&gt; types by removing the redundant “ny” text which prefaces &lt;code&gt;end_station_id&lt;/code&gt;, &lt;code&gt;end_station_id&lt;/code&gt;, &lt;code&gt;stn_id&lt;/code&gt;. The geographic coordinates are currently stored as type &lt;code&gt;chr&lt;/code&gt;. These could be regarded as quantitative variables, floating points with decimals. So converting to type &lt;code&gt;dbl&lt;/code&gt; or as a &lt;a href=&#34;https://r-spatial.github.io/sf/articles/sf1.html#simple-feature-geometry-types&#34;&gt;&lt;code&gt;POINT&lt;/code&gt;&lt;/a&gt; geometry type (more on this later in the module) may be sensible.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;../homework/02-homework_files/02-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 02-template.Rmd&lt;/a&gt; file there are code chunks for doing these conversions. There are some slightly more involved data transform procedures in this code. Don’t fixate too much on these, but the upshot can be seen when running &lt;code&gt;glimpse()&lt;/code&gt; on the converted data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(ny_trips)
## Rows: 1,882,273
## Columns: 10
## $ id               &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2…
## $ trip_duration    &amp;lt;dbl&amp;gt; 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 695, 206,…
## $ start_time       &amp;lt;dttm&amp;gt; 2020-06-01 00:00:03, 2020-06-01 00:00:03, 2020-06-01 00:00:09, 2020-06-01 00:00…
## $ stop_time        &amp;lt;dttm&amp;gt; 2020-06-01 00:00:03, 2020-06-01 00:00:03, 2020-06-01 00:00:09, 2020-06-01 00:00…
## $ start_station_id &amp;lt;int&amp;gt; 3419, 366, 389, 3255, 367, 248, 3232, 3263, 390, 319, 237, 3630, 3610, 3708, 465…
## $ end_station_id   &amp;lt;int&amp;gt; 3419, 336, 3562, 505, 497, 247, 390, 496, 3232, 455, 3263, 3630, 3523, 3740, 379…
## $ bike_id          &amp;lt;int&amp;gt; 39852, 37558, 37512, 39674, 21093, 39594, 43315, 16571, 28205, 41760, 30745, 380…
## $ user_type        &amp;lt;chr&amp;gt; &amp;quot;Customer&amp;quot;, &amp;quot;Subscriber&amp;quot;, &amp;quot;Customer&amp;quot;, &amp;quot;Customer&amp;quot;, &amp;quot;Customer&amp;quot;, &amp;quot;Subscriber&amp;quot;, &amp;quot;Sub…
## $ birth_year       &amp;lt;int&amp;gt; 1997, 1969, 1988, 1969, 1997, 1990, 1938, 1995, 1971, 1989, 1990, 1969, 1984, 19…
## $ gender           &amp;lt;chr&amp;gt; &amp;quot;female&amp;quot;, &amp;quot;unknown&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;unknown&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, …&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(ny_stations)
## Rows: 1,010
## Columns: 5
## $ id        &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, …
## $ stn_id    &amp;lt;int&amp;gt; 116, 119, 120, 127, 128, 143, 144, 146, 150, 151, 157, 161, 164, 167, 168, 173, 174, 19…
## $ name      &amp;lt;chr&amp;gt; &amp;quot;W 17 St &amp;amp; 8 Ave&amp;quot;, &amp;quot;Park Ave &amp;amp; St Edwards St&amp;quot;, &amp;quot;Lexington Ave &amp;amp; Classon Ave&amp;quot;, &amp;quot;Barrow S…
## $ longitude &amp;lt;dbl&amp;gt; -74.00150, -73.97803, -73.95928, -74.00674, -74.00297, -73.99338, -73.98069, -74.00911,…
## $ latitude  &amp;lt;dbl&amp;gt; 40.74178, 40.69609, 40.68677, 40.73172, 40.72710, 40.69240, 40.69840, 40.71625, 40.7208…&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;transform&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Transform&lt;/h3&gt;
&lt;div id=&#34;transform-with-dplyr&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Transform with &lt;code&gt;dplyr&lt;/code&gt;&lt;/h4&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:dplyr-verbs&#34;&gt;Table 9: &lt;/span&gt;dplyr funcitions (verbs) for manipulating data frames.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
function()
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Description
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;filter()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Picks rows (observations) if their values match a specified criteria
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;arrange()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Reorders rows (observations) based on their values
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;select()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Picks a subset of columns (variables) by name (or name characteristics)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;rename()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Changes the name of columns in the data frame
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;mutate()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adds new columns (or variables)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;group_by()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Chunks the dataset into groups for grouped operations
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;summarise()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Calculate single-row (non-grouped) or multiple-row (if grouped) summary values
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;..and more&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a href=&#34;https://dplyr.tidyverse.org/&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt;&lt;/a&gt; is one of the most important packages for supporting modern data analysis workflows. The package provides a &lt;strong&gt;grammar of data manipulation&lt;/strong&gt;, with access to functions that can be variously combined to support most data processing and transformation activity. Once you become familiar with &lt;code&gt;dplyr&lt;/code&gt; functions (or &lt;strong&gt;verbs&lt;/strong&gt;) you will find yourself generating analysis templates to re-use whenever you work on a new dataset.&lt;/p&gt;
&lt;p&gt;All &lt;code&gt;dplyr&lt;/code&gt; functions work in the same way:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start with a data frame.&lt;/li&gt;
&lt;li&gt;Pass some arguments to the function which control what you do to the data frame.&lt;/li&gt;
&lt;li&gt;Return the updated data frame.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So every &lt;code&gt;dplyr&lt;/code&gt; function expects a data frame and will always return a data frame.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-pipes-with-dplyr&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Use pipes &lt;code&gt;%&amp;gt;%&lt;/code&gt; with &lt;code&gt;dplyr&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;dplyr&lt;/code&gt; is most effective when its functions are chained together – you will see this shortly as we explore the New York bikeshare data. This chaining of functions can be achieved using the &lt;strong&gt;pipe&lt;/strong&gt; operator (&lt;code&gt;%&amp;gt;%&lt;/code&gt;). Pipes are used for passing information in a program. They take the output of a set of code (a &lt;code&gt;dplyr&lt;/code&gt; specification) and make it the input of the next set (another &lt;code&gt;dplyr&lt;/code&gt; specification).&lt;/p&gt;
&lt;p&gt;Pipes can be easily applied to &lt;code&gt;dplyr&lt;/code&gt; functions, and the functions of all packages that form the &lt;code&gt;tidyverse&lt;/code&gt;. I mentioned in &lt;a href=&#34;../class/01-class&#34;&gt;Session 1&lt;/a&gt; that &lt;code&gt;ggplot2&lt;/code&gt; provides a framework for specifying a &lt;strong&gt;layered grammar of graphics&lt;/strong&gt; (more on this in Session 3). Together with the pipe operator (&lt;code&gt;%&amp;gt;%&lt;/code&gt;), &lt;code&gt;dplyr&lt;/code&gt; supports a &lt;strong&gt;layered grammar of data manipulation&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;count-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;code&gt;count()&lt;/code&gt; rows&lt;/h4&gt;
&lt;p&gt;This might sound a little abstract so let’s use and combine some &lt;code&gt;dplyr&lt;/code&gt; functions to generate some statistical summaries on the New York bikeshare data.&lt;/p&gt;
&lt;p&gt;First we’ll count the number of trips made in Jun 2020 by gender. &lt;code&gt;dplyr&lt;/code&gt; has a convenience function for counting, so we could run the code below, also in the &lt;a href=&#34;../homework/02-homework_files/02-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 02-template.Rmd&lt;/a&gt; for this session. I’ve commented the code block to convey what each line achieves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ny_trips %&amp;gt;%  # Take the ny_trips data frame.
  count(gender, sort=TRUE) # Run the count function over the data frame and set the sort parameter to TRUE.
##    gender       n
## 1    male 1044621
## 2  female  586361
## 3 unknown  251291&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a few things happening in the &lt;code&gt;count()&lt;/code&gt; function. It takes the &lt;code&gt;gender&lt;/code&gt; variable from &lt;code&gt;ny_trips&lt;/code&gt;, organises or &lt;em&gt;groups&lt;/em&gt; the rows in the data frame according to its values (&lt;code&gt;female&lt;/code&gt; | &lt;code&gt;male&lt;/code&gt; | &lt;code&gt;unknown&lt;/code&gt;), counts the rows and then orders the &lt;em&gt;summarised&lt;/em&gt; output descending on the counts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summarise-over-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;code&gt;summarise()&lt;/code&gt; over rows&lt;/h4&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:aggregate-functions&#34;&gt;Table 10: &lt;/span&gt;A breakdown of aggregate functions commonly used with &lt;code&gt;summarise()&lt;/code&gt;.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Function
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Description
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;n()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Counts the number of observations
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;n_distinct(var)&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Counts the number of unique observations
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;sum(var)&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sums the values of observations
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;max(var)&lt;/code&gt;|&lt;code&gt;min(var)&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Finds the min|max values of observations
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;mean(var)&lt;/code&gt;|&lt;code&gt;median(var)&lt;/code&gt;|&lt;code&gt;sd(var)&lt;/code&gt;| &lt;code&gt;...&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Calculates central tendency of observations
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;code&gt;...&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Many more
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Often you will want to do more than simply counting and you may also want to be more explicit in the way the data frame is &lt;em&gt;grouped&lt;/em&gt; for computation. We’ll demonstrate this here with a more involved analysis of the usage data and using some key aggregate functions (Table &lt;a href=&#34;#tab:aggregate-functions&#34;&gt;10&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;A common workflow is to combine &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;summarise()&lt;/code&gt;, and in this case &lt;code&gt;arrange()&lt;/code&gt; to replicate the &lt;code&gt;count()&lt;/code&gt; example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ny_trips %&amp;gt;% # Take the ny_trips data frame.
  group_by(gender) %&amp;gt;% # Group by gender.
  summarise(count=n()) %&amp;gt;% # Count the number of observations per group.
  arrange(desc(count)) # Arrange the grouped and summarised (collapsed) rows according to count.
## # A tibble: 3 x 2
##  gender    count
##  &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1 male    1044621
## 2 female   586361
## 3 unknown  251291&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;ny_trips&lt;/code&gt; there is a variable measuring trip duration in seconds (&lt;code&gt;trip_duration&lt;/code&gt;) and distinguishing casual users from those formally registered to use the scheme (&lt;code&gt;user_type&lt;/code&gt; - &lt;code&gt;Customer&lt;/code&gt; vs. &lt;code&gt;Subscriber&lt;/code&gt;). It may be instructive to calculate some summary statistics to see how trip duration varies between these groups.&lt;/p&gt;
&lt;p&gt;The code below uses &lt;code&gt;group_by()&lt;/code&gt;, &lt;code&gt;summarise()&lt;/code&gt; and &lt;code&gt;arrange()&lt;/code&gt; in exactly the same way, but with the addition of other aggregate functions profiles the &lt;code&gt;trip_duration&lt;/code&gt; variable according to central tendency and by &lt;code&gt;user_type&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ny_trips %&amp;gt;% # Take the ny_trips data frame.
  group_by(user_type) %&amp;gt;% # Group by user type.
  summarise( # Summarise over the grouped rows, generate a new variable for each type of summary.
    count=n(),
    avg_duration=mean(trip_duration/60),
    median_duration=median(trip_duration/60),
    sd_duration=sd(trip_duration/60),
    min_duration=min(trip_duration/60),
    max_duration=max(trip_duration/60)
    ) %&amp;gt;%
  arrange(desc(count)) # Arrange on the count variable.

## # A tibble: 2 x 6
##  user_type    count avg_duration median_duration sd_duration min_duration max_duration
##  &amp;lt;chr&amp;gt;        &amp;lt;int&amp;gt;        &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 Subscriber 1306688         20.2            14.4        110.         1.02         33090
## 2 Customer    575585         43.6            23.2        393.         1.02         46982&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly there are some outlier trips that may need to be examined. Bikeshare schemes are built to incentivise short journeys of &amp;lt;30 minutes, but the maximum trip duration recorded above is clearly erroneous – 32 days. Ignoring these sorts of outliers by calculating the trip durations at the 95th percentiles is instructive. The max trip duration at the 95th percentile for &lt;code&gt;Subscribers&lt;/code&gt; was almost 27 minutes and for &lt;code&gt;Customers&lt;/code&gt; was 1 hours 26 mins. It makes sense that more casual users may have longer trip durations, as they are more likely to be tourists or occasional cyclists using the scheme for non-utility trips. However, they do skew the mean travel time.&lt;/p&gt;
&lt;p&gt;Returning to the breakdown of usage by gender, an interesting question is whether or not the male-female split in bikehare is similar to that of the cycling population of New York City as a whole. This might tell us something about whether the bikeshare scheme could be &lt;em&gt;representative&lt;/em&gt; of wider cycling. This could be achieved with the code below. A couple of new additions: we use &lt;code&gt;filter()&lt;/code&gt;, to remove observations where the gender of the cyclist is &lt;code&gt;unknown&lt;/code&gt;. We also use &lt;code&gt;mutate()&lt;/code&gt; for the first time, which allows us to modify or create new variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ny_trips %&amp;gt;% # Take the ny_trips data frame.
  filter(gender != &amp;quot;unknown&amp;quot;) %&amp;gt;% # Filter out rows with the value &amp;quot;unknown&amp;quot; on gender.
  group_by(gender) %&amp;gt;% # Group by gender.
  summarise(count=n()) %&amp;gt;% # Count the number of observations per group.
  mutate(prop=count/sum(count)) %&amp;gt;% # Add a new column called `prop`, divide the value in the row for the variable count by the sum of the count variable across all rows.
  arrange(desc(count)) # Arrange on the count variable.

## # A tibble: 2 x 3
##  gender   count  prop
##  &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1 male   1044621 0.640
## 2 female  586361 0.360&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As I’ve commented each line you hopefully get a sense of what is happening in the code above. I mentioned that &lt;code&gt;dplyr&lt;/code&gt; functions read like &lt;strong&gt;verbs&lt;/strong&gt;. This is a very deliberate design decision. With the code laid out as above – each &lt;code&gt;dplyr&lt;/code&gt; verb occupying a single line, separated by a pipe (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) – you can generally understand the code with a cursory glance. There are obvious benefits to this. Once you become familiar with &lt;code&gt;dplyr&lt;/code&gt; it becomes very easy to read, write and share code.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Remembering that &lt;strong&gt;pipes&lt;/strong&gt; (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) take the output of a set of code and make it the input of the next set, what do you think would happen if you were to comment out the call to &lt;code&gt;arrange()&lt;/code&gt; in the code block above? Try it for yourself. You will notice that I use separate lines for each call to the &lt;strong&gt;pipe&lt;/strong&gt; operator. This is good practice for supporting readibility of your code.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;manipulate-dates-with-lubridate&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Manipulate dates with &lt;a href=&#34;https://lubridate.tidyverse.org/&#34;&gt;&lt;code&gt;lubridate&lt;/code&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Let’s continue this investigation of usage by gender, and whether bikeshare might be representative of regular cycling, by profiling how usage varies over time. To do this we will need to work with &lt;code&gt;date-time&lt;/code&gt; variables. The &lt;a href=&#34;https://lubridate.tidyverse.org/&#34;&gt;&lt;code&gt;lubridate&lt;/code&gt;&lt;/a&gt; package provides various convenience functions for this.&lt;/p&gt;
&lt;p&gt;In the code block below we extract the &lt;em&gt;day of week&lt;/em&gt; and &lt;em&gt;hour of day&lt;/em&gt; from the &lt;code&gt;start_time&lt;/code&gt; variable using &lt;code&gt;lubridate&lt;/code&gt;’s &lt;a href=&#34;https://lubridate.tidyverse.org/reference/day.html&#34;&gt;day accessor&lt;/a&gt; functions. Documentation on these can be accessed in the usual way (&lt;code&gt;?&amp;lt;function-name&amp;gt;&lt;/code&gt;), but reading down the code it should be clear to you how this works. Next we count the number of trips made by hour of day, day of week and gender. The summarised data frame will be re-used several times in our analysis, so we store it as an object with a suitable name (&lt;code&gt;ny_temporal&lt;/code&gt;) using the assignment operator.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a hod dow summary by gender and assign it the name &amp;quot;ny_temporal&amp;quot;.
ny_temporal &amp;lt;- ny_trips %&amp;gt;%  # Take the ny_trips data frame.
  mutate(
    day=wday(start_time, label=TRUE), # Create a new column identify dow.
    hour=hour(start_time)) %&amp;gt;% # Create a new column identify hod.
  group_by(gender, day, hour) %&amp;gt;% # Group by day, hour, gender.
  summarise(count=n()) %&amp;gt;% # Count the grouped rows.
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Whether or not to store derived data tables, like the newly assigned &lt;code&gt;ny_temporal&lt;/code&gt;, in a session is not an easy decision. You want to try to avoid cluttering your Environment pane with many data objects. Often when generating charts it is necessary to create these sorts of derived tables as input data (to &lt;code&gt;ggplot2&lt;/code&gt;) – and so when doing visual data analysis you may end up with an unhelpfully large number of these derived tables. The general rule I apply: if the derived table is to be used &amp;gt;3 times in a data analysis or is computationally intensive, assign it (&lt;code&gt;&amp;lt;-&lt;/code&gt;) to an object.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;In Figure &lt;a href=&#34;#fig:plot-temporal&#34;&gt;2&lt;/a&gt; below these derived data are plotted. The template contains &lt;code&gt;ggplot2&lt;/code&gt; code for creating the graphic. Don’t obsess too much on it – more on this next session. The plot demonstrates a familiar weekday-weekend pattern of usage. Trip frequencies peak in the morning and evening rush hours during weekdays and mid/late-morning and afternoon during weekends. This is consistent with typical travel behaviour. Notice though that the weekday afternoon peak is much larger than the morning peak. There are several speculative explanations for this and re-running the plot on &lt;code&gt;Subscriber&lt;/code&gt; users only may be instructive. A secondary observation is that whilst men and women share this overall pattern of usage, the relative number of trips taken by each day of week varies. Men make many more trips at peak times during the start of the week than they do later in the week. The same pattern does not appear for women. This is certainly something to follow up on, for example by collecting data over a longer period of time.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:plot-temporal&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../../../../../../../../../class/02-class_files/hod_dow.png&#34; alt=&#34;Line charts generated with `ggplot2`. Plot data computed using `dplyr` and `lubridate`.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Line charts generated with &lt;code&gt;ggplot2&lt;/code&gt;. Plot data computed using &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;lubridate&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Our analysis is based on data from June 2020, a time when New York residents were emerging from lockdown. It would be instructive to compare with data from a non-Covid year. If there is a very clear contrast in usage between this data and a control (non-Covid) year, this suggests bikeshare data may be used for monitoring &lt;em&gt;behavioural change&lt;/em&gt;. The fact that bikeshare is collected continuously makes this possible. Check out &lt;a href=&#34;https://github.com/jwoLondon/mobv&#34;&gt;Jo Wood’s current work&lt;/a&gt; analysing Covid-related change in movement behaviours across a range of cities.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;relate-tables-with-join&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Relate tables with &lt;code&gt;join()&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Trip distance is not recorded directly in the &lt;code&gt;ny_trips&lt;/code&gt; table, but may be important for profiling usage behaviour. Calculating trip distance is eminently achievable as the &lt;code&gt;ny_trips&lt;/code&gt; table contains the origin and destination station of every trip and the &lt;code&gt;ny_stations&lt;/code&gt; table contains coordinates corresponding to those stations. To relate the two tables, we need to specify a &lt;strong&gt;join&lt;/strong&gt; between them.&lt;/p&gt;
&lt;p&gt;A sensible approach is to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Select all uniquely cycled trip pairs (origin-destination pairs) that appear in the &lt;code&gt;ny_trips&lt;/code&gt; table.&lt;/li&gt;
&lt;li&gt;Bring in the corresponding coordinate pairs representing the origin and destination stations by joining on the &lt;code&gt;ny_stations&lt;/code&gt; table.&lt;/li&gt;
&lt;li&gt;Calculate the distance between the coordinate pairs representing the origin and destination.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code below is one way of achieving this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od_pairs &amp;lt;- ny_trips %&amp;gt;% # Take the ny_trips data frame.
select(start_station_id, end_station_id) %&amp;gt;% unique() %&amp;gt;% # Select trip origin and destination (OD) station columns and extract unique OD pairs.
  left_join(ny_stations %&amp;gt;% select(stn_id, longitude, latitude), by=c(&amp;quot;start_station_id&amp;quot;=&amp;quot;stn_id&amp;quot;)) %&amp;gt;% # Select lat, lon columns from ny_stations and join on the origin column.
  rename(o_lon=longitude, o_lat=latitude) %&amp;gt;% # Rename new lat, lon columns -- associate with origin station.
  left_join(ny_stations %&amp;gt;% select(stn_id, longitude, latitude), by=c(&amp;quot;end_station_id&amp;quot;=&amp;quot;stn_id&amp;quot;)) %&amp;gt;% # Select lat, lon columns from ny_stations and join on the destination column.
  rename(d_lon=longitude, d_lat=latitude) %&amp;gt;%  # Rename new lat, lon columns -- associate with destination station.
  rowwise() %&amp;gt;% # For computing distance calculation one row-at-a-time.
  mutate(dist=geosphere::distHaversine(c(o_lat, o_lon), c(d_lat, d_lon))/1000) %&amp;gt;% # Calculate distance and express in kms.
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code block above introduces some new functions: &lt;code&gt;select()&lt;/code&gt; to pick or drop variables, &lt;code&gt;rename()&lt;/code&gt; to rename variables and a convenience function for calculating straight line distance from polar coordinates (&lt;code&gt;distHaversine()&lt;/code&gt;). The key function to emphasise is the &lt;code&gt;left_join()&lt;/code&gt;. If you’ve worked with relational databases and &lt;code&gt;SQL&lt;/code&gt;, &lt;code&gt;dplyr&lt;/code&gt;’s join functions will be familiar to you. In a &lt;code&gt;left_join&lt;/code&gt;, all the values from the main table are retained, the one on the left – &lt;code&gt;ny_trips&lt;/code&gt;, and variables from the table on the right (&lt;code&gt;ny_stations&lt;/code&gt;) are added. We specify explicitly the variable on which the tables should be joined with the &lt;code&gt;by=&lt;/code&gt; parameter, &lt;code&gt;station_id&lt;/code&gt; in this case. If there is a &lt;code&gt;station_id&lt;/code&gt; in &lt;code&gt;ny_trips&lt;/code&gt; that doesn’t exist in &lt;code&gt;ny_stations&lt;/code&gt; then &lt;code&gt;NA&lt;/code&gt; is returned.&lt;/p&gt;
&lt;p&gt;Other &lt;strong&gt;join&lt;/strong&gt; functions provided by &lt;code&gt;dplyr&lt;/code&gt; are in the table below. Rather than discussing each, I recommend consulting &lt;a href=&#34;https://r4ds.had.co.nz/relational-data.html&#34;&gt;Chapter 13&lt;/a&gt; of &lt;span class=&#34;citation&#34;&gt;Wickham and Grolemund (&lt;a href=&#34;#ref-wickham_r_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;1&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
&lt;code&gt;*_join(x, y) ...&lt;/code&gt;
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:table-joins&#34;&gt;Table 11: &lt;/span&gt;A breakdown of &lt;code&gt;dplyr&lt;/code&gt; join functions.
&lt;/caption&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em; &#34;&gt;
&lt;code&gt;left_join()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
all rows from x
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em; &#34;&gt;
&lt;code&gt;right_join()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
all rows from y
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em; &#34;&gt;
&lt;code&gt;full_join()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
all rows from both x and y
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em; &#34;&gt;
&lt;code&gt;semi_join()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
all rows from x where there are matching values in y, keeping just columns from x
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em; &#34;&gt;
&lt;code&gt;inner_join()&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
all rows from x where there are matching values in y, return all combination of multiple matches in the case of multiple matches
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em; &#34;&gt;
&lt;code&gt;anti_join&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
return all rows from x where there are not matching values in y, never duplicate rows of x
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:plot-dist&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../../../../../../../../../class/02-class_files/dist.png&#34; alt=&#34;Histograms generated with `ggplot2`. Plot data computed using `dplyr` and `lubridate`&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Histograms generated with &lt;code&gt;ggplot2&lt;/code&gt;. Plot data computed using &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;lubridate&lt;/code&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From the newly created distance variable, we can calculate the average (mean) trip distance for the 1.9m trips – 1.6km. This might seem very short, but remember that the distance calculation is problematic in that these are straight-line distances between pairs of docking stations. Really we should be calculating network distances derived from the cycle network in New York. A separate reason – discovered when generating a histogram on the &lt;code&gt;dist&lt;/code&gt; variable – is that there are a large number of trips (124,403) that start and end at the same docking station. Initially these might seem to be unsuccessful hires – people failing to undock a bike for example. We could investigate this further by paying attention to the docking stations at which same origin-destination trips occur, as in the code block below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ny_trips %&amp;gt;%
  filter(start_station_id==end_station_id) %&amp;gt;%
  group_by(start_station_id) %&amp;gt;% summarise(count=n()) %&amp;gt;%
  left_join(ny_stations %&amp;gt;%  select(stn_id, name), by=c(&amp;quot;start_station_id&amp;quot;=&amp;quot;stn_id&amp;quot;)) %&amp;gt;%
  arrange(desc(count))

## # A tibble: 958 x 3
##    start_station_id count name
##    &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;
##  1 ny3423            2017 West Drive &amp;amp; Prospect Park West
##  2 ny3881            1263 12 Ave &amp;amp; W 125 St
##  3 ny514             1024 12 Ave &amp;amp; W 40 St
##  4 ny3349             978 Grand Army Plaza &amp;amp; Plaza St West
##  5 ny3992             964 W 169 St &amp;amp; Fort Washington Ave
##  6 ny3374             860 Central Park North &amp;amp; Adam Clayton Powell Blvd
##  7 ny3782             837 Brooklyn Bridge Park - Pier 2
##  8 ny3599             829 Franklin Ave &amp;amp; Empire Blvd
##  9 ny3521             793 Lenox Ave &amp;amp; W 111 St
## 10 ny2006             782 Central Park S &amp;amp; 6 Ave
## # … with 948 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of the top 10 docking stations are either in parks, near parks or located along river promenades. This coupled with the fact that these trips occur in much greater relative number for casual than regular users (&lt;code&gt;Customer&lt;/code&gt; vs &lt;code&gt;Subscriber&lt;/code&gt;) is further evidence that these are valid trips.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-functions-of-your-own&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Write functions of your own&lt;/h4&gt;
&lt;p&gt;Through most of the module we will be making use of functions written by others – mainly those developed for packages that form the &lt;code&gt;tidyverse&lt;/code&gt; and therefore that follow a consistent syntax. However, there may be times where you need to abstract over some of your code to make functions of your own. &lt;a href=&#34;https://r4ds.had.co.nz/functions.html&#34;&gt;Chapter 19&lt;/a&gt; of &lt;span class=&#34;citation&#34;&gt;Wickham and Grolemund (&lt;a href=&#34;#ref-wickham_r_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; presents some helpful guidelines around the circumstances under which the data scientist typically tends to write functions. Most often this is when you find yourself copy and pasting the same chunks of code with minimal adaptation.&lt;/p&gt;
&lt;p&gt;Functions have three key characteristics:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;They are (usually) &lt;strong&gt;named&lt;/strong&gt; – the name should be expressive and communicate what the function does (we talk about &lt;code&gt;dplyr&lt;/code&gt; &lt;strong&gt;verbs&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;They have brackets &lt;code&gt;&amp;lt;function()&amp;gt;&lt;/code&gt; usually containing &lt;strong&gt;arguments&lt;/strong&gt; – inputs which determine what the function does and returns.&lt;/li&gt;
&lt;li&gt;Immediately followed by &lt;code&gt;&amp;lt;function()&amp;gt;&lt;/code&gt; are &lt;code&gt;{}&lt;/code&gt; used to contain the &lt;strong&gt;body&lt;/strong&gt; – in this is code that performs a distinct task, described by the function’s &lt;strong&gt;name&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Effective functions are &lt;strong&gt;short&lt;/strong&gt;, perform &lt;strong&gt;single&lt;/strong&gt; discrete operations and are &lt;strong&gt;intuitive&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You will recall that in the &lt;code&gt;ny_trips&lt;/code&gt; table there is a variable called &lt;code&gt;birth_year&lt;/code&gt;. From this we can derive cyclists’ approximate age. Below I have written a function &lt;code&gt;get_age()&lt;/code&gt; for doing this. The function expects two &lt;strong&gt;arguments&lt;/strong&gt;: &lt;code&gt;yob&lt;/code&gt; – a year of birth as type &lt;code&gt;chr&lt;/code&gt;; &lt;code&gt;yref&lt;/code&gt; – a reference year. In the &lt;strong&gt;body&lt;/strong&gt;, &lt;code&gt;lubridate&lt;/code&gt;’s &lt;code&gt;as.period&lt;/code&gt; function is used to calculate the time in years that elapsed, the value that the function &lt;strong&gt;returns&lt;/strong&gt;. Once defined, and loaded into the session by being executed, it can be used (as below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function for calculating time elapsed between two dates in years (age).
get_age &amp;lt;- function(yob, yref) {
    period &amp;lt;- lubridate::as.period(lubridate::interval(yob, yref),unit = &amp;quot;year&amp;quot;)
    return(period$year)
}

ny_trips &amp;lt;- ny_trips %&amp;gt;% # Take the ny_trips data frame.
  mutate(
    age=get_age(as.POSIXct(birth_year, format=&amp;quot;%Y&amp;quot;), as.POSIXct(&amp;quot;2020&amp;quot;, format=&amp;quot;%Y&amp;quot;)) # Calculate age from birth_date.
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the two new derived variables – distance travelled and age – in our analysis. In Figure &lt;a href=&#34;#fig:plot-speeds&#34;&gt;4&lt;/a&gt;, we explore how &lt;strong&gt;approximate&lt;/strong&gt; travel speeds vary by age, gender and trip distance. The code used to generate the summary data and plot is in the template file. Again the average “speed” calculation should be treated very cautiously as it is based on straight line distances and it is very difficult to select out “utility” from “leisure” trips. I have tried to do the latter by selecting trips that occur only on weekdays and that are made by &lt;code&gt;Subscriber&lt;/code&gt; cyclists. Additionally, due to the heavy subsetting data become a little volatile for certain age groups and so I’ve aggregated the age variable into 5-year bands. Collecting more data is probably a good idea.&lt;/p&gt;
&lt;p&gt;There are nevertheless some interesting patterns. Men tend to cycle at faster speeds than do women, although this gap narrows with the older age groups. The effect of age on speed cycled is more apparent for the longer trips. This trend is reasonably strong, although the volatility in the older age groups for trips &amp;gt;4.5km suggests we probably need more data and a more involved analysis to establish this. For example, it may be that the comparatively rare occurrence of trips in the 65-70 age group is made by only a small subset of cyclists. A larger dataset would result in a regression to the mean effect and negate any noise caused by outlier individuals. Certainly Figure &lt;a href=&#34;#tab:variable-types&#34;&gt;1&lt;/a&gt; is an interesting data graphic – and the type of exploratory analysis demonstrated here, using &lt;code&gt;dplyr&lt;/code&gt; functions, is most definitely consistent with that identified in the previous session when introducing &lt;em&gt;Geographic Data Science&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:plot-speeds&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../../../../../../../../../class/02-class_files/speeds.png&#34; alt=&#34;Line charts generated with `ggplot2`. Plot data computed using `dplyr` and `lubridate`&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Line charts generated with &lt;code&gt;ggplot2&lt;/code&gt;. Plot data computed using &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;lubridate&lt;/code&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tidy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tidy&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;ny_trips&lt;/code&gt; and &lt;code&gt;ny_stations&lt;/code&gt; data already comply with the rules for &lt;strong&gt;tidy&lt;/strong&gt; data &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wickham_tidy_2014&#34; role=&#34;doc-biblioref&#34;&gt;Wickham 2014&lt;/a&gt;)&lt;/span&gt;. Each row in &lt;code&gt;ny_trips&lt;/code&gt; is a distinct trip and each row in &lt;code&gt;ny_stations&lt;/code&gt; a distinct station. However throughout the module we will undoubtedly encounter datasets that need to be reshaped. There are two key functions to learn here, made available via the &lt;a href=&#34;https://tidyr.tidyverse.org/articles/pivot.html&#34;&gt;&lt;code&gt;tidyr&lt;/code&gt;&lt;/a&gt; package: &lt;a href=&#34;https://tidyr.tidyverse.org/reference/pivot_longer.html&#34;&gt;&lt;code&gt;pivot_longer()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://tidyr.tidyverse.org/reference/pivot_wider.html&#34;&gt;&lt;code&gt;pivot_wider()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;pivot_longer()&lt;/code&gt; is used to tidy data in which observations are spread across columns, as in Table &lt;a href=&#34;#tab:gapminder-untidy1&#34;&gt;6&lt;/a&gt; (the &lt;code&gt;gapminder&lt;/code&gt; dataset). &lt;code&gt;pivot_wider()&lt;/code&gt; is used to tidy data in which observations are spread across rows, as in Table &lt;a href=&#34;#tab:gapminder-untidy2&#34;&gt;7&lt;/a&gt; (the &lt;code&gt;gapminder&lt;/code&gt; dataset). You will find yourself using these functions, particularly &lt;code&gt;pivot_longer()&lt;/code&gt;, not only for fixing messy data, but for flexibly reshaping data for use in &lt;code&gt;ggplot2&lt;/code&gt; specifications (more on this in sessions 3 and 4) or joining tables.&lt;/p&gt;
&lt;p&gt;A quick breakdown of &lt;code&gt;pivot_longer&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pivot_longer(
  data,
  cols, # Columns to pivot longer (across rows).
  names_to=&amp;quot;name&amp;quot;, # Name of the column to create from values held in spread *column names*.
  values_to=&amp;quot;name&amp;quot; # Name of column to create form values stored in spread *cells*
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quick breakdown of &lt;code&gt;pivot_wider&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pivot_wider(
  data,
  names_from, # Column in the long format which contains what will be column names in the wide format.
  values_from # Column in the long format which contains what will be values in the new wide format.
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the homework you will be tidying some messy derived tables based on the bikeshare data using both of these functions, but we can demonstrate their purpose in &lt;strong&gt;tidying&lt;/strong&gt; the messy &lt;code&gt;gapminder&lt;/code&gt; data in Table &lt;a href=&#34;#tab:gapminder-untidy2&#34;&gt;7&lt;/a&gt;. Remember that these data were messy as the observations by gender were spread across the columns:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;untidy_wide
##   country     year  f_cases m_cases f_population m_population
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;
## 1 Afghanistan 1999  447     298     9993400      9993671
## 2 Afghanistan 2000  1599    1067    10296280     10299080
## 3 Brazil      1999  16982   20755   86001181     86005181
## 4 Brazil      2000  39440   41048   87251329     87253569
## 5 China       1999  104007  108252  636451250    636464022
## 6 China       2000  104746  109759  640212600    640215983&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we need to gather the problematic columns with &lt;code&gt;pivot_longer()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;untidy_wide %&amp;gt;%
  pivot_longer(cols=c(f_cases: m_population), names_to=c(&amp;quot;gender_count_type&amp;quot;), values_to=c(&amp;quot;counts&amp;quot;))

##   country     year  gender_count_type       counts
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;
##  1 Afghanistan 1999  f_cases                447
##  2 Afghanistan 1999  m_cases                298
##  3 Afghanistan 1999  f_population           9993400
##  4 Afghanistan 1999  m_population           9993671
##  5 Afghanistan 2000  f_cases                1599
##  6 Afghanistan 2000  m_cases                1067
##  7 Afghanistan 2000  f_population           10296280
##  8 Afghanistan 2000  m_population           10299080
##  9 Brazil      1999  f_cases                16982
## 10 Brazil      1999  m_cases                20755
## # … with 14 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this has usefully collapsed the dataset by gender, we now have a problem similar to that in Table &lt;a href=&#34;#tab:gapminder-untidy1&#34;&gt;6&lt;/a&gt; where observations are spread across the rows – in this instance &lt;code&gt;cases&lt;/code&gt; and &lt;code&gt;population&lt;/code&gt; are better treated as separate variables. This can be fixed by &lt;code&gt;separating&lt;/code&gt; the &lt;code&gt;gender_count_type&lt;/code&gt; variables and then spreading the values of the new &lt;code&gt;count_type&lt;/code&gt; (&lt;code&gt;cases&lt;/code&gt;, &lt;code&gt;population&lt;/code&gt;) across the columns. Hopefully you can see how this gets us to the &lt;strong&gt;tidy&lt;/strong&gt; &lt;code&gt;gapminder&lt;/code&gt; data structure in Table &lt;a href=&#34;#tab:gapminder-tidy&#34;&gt;5&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;untidy_wide %&amp;gt;%
  pivot_longer(cols=c(f_cases: m_population), names_to=c(&amp;quot;gender_count_type&amp;quot;), values_to=c(&amp;quot;counts&amp;quot;)) %&amp;gt;%
  separate(col=gender_count_type, into=c(&amp;quot;gender&amp;quot;, &amp;quot;count_type&amp;quot;), sep=&amp;quot;_&amp;quot;)

##    country     year  gender count_type counts
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;
##  1 Afghanistan 1999  f      cases      447
##  2 Afghanistan 1999  m      cases      298
##  3 Afghanistan 1999  f      population 9993400
##  4 Afghanistan 1999  m      population 9993671
##  5 Afghanistan 2000  f      cases      1599
##  6 Afghanistan 2000  m      cases      1067
##  7 Afghanistan 2000  f      population 10296280
##  8 Afghanistan 2000  m      population 10299080
##  9 Brazil      1999  f      cases      16982
## 10 Brazil      1999  m      cases      20755
## # … with 14 more rows

untidy_wide %&amp;gt;%
  pivot_longer(cols=c(f_cases: m_population), names_to=c(&amp;quot;gender_count_type&amp;quot;), values_to=c(&amp;quot;counts&amp;quot;)) %&amp;gt;%
  separate(col=gender_count_type, into=c(&amp;quot;gender&amp;quot;, &amp;quot;count_type&amp;quot;), sep=&amp;quot;_&amp;quot;) %&amp;gt;%
  pivot_wider(names_from=count_type, values_from=counts)

##    country     year  gender cases  population
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;
##  1 Afghanistan 1999  f      447    9993400
##  2 Afghanistan 1999  m      298    9993671
##  3 Afghanistan 2000  f      1599   10296280
##  4 Afghanistan 2000  m      1067   10299080
##  5 Brazil      1999  f      16982  86001181
##  6 Brazil      1999  m      20755  86005181
##  7 Brazil      2000  f      39440  87251329
##  8 Brazil      2000  m      41048  87253569
##  9 China       1999  f      104007 636451250
## 10 China       1999  m      108252 636464022
## 11 China       2000  f      104746 640212600
## 12 China       2000  m      109759 640215983&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Developing the vocabulary and technical skills to systematically describe and organise data is crucial to modern data analysis. This session has covered the fundamentals here: that data consist of &lt;strong&gt;observations&lt;/strong&gt; and &lt;strong&gt;variables&lt;/strong&gt; of different types &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-stevens_on_1946&#34; role=&#34;doc-biblioref&#34;&gt;Stevens 1946&lt;/a&gt;)&lt;/span&gt; and that in order to work effectively with datasets, especially in a functional way in &lt;code&gt;R&lt;/code&gt;, these data must be organised according to the rules of &lt;strong&gt;tidy&lt;/strong&gt; data &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wickham_tidy_2014&#34; role=&#34;doc-biblioref&#34;&gt;Wickham 2014&lt;/a&gt;)&lt;/span&gt;. Most of the session content was dedicated to the techniques that enable these concepts to be operationalised. We covered how to download, transform and reshape a reasonably large set of data from New York’s &lt;a href=&#34;&#34;&gt;Citibike&lt;/a&gt; scheme. In doing so, we generated insights that might inform further data collection and analysis activity. In the next session we will apply and extend this conceptual and technical knowledge as we introduce the fundamentals of visual data analysis and &lt;code&gt;ggplot2&lt;/code&gt;’s grammar of graphics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-lovelace_is_2020&#34; class=&#34;csl-entry&#34;&gt;
Lovelace, R., R. Beecham, E. Heinen, E. Vidal Tortosa, Y. Yuanxuan, C. Slade, and A. Roberts. 2020. &lt;span&gt;“Is the London Cycle Hire Scheme Becoming More Inclusive? An Evaluation of the Shifting Spatial Distribution of Uptake Based on 70 Million Trips.”&lt;/span&gt; &lt;em&gt;Transportation Research Part A: Policy and Practice&lt;/em&gt; 140 (October): 1–15.
&lt;/div&gt;
&lt;div id=&#34;ref-stevens_on_1946&#34; class=&#34;csl-entry&#34;&gt;
Stevens, S. 1946. &lt;span&gt;“On the Theory of Scales of Measurement.”&lt;/span&gt; &lt;em&gt;Science&lt;/em&gt; 103 (2684): 677–80.
&lt;/div&gt;
&lt;div id=&#34;ref-wickham_tidy_2014&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H. 2014. &lt;span&gt;“Tidy Data.”&lt;/span&gt; &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 59 (10): 1–23.
&lt;/div&gt;
&lt;div id=&#34;ref-wickham_r_2017&#34; class=&#34;csl-entry&#34;&gt;
Wickham, H., and G. Grolemund. 2017. &lt;em&gt;&lt;span&gt;R&lt;/span&gt; for Data Science: Import, Tidy, Transform, Visualize, and Model Data&lt;/em&gt;. Sebastopol, California: O’Reilly Media.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Note that the version of the trips data downloaded from my external repo contains a sample of just 500k records – this is not ideal, but was due to data storage limits on my external repo.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction: Vis for Geographic Data Science</title>
      <link>/class/01-class/</link>
      <pubDate>Mon, 09 May 2022 00:00:00 +0000</pubDate>
      <guid>/class/01-class/</guid>
      <description>
&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;h2&gt;Contents&lt;/h2&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#session-outcomes&#34;&gt;Session outcomes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#welcome-to-visualization-for-geographic-data-science&#34;&gt;Welcome to &lt;em&gt;Visualization for Geographic Data Science&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-vis-for-gds&#34;&gt;Why &lt;em&gt;vis-for-gds&lt;/em&gt;?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#geographic-data-science&#34;&gt;Geographic Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geographic-data-science-and-visualization&#34;&gt;Geographic Data Science and &lt;em&gt;Visualization&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-vis-for-gds&#34;&gt;What &lt;em&gt;vis-for-gds&lt;/em&gt;?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-vis-for-gds&#34;&gt;How &lt;em&gt;vis-for-gds&lt;/em&gt;?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-for-modern-data-analysis&#34;&gt;R for modern data analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rmarkdown-for-reproducible-research&#34;&gt;Rmarkdown for reproducible research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started-with-r-and-rstudio&#34;&gt;Getting started with R and RStudio&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#install-r-and-rstudio&#34;&gt;Install R and RStudio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#open-the-rstudio-ide&#34;&gt;Open the RStudio IDE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compute-in-the-console&#34;&gt;Compute in the console&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#install-some-packages&#34;&gt;Install some packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#experiment-with-r-markdown&#34;&gt;Experiment with R Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-scripts&#34;&gt;R Scripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#create-an-rstudio-project&#34;&gt;Create an RStudio Project&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;session-outcomes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session outcomes&lt;/h2&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;knowledge&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Appreciate&lt;/strong&gt; the motivation for this module – why visualization, why &lt;code&gt;R&lt;/code&gt; and why &lt;code&gt;ggplot2&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- - [x] An **awareness** of the challenges modern data analysis --&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;By the end of this session you should gain the following &lt;strong&gt;&lt;em&gt;practical skills&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-objective&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Navigate the materials on this course website, having familiarised yourself with its structure&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Open &lt;code&gt;R&lt;/code&gt; using the RStudio Integrated Developer Environment (IDE)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Install and enable &lt;code&gt;R&lt;/code&gt; packages and query package documentation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Perform basic calculations via the &lt;code&gt;R  Console&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Render &lt;code&gt;R Markdown&lt;/code&gt; files&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Create &lt;code&gt;R&lt;/code&gt; Projects&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Read-in datasets from external resources as objects (specifically &lt;code&gt;tibbles&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;welcome-to-visualization-for-geographic-data-science&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Welcome to &lt;em&gt;Visualization for Geographic Data Science&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Welcome to &lt;em&gt;Visualization for Geographic Data Science&lt;/em&gt; (&lt;em&gt;vis-for-gds&lt;/em&gt;). In this first session we’ll cover the background to the module – the why, what and how of &lt;em&gt;&lt;strong&gt;vis&lt;/strong&gt;-for-gds&lt;/em&gt;. If you’ve not already you should check out the course outline on the &lt;a href=&#34;../syllabus/&#34;&gt;Syllabus&lt;/a&gt; page for an overall preview of the module.&lt;/p&gt;
&lt;p&gt;The main home for this module is &lt;a href=&#34;../../&#34;&gt;this website&lt;/a&gt;. However, via &lt;a href=&#34;https://minerva.leeds.ac.uk/webapps/blackboard/execute/announcement?method=search&amp;amp;context=course&amp;amp;course_id=_521510_1&amp;amp;handle=cp_announcements&amp;amp;mode=cpview&#34;&gt;Minerva&lt;/a&gt; you can access the &lt;a href=&#34;https://minerva.leeds.ac.uk/bbcswebdav/pid-8457865-dt-content-rid-18601408_2/courses/202021_37303_GEOG5009M/GEOG5009_handbook.pdf&#34;&gt;Module Handbook&lt;/a&gt;. You will use submission boxes in &lt;a href=&#34;https://minerva.leeds.ac.uk/webapps/blackboard/content/listContentEditable.jsp?content_id=_7985652_1&amp;amp;course_id=_521510_1&amp;amp;mode=reset&#34;&gt;Minerva&lt;/a&gt; to upload coursework in the usual way.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-vis-for-gds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why &lt;em&gt;vis-for-gds&lt;/em&gt;?&lt;/h2&gt;
&lt;!-- https://www.forbes.com/sites/bernardmarr/2020/04/09/the-vital-role-of-big-data-in-the-fight-against-coronavirus/ --&gt;
&lt;div id=&#34;geographic-data-science&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Geographic Data Science&lt;/h3&gt;
&lt;p&gt;It is now taken-for-granted that over the last decade or so new data, new technology and new ways of doing science have transformed how we approach the world’s problems. Evidence for this can be seen in the response to the Covid-19 pandemic. Enter &lt;a href=&#34;https://www.google.com/search?q=covid-19+github&amp;amp;rlz=1C5CHFA_enGB632GB632&amp;amp;oq=covid-19+github&amp;amp;aqs=chrome..69i57j69i60l3.5575j0j1&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&#34;&gt;Covid19 github&lt;/a&gt; into a search and you’ll be confronted with hundreds of repositories demonstrating how an ever-expanding array of data related to the pandemic can be collected, processed and analysed. &lt;em&gt;Data Science&lt;/em&gt; is a term used widely to capture this shift and &lt;em&gt;Geographic Data Science&lt;/em&gt; (GDS), probably first discussed coherently by &lt;span class=&#34;citation&#34;&gt;Arribas-Bel and Reades (&lt;a href=&#34;#ref-arribas_geography_2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Singleton and Arribas-Bel (&lt;a href=&#34;#ref-singleton_geographic_2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, when observing that many of data science’s applications are – or at least should be – of inherent interest to geographers.&lt;/p&gt;
&lt;p&gt;Since gaining traction in the corporate world, the definition of Data Science has been somewhat stretched, but it has its origins in the work of John Tukey’s &lt;em&gt;The Future of Data Analysis&lt;/em&gt; (1962). Drawing on this, and a survey of more recent work, &lt;span class=&#34;citation&#34;&gt;Donoho (&lt;a href=&#34;#ref-donoho_fifty_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; neatly identifies six key facets that a data science discipline might encompass &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;data gathering, preparation, and exploration;&lt;/li&gt;
&lt;li&gt;data representation and transformation;&lt;/li&gt;
&lt;li&gt;computing with data;&lt;/li&gt;
&lt;li&gt;data visualization and presentation;&lt;/li&gt;
&lt;li&gt;data modelling;&lt;/li&gt;
&lt;li&gt;and a more introspective “science about data science”&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;geographic-data-science-and-visualization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Geographic Data Science and &lt;em&gt;Visualization&lt;/em&gt;&lt;/h3&gt;
&lt;!-- &gt; Visualization is fundamental to meeting the unprecedented challenges and exploiting the wonderful opportunities of the ever-expanding deluge of data confronting virtually every field.&#34; \
&gt; -- Prof. Jim Hollan of UC San Diego --&gt;
&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Visual approaches to data analysis are particularly suited to Geographic Data Science applications because where datasets are being repurposed for social and natural sciences research for the first time,  contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone and so where the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
&lt;p&gt;Glancing at this list, &lt;em&gt;visualization&lt;/em&gt; could be interpreted as a single facet of Data Science process &lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; – something that happens after data gathering, preparation, exploration, but before modelling. In this module you’ll learn that visualization is intrinsic to, and should inform, each of these activities, especially so when working with data sets that are spatial – for Geographic Data Science.&lt;/p&gt;
&lt;p&gt;Let’s develop this idea by asking &lt;strong&gt;why data visualizations are used in the first place&lt;/strong&gt;. In her book &lt;em&gt;Visualization Analysis and Design&lt;/em&gt;, Tamara &lt;span class=&#34;citation&#34;&gt;Munzner (&lt;a href=&#34;#ref-munzner_visualization_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; considers how humans and computers interface in the decision-making process. She makes the point that data visualization is ultimately about connecting people with data in order to make decisions – or to install humans in a ‘decision-making-loop’. There are occasionally decision-making loops that are entirely computational and where an automated solution exists and is trusted. However, most require some form of human intervention.&lt;/p&gt;
&lt;p&gt;The canonical example demonstrating how relying on computation alone can be problematic, and so for the use of visualization, is &lt;a href=&#34;https://en.wikipedia.org/wiki/Anscombe%27s_quartet&#34;&gt;Anscombe’s quartet&lt;/a&gt;. Here, &lt;span class=&#34;citation&#34;&gt;Anscombe (&lt;a href=&#34;#ref-anscombe_graphs_1973&#34; role=&#34;doc-biblioref&#34;&gt;1973&lt;/a&gt;)&lt;/span&gt; presents four datasets, each containing eleven observations and two variables for each observation. The data are synthetic, but let’s say that they are the weight and height of independent samples taken from a population of postgraduate students studying Data Science.&lt;/p&gt;
&lt;p&gt;Presented with a new dataset it makes sense to compute some summaries and doing so, we observe that the data appear identical – they contain the same means, variances and strong positive correlation coefficient. This seems appropriate since the data are measuring weight and height. Although there may be some variation, we’d expect taller students to be heavier. Given these statistical summaries we can be assured that we are drawing samples from the same population of (Data Science) students.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:anscombe-data&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../../../../../../../../../class/01-class_files/anscombe_data.png&#34; alt=&#34;Data from Anscombe&#39;s quartet&#34; width=&#34;90%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Data from Anscombe’s quartet
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Laying out the data in a meaningful way, horizontally according to &lt;em&gt;weight&lt;/em&gt; (&lt;em&gt;x&lt;/em&gt;) and vertically according to the &lt;em&gt;height&lt;/em&gt; (&lt;em&gt;y&lt;/em&gt;) to form a scatterplot, we quickly see that whilst these data contain the same statistical properties they are very different. Only &lt;code&gt;dataset #1&lt;/code&gt; now looks plausible if it were truly a measure of weights and heights drawn from a population of students.&lt;/p&gt;
&lt;p&gt;Anscombe’s is a deliberately contrived example&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, but there are real cases of important structure being missed, leading to poorly specified models and potentially faulty claims.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:anscombe-plot&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../../../../../../../../../class/01-class_files/anscombe.png&#34; alt=&#34;Plots of Anscombe&#39;s quartet&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Plots of Anscombe’s quartet
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is not to undermine the importance of numerical analysis. Numeric summaries that simplify patterns are extremely useful and Statistics has at its disposal an array of tools for helping to guard against making false claims from datasets – a theme that we will return to in session 6, 7 and 8 when we think critically about the use of visual approaches for data anlysis. There remain, though, certain classes of relation and context that cannot be easily captured through statistics alone.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Geographic&lt;/em&gt; context is undoubtedly challenging to capture numerically; many of the early examples of data visualization have been of spatial phenomena and generated by Geographers &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-friendly_brief_2007&#34; role=&#34;doc-biblioref&#34;&gt;Friendly 2007&lt;/a&gt;)&lt;/span&gt;. We can also probably make a special case for the use of visual approaches in Geographic Data Science (GDS) applications due to its exploratory nature. Often in GDS datasets are being repurposed for social and natural sciences research for the first time; contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone; and so the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance. In this module we will demonstrate this as we &lt;em&gt;explore&lt;/em&gt; (Session 4 and 5), &lt;em&gt;model under uncertainty&lt;/em&gt; (Session 6 and 7) and &lt;em&gt;communicate&lt;/em&gt; (Session 8 and 9) with various social science datasets.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-instruction&#34;&gt;
  &lt;div&gt;
    Watch &lt;a href=&#34;https://www.gicentre.net/jwo/index&#34;&gt;Jo Wood’s&lt;/a&gt; talk demonstrating how visual techniques can be used to analyse urban travel behaviours. In the video Jo argues that bikeshare schemes can help democratise cycling, but also for their potential contributions to research – he briefly contrasts new, passively collected data sets with more “traditional” actively collected data for analysing how people move around cities. A compelling case is then made for the use of visualization to support this activity. Related work and further discussion is published in &lt;span class=&#34;citation&#34;&gt;Beecham and Wood (&lt;a href=&#34;#ref-beecham_exploring_2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;https://www.youtube.com/embed/FaRBUnO5PZI&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;!-- &gt; Effective data visualizations should expose structure in data that would be difficult to expose through non-visual means --&gt;
&lt;!-- Problems are typically ill-specified, the relevant data/information informing the problem are not immediately obvious and analysis procedures and their interpretation are certainly subject to interpretation.

 &#39;computer in the loop&#39; and the &#39;human in the loop&#39;. In other words, what is it that computation can offer in supporting decision making and what is it that humans can offer. --&gt;
&lt;!-- @donoho_fifty_2017 neatly remarks that _data science_ probably has its origins in the work of John Tukey&#39;s _The Future of Data Analysis_ (1962), and that a _data science_ discipline might incorporate six key facets: data gathering, preparation, and exploration; data representation and transformation; computing with data; data modelling; data visualisation and presentation; and a more introspective “science about data science”. --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-vis-for-gds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What &lt;em&gt;vis-for-gds&lt;/em&gt;?&lt;/h2&gt;
&lt;p&gt;This is a very practical module. With the exception of this Introduction, the weekly sessions will blend both theory and practical coding activity. We will cover fundamentals around visual data analysis from Information Visualization and Statistics. As you read the session materials you will be writing data processing and analysis code and so be generating analysis outputs of your own. We will also be working with real datasets – from the Political Science, Urban and Transport Planning and Health domains. So we will hopefully be generating real findings and knowledge.&lt;/p&gt;
&lt;p&gt;To do this in a genuine way – to generate real knowledge from datasets – we will have to cover a reasonably broad set of data processing and analysis procedures. As well as developing expertise around designing data-rich, visually compelling graphics (of the sort demonstrated in &lt;a href=&#34;https://www.gicentre.net/jwo/index&#34;&gt;Jo Wood’s TEDx talk&lt;/a&gt;), we will need to cover more tedious aspects of &lt;strong&gt;data processing&lt;/strong&gt; and &lt;strong&gt;wrangling&lt;/strong&gt;. Additionally, if we are to learn how to generate and communicate and make claims under uncertainty with our data graphics, then we will need to cover some aspects of estimation and modelling from Statistics. In short, we will cover most of &lt;span class=&#34;citation&#34;&gt;Donoho (&lt;a href=&#34;#ref-donoho_fifty_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;’s six key facets of a data science discipline:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;data gathering, preparation, and exploration (Sessions 2, 3, 5);&lt;/li&gt;
&lt;li&gt;data representation and transformation (Sessions 2, 3);&lt;/li&gt;
&lt;li&gt;computing with data (Session 2, All sessions);&lt;/li&gt;
&lt;li&gt;data visualization and presentation (All sessions);&lt;/li&gt;
&lt;li&gt;data modelling (Sessions 4, 6, 7, 8);&lt;/li&gt;
&lt;li&gt;and a more introspective “science about data science” (All sessions)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is already a rich and impressive set of open &lt;a href=&#34;../useful&#34;&gt;Resources&lt;/a&gt; practically introducing how to do modern &lt;a href=&#34;https://r4ds.had.co.nz/index.html&#34;&gt;Data Science&lt;/a&gt;, &lt;a href=&#34;https://socviz.co/&#34;&gt;Visualization&lt;/a&gt; and &lt;a href=&#34;https://geocompr.robinlovelace.net/&#34;&gt;Geographic Analysis&lt;/a&gt;. We will certainly draw on these at different stages in the module. What makes this module different from these existing resources is that we will be &lt;strong&gt;doing&lt;/strong&gt; applied data science throughout – we will be identifying and diagnosing problems when gathering data, discovering patterns (some maybe even spurious) as we do exploratory analysis, and attempt to make claims under uncertainty as we generate models based on observed patterns. We will work with both new, passively-collected datasets, as well as more traditional, actively collected datasets located within various social science domains.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-vis-for-gds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How &lt;em&gt;vis-for-gds&lt;/em&gt;?&lt;/h2&gt;
&lt;div id=&#34;r-for-modern-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R for modern data analysis&lt;/h3&gt;
&lt;p&gt;Through the module we will apply modern approaches to data analysis. All data collection, analysis and reporting activity will be completed using &lt;a href=&#34;https://www.r-project.org/&#34;&gt;&lt;code&gt;R&lt;/code&gt;&lt;/a&gt; and the &lt;a href=&#34;https://rstudio.com/&#34;&gt;&lt;code&gt;RStudio&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Integrated_development_environment&#34;&gt;Integrated Development Environment&lt;/a&gt; (IDE). Released as open source software as part of a research project in 1995, for some time &lt;code&gt;R&lt;/code&gt; was the preserve of academics. From 2010s onwards, the &lt;code&gt;R&lt;/code&gt; community expanded rapidly and along with &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;code&gt;Python&lt;/code&gt;&lt;/a&gt; is regarded as the key technology for doing data analysis. &lt;code&gt;R&lt;/code&gt; is used increasingly outside of academia, by organisations such as Google &lt;a href=&#34;https://research.google.com/pubs/pub37483.html&#34;&gt;[example]&lt;/a&gt;, Facebook &lt;a href=&#34;http://flowingdata.com/2010/12/13/facebook-worldwide-friendships-mapped/&#34;&gt;[example]&lt;/a&gt;, Twitter &lt;a href=&#34;https://blog.twitter.com/official/en_us/a/2013/the-geography-of-tweets.html&#34;&gt;[example]&lt;/a&gt;, New York Times &lt;a href=&#34;http://www.nytimes.com/interactive/2012/05/05/sports/baseball/mariano-rivera-and-his-peers.html?ref=baseballexample&#34;&gt;[example]&lt;/a&gt;, BBC &lt;a href=&#34;https://bbc.github.io/rcookbook/&#34;&gt;[example]&lt;/a&gt; and many more.&lt;/p&gt;
&lt;p&gt;There are many benefits that come from being fully open-source, with a critical mass of users. Firstly, there is an array of online forums, tutorials and code examples from which to learn. Second, with such a large community, there are numerous expert &lt;code&gt;R&lt;/code&gt; users who themselves contribute by developing &lt;strong&gt;libraries&lt;/strong&gt; or &lt;strong&gt;packages&lt;/strong&gt; that extend its use. As a result &lt;code&gt;R&lt;/code&gt; is employed for a very wide set of use cases – this website was even built in &lt;code&gt;R&lt;/code&gt; using amongst other things the &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The key reason for our use of &lt;code&gt;R&lt;/code&gt; is the ecosystem of users and &lt;strong&gt;packages&lt;/strong&gt; that have emerged in recent years. An R package is a bundle of code, data and documentation, usually hosted on the &lt;a href=&#34;https://cran.r-project.org/&#34;&gt;CRAN&lt;/a&gt; (Comprehensive R Archive Network).
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Of particular importance is the &lt;a href=&#34;http://www.tidyverse.org&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt; package. This is a set of packages for doing Data Science authored by a software development team at &lt;a href=&#34;https://rstudio.com/&#34;&gt;RStudio&lt;/a&gt; led by &lt;a href=&#34;http://hadley.nz&#34;&gt;Hadley Wickham&lt;/a&gt;. &lt;code&gt;tidyverse&lt;/code&gt; packages share a principled underlying philosophy, syntax and documentation. Contained within the &lt;code&gt;tidyverse&lt;/code&gt; is its data visualization package, &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;&lt;code&gt;ggplot2&lt;/code&gt;&lt;/a&gt;. This package pre-dates the &lt;code&gt;tidyverse&lt;/code&gt; – it started as &lt;a href=&#34;http://had.co.nz/thesis/practical-tools-hadley-wickham.pdf&#34;&gt;Hadley Wickham’s PhD thesis&lt;/a&gt; and is one of the most widely-used toolkits for generating data graphics. As with other heavily used visualization toolkits (&lt;a href=&#34;https://www.tableau.com/en-gb&#34;&gt;&lt;code&gt;Tableau&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://vega.github.io/vega-lite/&#34;&gt;&lt;code&gt;vega-lite&lt;/code&gt;&lt;/a&gt;) it is inspired by Leland Wilkinson’s &lt;a href=&#34;https://www.springer.com/gp/book/9780387245447&#34;&gt;The Grammar of Graphics&lt;/a&gt;, the &lt;code&gt;gg&lt;/code&gt; in &lt;code&gt;ggplot&lt;/code&gt; stands for Grammar of Graphics. Understanding the design principles behind the &lt;em&gt;Grammar of Graphics&lt;/em&gt; (and &lt;code&gt;tidyverse&lt;/code&gt;) is necessary for modern data analysis and so we will cover this in detail in Session 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rmarkdown-for-reproducible-research&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rmarkdown for reproducible research&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them.&lt;/p&gt;
&lt;p&gt;Roger Peng, Jeff Leek and Brian Caffo&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In recent years there has been much introspection into how science works – around how statistical claims are made from reasoning over evidence. This came on the back of, amongst other things, a high profile paper published in &lt;a href=&#34;http://science.sciencemag.org/content/349/6251/aac4716&#34;&gt;Science&lt;/a&gt;, which found that of 100 recent peer-reviewed psychology experiments, the findings of only 39 could be replicated. The upshot is that researchers must now make every possible effort to make their work transparent, such that “&lt;em&gt;all&lt;/em&gt; aspects of the answer generated by any given analysis [can] be tested” &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brunsdon_opening_2020&#34; role=&#34;doc-biblioref&#34;&gt;Brunsdon and Comber 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A reproducible research project should be accompanied with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;code&lt;/em&gt; and &lt;em&gt;data&lt;/em&gt; that allows tables and figures presented in research outputs to be regenerated&lt;/li&gt;
&lt;li&gt;&lt;em&gt;code&lt;/em&gt; and &lt;em&gt;data&lt;/em&gt; that &lt;em&gt;does&lt;/em&gt; what it &lt;em&gt;claims&lt;/em&gt; (the code works)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;code&lt;/em&gt; and &lt;em&gt;data&lt;/em&gt; that can be justified and explained through proper documentation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If these goals are met, then it may be possible for others to use the code on new and different data to study whether the findings reported in one project might be &lt;strong&gt;replicated&lt;/strong&gt; or to use the same data, but update the code to, for example, extend the original analysis (to perform a re-analysis). This model – generate findings, test for replicability in new contexts and re-analysis – is essentially how knowledge development has always worked. However, to achieve this the data and procedures on which findings were generated must be made open and transparent.&lt;/p&gt;
&lt;p&gt;In this setting, traditional proprietary data analysis software such as SPSS and Esri’s ArcGIS that support point-and-click interaction is problematic. First, whilst these software may rely on the sorts of packages and libraries with bundled code that &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;Python&lt;/code&gt; uses for implementing statistical procedures, those libraries are closed. It is not possible, and therefore less common, for the researcher to fully interrogate into the underlying processes that are being implemented and the results need to be taken more or less on faith. Second, but probably most significantly (for us), it would be tedious to make notes describing all interactions performed when working with a dataset in SPSS or ArcGIS.&lt;/p&gt;
&lt;p&gt;As a declarative programming language, it is very easy to provide such a provenance trail for your workflows in &lt;code&gt;R&lt;/code&gt; since this necessarily exists in the analysis scripts. But more importantly, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Integrated_development_environment&#34;&gt;Integrated Development Environments&lt;/a&gt; (IDEs) through which &lt;code&gt;R&lt;/code&gt; (and &lt;code&gt;Python&lt;/code&gt;) are most often accessed provide notebook environments that allow users to curate reproducible computational documents that blend &lt;strong&gt;input code&lt;/strong&gt;, &lt;strong&gt;explanatory prose&lt;/strong&gt; and &lt;strong&gt;outputs&lt;/strong&gt;. In this module we will prepare these sorts of notebooks using &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R Markdown&lt;/a&gt;.&lt;/p&gt;
&lt;!-- Within Spatial Statistics, results from a Geographically Weighted Regression analysis implemented in ArcGIS were found to be inconsistent with those generated in `R` and `Python` [] -- inconsistencies that could not be examined as the code behind ArcGIS is closed. --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started-with-r-and-rstudio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting started with R and RStudio&lt;/h2&gt;
&lt;p&gt;I mentioned that the weekly sessions will blend both theory and practical coding activity. This Introduction has been dedicated more towards conceptual and procedural matters. For the practical element this time, we want to get you configured and familiar with &lt;code&gt;R&lt;/code&gt; and RStudio.&lt;/p&gt;
&lt;div id=&#34;install-r-and-rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Install R and RStudio&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Install the latest version of &lt;a href=&#34;https://cloud.r-project.org/&#34;&gt;R&lt;/a&gt;. Note that there are installations for &lt;a href=&#34;https://cloud.r-project.org/bin/windows/&#34;&gt;Windows&lt;/a&gt;, &lt;a href=&#34;https://cloud.r-project.org/&#34;&gt;macOS&lt;/a&gt; and &lt;a href=&#34;https://cloud.r-project.org/&#34;&gt;Linux&lt;/a&gt;. Run the installation from the file you downloaded (an &lt;code&gt;.exe&lt;/code&gt; or &lt;code&gt;.pkg&lt;/code&gt; extension).&lt;/li&gt;
&lt;li&gt;Install the latest version of &lt;a href=&#34;https://rstudio.com/products/rstudio/download/#download&#34;&gt;RStudio Desktop&lt;/a&gt;. Note again that there are separate installations depedning on operating system – for Windows an &lt;code&gt;.exe&lt;/code&gt; extension, macOS a &lt;code&gt;.dmg&lt;/code&gt; extension.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;open-the-rstudio-ide&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Open the RStudio IDE&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:rstudio-annotate&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../../../../../../../../../class/01-class_files/rstudio_annotate.png&#34; alt=&#34;The RStudio IDE&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: The RStudio IDE
&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Once installed, open the RStudio IDE.&lt;/li&gt;
&lt;li&gt;Open an R Script by clicking &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;New File&lt;/code&gt; &amp;gt; &lt;code&gt;R Script&lt;/code&gt; .&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You should see a set of windows roughly similar to those in the Figure (although I’ve already started on some of the computing exercises in the next section). The top left pane is used either as a &lt;em&gt;Code Editor&lt;/em&gt; (the tab named &lt;code&gt;Untitled1&lt;/code&gt;) or data viewer. This is where you’ll write, organise and comment R code for execution or inspect datasets as a spreadsheet representation. Below this in the bottom left pane is the R Console, in which you write and execute commands directly. To the top right is a pane with the tabs Environment and History. This displays all objects – data and plot items, calculated functions – stored in-memory during an R session. In the bottom right is a pane for navigating through project directories, displaying plots, details of installed and loaded packages and documentation on their functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-in-the-console&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compute in the console&lt;/h3&gt;
&lt;p&gt;You will write and execute almost all code from the code editor pane. To start though let’s use &lt;code&gt;R&lt;/code&gt; as a calculator by typing some commands into the Console. You’ll create an object (&lt;code&gt;x&lt;/code&gt;) and assign it a value using the assignment operator (&lt;code&gt;&amp;lt;-&lt;/code&gt;), then perform some simple statistical calculations using functions that are held within the &lt;code&gt;base&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The &lt;code&gt;base&lt;/code&gt; package is core and native to &lt;code&gt;R&lt;/code&gt;. Unlike all other packages, it does not need to be installed and called explicitly. One means of checking the package to which a function you are using belongs is to call the help command (&lt;code&gt;?&lt;/code&gt;) on that function: e.g. &lt;code&gt;?mean()&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Type the commands contained in the code block below into your R Console. Notice that since you are assigning values to each of these objects they are stored in memory and appear under the Global Environment pane.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create variable and assign a value.
x &amp;lt;- 4
# Perform some calculations using R as a calculator.
x_2 &amp;lt;- x^2
# Perform some calculations using functions that form baseR.
x_root &amp;lt;- sqrt(x_2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;install-some-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Install some packages&lt;/h3&gt;
&lt;p&gt;There are two steps to getting packages down and available in your working environment:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;install.packages(&#34;&amp;lt;package-name&amp;gt;&#34;)&lt;/code&gt; downloads the named package from a repository.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;library(&amp;lt;package-name&amp;gt;)&lt;/code&gt; makes the package available in your current session.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Install the &lt;code&gt;tidyverse&lt;/code&gt;, the core collection of packages for doing Data Science in &lt;code&gt;R&lt;/code&gt;, by running the code below:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;tidyverse&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have little or no experience in &lt;code&gt;R&lt;/code&gt;, it is easy to get confused around downloading and then using packages in a session. For example, let’s say we want to make use of the simple features package (&lt;a href=&#34;https://r-spatial.github.io/sf/index.html&#34;&gt;&lt;code&gt;sf&lt;/code&gt;&lt;/a&gt;), which we will draw on heavily in the module for performing spatial operations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the code below:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unless you’ve previously installed &lt;code&gt;sf&lt;/code&gt;, you’ll probably get an error message that looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Error in library(sf): there is no package called ‘sf’&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So let’s install it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the code below:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;sf&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now it’s installed, why not bring up some documentation on one of its functions (&lt;a href=&#34;https://r-spatial.github.io/sf/reference/geos_binary_pred.html&#34;&gt;&lt;code&gt;st_contains()&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the code below:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?st_contains()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since you’ve downloaded the package but not made it available to your session, you should get the message:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; No documentation for ‘st_contains’ in specified packages and libraries&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So let’s try again, by first calling &lt;code&gt;library(sf)&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the code below:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sf)
## Linking to GEOS 3.7.2, GDAL 2.4.1, PROJ 6.1.0
?st_contains()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s install some of the remaining core packages on which the module depends.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the block below, which passes a &lt;a href=&#34;https://r4ds.had.co.nz/vectors.html&#34;&gt;vector&lt;/a&gt; of package names to the &lt;code&gt;install.packages()&lt;/code&gt; function:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;quot;devtools&amp;quot;,&amp;quot;here&amp;quot;, &amp;quot;rmarkdown&amp;quot;, &amp;quot;knitr&amp;quot;,&amp;quot;fst&amp;quot;,&amp;quot;tidyverse&amp;quot;,
&amp;quot;lubridate&amp;quot;, &amp;quot;tidymodels&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    If you wanted to make use of a package only very occasionally in a single session, you could access it without explicitly loading it via &lt;code&gt;library(&amp;lt;package-name&amp;gt;)&lt;/code&gt;, using this syntax: &lt;code&gt;&amp;lt;package-name&amp;gt;::&amp;lt;function_name&amp;gt;&lt;/code&gt;, e.g. &lt;code&gt;?sf::st_contains()&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;experiment-with-r-markdown&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Experiment with R Markdown&lt;/h3&gt;
&lt;p&gt;R Markdown documents are suffixed with the extension &lt;code&gt;.Rmd&lt;/code&gt; and based partly on &lt;a href=&#34;https://en.wikipedia.org/wiki/Markdown&#34;&gt;Markdown&lt;/a&gt;, a lightweight markup language originally used as a means of minimising tedious mark-up tags (&lt;code&gt;&amp;lt;header&amp;gt;&amp;lt;/header&amp;gt;&lt;/code&gt;) when preparing HTML documents. The idea is that you trade some flexibility in the formatting of your HTML for ease-of-writing. Working with R Markdown is very similar to Markdown. Sections are denoted hierarchically with hashes (&lt;code&gt;#&lt;/code&gt;, &lt;code&gt;##&lt;/code&gt;, &lt;code&gt;###&lt;/code&gt;) and emphasis using &lt;code&gt;*&lt;/code&gt; symbols (&lt;code&gt;*emphasis* **added**&lt;/code&gt; reads &lt;em&gt;emphasis&lt;/em&gt; &lt;strong&gt;added&lt;/strong&gt; ). Different from standard Markdown, however, R Markdown documents can also contain code chunks to be run when the document is rendered or typeset – they are a mechanism for producing elegant reproducible notebooks.&lt;/p&gt;
&lt;p&gt;Each session of the module has an accompanying R Markdown file. In later sessions you will use these to author computational notebooks that blend code, analysis prose and outputs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the &lt;a href=&#34;../homework/01-homework_files/01-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 01-template.Rmd&lt;/a&gt; file for this session and open it in RStudio by clicking &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Open File ...&lt;/code&gt; &amp;gt; &lt;code&gt;&amp;lt;your-downloads&amp;gt;/01-template.Rmd&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A quick anatomy of an R Markdown files :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/YAML&#34;&gt;YAML&lt;/a&gt; - positioned at the head of the document and contains metadata determining amongst other things the author details and the output format when typesetting.&lt;/li&gt;
&lt;li&gt;TEXT - incorporated throughout to document and comment on your analysis.&lt;/li&gt;
&lt;li&gt;CODE chunks - containing discrete that are to be run when the .Rmd file is typeset or &lt;em&gt;knit&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:rmarkdown-annotate&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../../../../../../../../../class/01-class_files/rmarkdown_annotate.png&#34; alt=&#34;The anatomy of R Markdown&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: The anatomy of R Markdown
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/YAML&#34;&gt;YAML&lt;/a&gt; section of an &lt;code&gt;.Rmd&lt;/code&gt; file controls how your file is typeset and consists of &lt;code&gt;key: value&lt;/code&gt; pairs enclosed by &lt;code&gt;---&lt;/code&gt;. Notice that you can change the output format – so should you wish you can generate for example &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt; files for your reports.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
author: &amp;quot;Roger Beecham&amp;quot;
date: &amp;#39;2021-05-01&amp;#39;
title: &amp;quot;Session 01&amp;quot;
output:html_document
---&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R Markdown files are rendered or typeset with the &lt;strong&gt;Knit&lt;/strong&gt; button, annotated in the Figure above. This starts the &lt;code&gt;knitr&lt;/code&gt; package and executes all the code chunks and outputs a &lt;strong&gt;markdown&lt;/strong&gt; (&lt;code&gt;.md&lt;/code&gt;) file. The markdown file can then be converted to many different output formats via &lt;a href=&#34;https://pandoc.org/&#34;&gt;pandoc&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Knit&lt;/strong&gt; the &lt;a href=&#34;../homework/01-homework_files/01-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 01-template.Rmd&lt;/a&gt; file for this session, either by clicking the &lt;em&gt;Knit&lt;/em&gt; button or by typing &lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;⇧&lt;/kbd&gt; + &lt;kbd&gt;K&lt;/kbd&gt; on Windows, &lt;kbd&gt;⌘&lt;/kbd&gt; + &lt;kbd&gt;⇧&lt;/kbd&gt; + &lt;kbd&gt;K&lt;/kbd&gt; on macOS.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You will notice that R Markdown &lt;strong&gt;code chunks&lt;/strong&gt; can be customised in different ways. This is achieved by populating fields in the curly brackets at the start of the code chunk:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r &amp;lt;chunk-name&amp;gt;, echo=TRUE, eval=FALSE, cache=FALSE}
  # Some code that is either run or rendered.
```&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quick overview of the parameters.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;chunk-name&amp;gt;&lt;/code&gt; - Chunks can be given distinct names. This is useful for navigating R markdown file. It also supports chaching – chunks with distinct names are only run once, important if certain chunks take some time to execute.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;echo=&amp;lt;TRUE|FALSE&amp;gt;&lt;/code&gt; - Determines whether the code is visible or hidden from the typeset file. If you output file is a data analysis report you may not wish to expose lengthy code chunks as these may disrupt the discursive text that appears outside of the code chunks.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;eval=&amp;lt;TRUE|FALSE&amp;gt;&lt;/code&gt; - Determines whether the code is evaluated (executed). This is useful if you wish to present some code in your document for display purposes.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cache=&amp;lt;TRUE|FALSE&amp;gt;&lt;/code&gt; - Determines where the results from the code chunk are cached.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As part of the &lt;a href=&#34;../homework/01-homework/&#34;&gt;homework&lt;/a&gt; from this session you will do some more research on R Markdown. It is worth in advance downloading RStudio’s cheatsheets, which provide comprehensive details on how to configure R Markdown documents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open RStudio and select &lt;code&gt;Help&lt;/code&gt; &amp;gt; &lt;code&gt;Cheatsheets&lt;/code&gt; &amp;gt; &lt;code&gt;R Markdown Cheat Sheet&lt;/code&gt; | &lt;code&gt;R Markdown Reference Guide&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-scripts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Scripts&lt;/h3&gt;
&lt;p&gt;Whilst there are obvious benefits to working in R Markdown documents when doing data analysis, there may be occasions where working in an script is preferable. Scripts are plain text files with the extension &lt;code&gt;.R&lt;/code&gt;. Comments – text that are not executed as code – are denoted with the &lt;code&gt;#&lt;/code&gt; symbol.&lt;/p&gt;
&lt;p&gt;I tend to use R Scripts for writing discrete but substantial code blocks that are to be executed. For example, I might generate a set of &lt;a href=&#34;https://r4ds.had.co.nz/functions.html&#34;&gt;functions&lt;/a&gt; that relate to a particular use case and bundle these together in an R script. These then might be referred to in a data analysis from an &lt;code&gt;.Rmd&lt;/code&gt;, which makes various use of these functions in a similar way as one might import a package. Below is an example script that we will encounter later in the module when creating flow visualizations in R very similar to those that appear in Jo Wood’s TEDx talk. This script is saved with the fie name &lt;code&gt;bezier_path.R&lt;/code&gt;. If it were stored in a sensible location, like a project’s &lt;code&gt;code&lt;/code&gt; folder, it could be called from an R Markdown file with &lt;code&gt;source(./code/bezier_path)&lt;/code&gt;. R Scripts can be edited in the same way as R Markdown files in RStudio, via the &lt;em&gt;Code Editor&lt;/em&gt; pane.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# bezier_path.R
#
# Author: Roger Beecham
##############################################################################

#&amp;#39; Functions for generating input data for asymmetric bezier curve for OD data,
#&amp;#39; such that the origin is straight and destination curve. The retuned tibble
#&amp;#39; is passed to geom_bezier().Parametrtisation follows that published in
#&amp;#39; Wood et al. 2011. doi: 10.3138/carto.46.4.239.
#&amp;#39; @param data A df with origin and destination pairs representing 2D locations
#&amp;#39; (o_east, o_north, d_east, d_north) in cartesian (OSGB) space.
#&amp;#39; @param degrees For converting to radians.
#&amp;#39; @return A tibble of coordinate pairs representing asymmetric curve

get_trajectory &amp;lt;- function(data) {
  o_east=data$o_east
  o_north=data$o_north
  d_east=data$d_east
  d_north=data$d_north
  od_pair=data$od_pair

  curve_angle=get_radians(-90)
  east=(o_east-d_east)/6
  north=(o_north-d_north)/6
  c_east=d_east + east*cos(curve_angle) - north*sin(curve_angle)
  c_north=d_north + north*cos(curve_angle) + east*sin(curve_angle)
  d &amp;lt;- tibble(
    x=c(o_east,c_east,d_east),
    y=c(o_north,c_north,d_north),
    od_pair=od_pair
  )
}

# Convert degrees to radians.
get_radians &amp;lt;- function(degrees) {
  (degrees * pi) / (180)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To an extent R Scripts are more straightforward than R Markdown files in that you don’t have to worry about configuring code chunks. They are really useful for quickly developing bits of code. This can be achieved by highlighting over the code that you wish to execute and clicking the &lt;code&gt;Run&lt;/code&gt; icon at the top of the &lt;em&gt;Code Editor&lt;/em&gt; pane or by typing &lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;rtn&lt;/kbd&gt; on Windows, &lt;kbd&gt;⌘&lt;/kbd&gt; + &lt;kbd&gt;rtn&lt;/kbd&gt; on macOS&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-an-rstudio-project&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create an RStudio Project&lt;/h3&gt;
&lt;p&gt;Throughout this module we will use project-oriented workflows. This is where all files pertaining to a data analysis – data, code and outputs – are organised from a single root folder and where &lt;em&gt;file path discipline&lt;/em&gt; is used such that all paths are relative to the project’s root folder (see &lt;a href=&#34;https://rstats.wtf/project-oriented-workflow.html&#34;&gt;Bryan &amp;amp; Hester 2020&lt;/a&gt;). You can imagine that this sort of self-contained project set-up is necessary for achieving reproducibility of your research. It allows anyone to take a project and run it on their own machines without having to make any adjustments.&lt;/p&gt;
&lt;p&gt;You might have noticed that when you open RStudio it automatically &lt;em&gt;points to&lt;/em&gt; a working directory, likely the home folder for your local machine, denoted with &lt;code&gt;~/&lt;/code&gt; in the Console. RStudio will by default save any outputs to this folder and will also expect any data you use to be saved there. Clearly if you want to incorporate neat, self-contained project workflows then you will want to organise your work from a dedicated project folder rather than the default home folder for your machine. This can be achieved with the &lt;code&gt;setwd(&amp;lt;path-to-your-project&amp;gt;)&lt;/code&gt; function. The problem with doing this is that you insert a path which cannot be understood outside of your local machine at the time it was created. This is a real pain. It makes simple things like moving projects around on your machine an arduous task and most importantly it hinders reproducibility if others are to reuse your work.&lt;/p&gt;
&lt;p&gt;RStudio Projects are a really excellent feature of the RStudio IDE that resolve these problems. Whenever you load up an RStudio Project, R starts up and the working directory is automatically set to the project’s root folder. If you were to move the project elsewhere on your machine, or to another machine, a new root is automatically generated – so RStudio projects ensure that relative paths work.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:rstudio-project&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../../../../../../../../../class/01-class_files/rstudio_project.png&#34; alt=&#34;Creating an RStudio Project&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Creating an RStudio Project
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let’s create a new Project for this module:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Select &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;New Project&lt;/code&gt; &amp;gt; &lt;code&gt;New Directory&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Browse to a sensible location and give the project a suitable name. Then click &lt;code&gt;Create Project&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You will notice that the top of the Console window now indicates the root for this new project, in my case &lt;code&gt;~projects/vis-for-gds&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the root of your project, create folders called &lt;code&gt;reports&lt;/code&gt;, &lt;code&gt;code&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt;, &lt;code&gt;figures&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Save this session’s &lt;a href=&#34;../homework/01-homework_files/01-template.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; 01-template.Rmd&lt;/a&gt; file to the &lt;code&gt;reports&lt;/code&gt; folder.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Your project’s folder structure should now look like this:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;vis-for-gds\
  vis-for-gds.Rproj
  code\
  data\
  figures\
  reports\
    01-template.Rmd&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Visual data analysis approaches are necessary for exploring complex patterns in data and to make and communicate claims under uncertainty. This is especially true of Geographic Data Science applications, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;datasets are being repurposed for social and natural sciences research for the first time;&lt;/li&gt;
&lt;li&gt;contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone;&lt;/li&gt;
&lt;li&gt;and, consequently, where the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this module we will demonstrate this as we explore (Session 4 and 5), model under uncertainty (Session 6 and 7) and communicate (Session 8 and 9) with various social science datasets. We will work with both new, large-scale behavioural datasets, as well as more traditional, administrative datasets located within various social science domains: Political Science, Crime Science, Urban and Transport Planning.&lt;/p&gt;
&lt;p&gt;We will do so using the statistical programming environment &lt;code&gt;R&lt;/code&gt;, which along with &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;code&gt;Python&lt;/code&gt;&lt;/a&gt;, is &lt;em&gt;the&lt;/em&gt; programming environment for modern data analysis. We will make use of various tools and software libraries that form part of the &lt;code&gt;R&lt;/code&gt; ecosystem – the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt; for doing modern data science and &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R Markdown&lt;/a&gt; for authoring reproducible research projects.&lt;/p&gt;
&lt;!-- https://www.sciencedirect.com/science/article/pii/S1353829220311758?via%3Dihub --&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-anscombe_graphs_1973&#34; class=&#34;csl-entry&#34;&gt;
Anscombe, F. 1973. &lt;span&gt;“Graphs in Statistical Analysis.”&lt;/span&gt; &lt;em&gt;American Statistician&lt;/em&gt; 27 (1): 17–21. doi:&lt;a href=&#34;https://doi.org/10.1080/00031305.1973.10478966&#34;&gt;10.1080/00031305.1973.10478966&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-arribas_geography_2018&#34; class=&#34;csl-entry&#34;&gt;
Arribas-Bel, D., and J. Reades. 2018. &lt;span&gt;“Geography and Computers: Past, Present, and Future.”&lt;/span&gt; &lt;em&gt;Geography Compass&lt;/em&gt; 12 (10): e12403. doi:&lt;a href=&#34;https://doi.org/10.1111/gec3.12403&#34;&gt;10.1111/gec3.12403&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-beecham_exploring_2014&#34; class=&#34;csl-entry&#34;&gt;
Beecham, R., and J. Wood. 2014. &lt;span&gt;“Exploring Gendered Cycling Behaviours Within a Large-Scale Behavioural Data-Set.”&lt;/span&gt; &lt;em&gt;Transportation Planning and Technology&lt;/em&gt; 37 (1). Taylor &amp;amp; Francis: 83–97.
&lt;/div&gt;
&lt;div id=&#34;ref-brunsdon_opening_2020&#34; class=&#34;csl-entry&#34;&gt;
Brunsdon, C., and A. Comber. 2020. &lt;span&gt;“Opening Practice: Supporting Reproducibility and Critical Spatial Data Science.”&lt;/span&gt; &lt;em&gt;Journal of Geographical Systems&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-donoho_fifty_2017&#34; class=&#34;csl-entry&#34;&gt;
Donoho, D. 2017. &lt;span&gt;“50 Years of Data Science.”&lt;/span&gt; &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 26 (6): 745–66. doi:&lt;a href=&#34;https://doi.org/10.1080/10618600.2017.1384734&#34;&gt;10.1080/10618600.2017.1384734&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-friendly_brief_2007&#34; class=&#34;csl-entry&#34;&gt;
Friendly, M. 2007. &lt;span&gt;“A Brief History of Data Visualization.”&lt;/span&gt; In &lt;em&gt;Handbook of Computational Statistics: Data Visualization&lt;/em&gt;, edited by C. Chen, W. Härdle, and A Unwin, III:1–34. Heidelberg: Springer-Verlag. &lt;a href=&#34;http://datavis.ca/papers/hbook.pdf&#34;&gt;http://datavis.ca/papers/hbook.pdf&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-matejka_same_2017&#34; class=&#34;csl-entry&#34;&gt;
Matejka, J., and G. Fitzmaurice. 2017. &lt;span&gt;“Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics Through Simulated Annealing.”&lt;/span&gt; In, 1290–94. CHI ’17. New York, NY, USA: Association for Computing Machinery. doi:&lt;a href=&#34;https://doi.org/10.1145/3025453.3025912&#34;&gt;10.1145/3025453.3025912&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-munzner_visualization_2014&#34; class=&#34;csl-entry&#34;&gt;
Munzner, T. 2014. &lt;em&gt;Visualization Analysis and Design&lt;/em&gt;. AK Peters Visualization Series. Boca Raton, FL: CRC Press.
&lt;/div&gt;
&lt;div id=&#34;ref-singleton_geographic_2019&#34; class=&#34;csl-entry&#34;&gt;
Singleton, A., and D. Arribas-Bel. 2019. &lt;span&gt;“Geographic Data Science.”&lt;/span&gt; &lt;em&gt;Geographical Analysis&lt;/em&gt;. doi:&lt;a href=&#34;https://doi.org/10.1111/gean.12194&#34;&gt;10.1111/gean.12194&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For an excellent precis and interpretation of this for geographers, see &lt;span class=&#34;citation&#34;&gt;Arribas-Bel and Reades (&lt;a href=&#34;#ref-arribas_geography_2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Although not the case when actually reading &lt;span class=&#34;citation&#34;&gt;Donoho (&lt;a href=&#34;#ref-donoho_fifty_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Checkout &lt;span class=&#34;citation&#34;&gt;Matejka and Fitzmaurice (&lt;a href=&#34;#ref-matejka_same_2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;’s Same Stats, Different Graphs paper for a fun take one this.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
